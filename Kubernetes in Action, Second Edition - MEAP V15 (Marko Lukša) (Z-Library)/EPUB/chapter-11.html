
        <html lang="en">
        <head>
        <meta charset="UTF-8"/>
        </head>
        <body>
        <div><div class="readable-text" data-hash="96801d16c9662976089569e942ffe2fc" data-text-hash="94e7074bf7236a509a09d2a3b7c346bc" id="1" refid="1">
<h1>11 Exposing Pods with Services</h1>
</div>
<div class="introduction-summary">
<h3 class="intro-header">This chapter covers</h3>
<ul>
<li class="readable-text" data-hash="8a31fb351246ede9174badce044b4d83" data-text-hash="8a31fb351246ede9174badce044b4d83" id="2" refid="2">Communication between pods</li>
<li class="readable-text" data-hash="12593caf5854f29224b8278d9d654bbc" data-text-hash="12593caf5854f29224b8278d9d654bbc" id="3" refid="3">Distributing client connections over a group of pods providing the same service</li>
<li class="readable-text" data-hash="3bb2fc21510e16b19a4c6d66314a4ac3" data-text-hash="3bb2fc21510e16b19a4c6d66314a4ac3" id="4" refid="4">Discovering services in the cluster through DNS and environment variables</li>
<li class="readable-text" data-hash="f2eb24038c2eb9c7f1a4615d32530ab4" data-text-hash="f2eb24038c2eb9c7f1a4615d32530ab4" id="5" refid="5">Exposing services to clients outside the cluster</li>
<li class="readable-text" data-hash="a182bb60e33e1666a69f975e69299d0c" data-text-hash="a182bb60e33e1666a69f975e69299d0c" id="6" refid="6">Using readiness probes to add or remove individual pods from services</li>
</ul>
</div>
<div class="readable-text" data-hash="c61ea4c89e4833f2effb374ba6667f7a" data-text-hash="3f41c6279d45c694a6be1325a39c9271" id="7" refid="7">
<p>Instead of running a single pod to provide a particular service, people nowadays typically run several replicas of the pod so that the load can be distributed across multiple cluster nodes. But that means all pod replicas providing the same service should be reachable at a single address so clients can use that single address, rather than having to keep track of and connect directly to individual pod instances. In Kubernetes, you do that with Service objects.</p>
</div>
<div class="readable-text" data-hash="7a829511e567bbc94d540631f02f0d08" data-text-hash="eb3ba0f84c576c810ef29d8fcb39ecda" id="8" refid="8">
<p>The Kiada suite you&#8217;re building in this book consists of three services - the Kiada service, the Quiz service, and the Quote service. So far, these are three isolated services that you interact with individually, but the plan is to connect them, as shown in the following figure.</p>
</div>
<div class="browsable-container figure-container" data-hash="94e709885c8d4e6c28a91d7052173e9e" data-text-hash="5f8ea6de9f7d63293e4085d23c30f2a8" id="9" refid="9">
<h5>Figure 11.1 The architecture and operation of the Kiada suite.</h5>
<img alt="" data-processed="true" height="238" id="Picture_113" loading="lazy" src="EPUB/images/11image002.png" width="762">
</div>
<div class="readable-text" data-hash="42703c8cc64609f3d7660884b807125b" data-text-hash="9d33a4af73883f354a92372871f23bc1" id="10" refid="10">
<p>The Kiada service will call the other two services and integrate the information they return into the response it sends to the client. Multiple pod replicas will provide each service, so you&#8217;ll need to use Service objects to expose them.</p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="260cc6dcef2c22785feb4596e3fe5a61" data-text-hash="10de4bc81f754b19b0d27246a0589c05" id="11" refid="11">
<h5>NOTE</h5>
</div>
<div class="readable-text" data-hash="34bf7daabb712e88988cf920258d1f37" data-text-hash="db800291f87619d3e53131aef43defe7" id="12" refid="12">
<p> You&#8217;ll find the code files for this chapter at <a href="master.html">https://github.com/luksa/kubernetes-in-action-2nd-edition/tree/master/Chapter11</a>.</p>
</div>
</div>
<div class="readable-text" data-hash="0d4cdb37e21611269b02bbbe076ffcc5" data-text-hash="89f55bb7e88a1935f191f9b471983f94" id="13" refid="13">
<p>Before you create the Service objects, deploy the pods and the other objects by applying the manifests in the <code>Chapter11/SETUP/</code> directory as follows:</p>
</div>
<div class="browsable-container listing-container" data-hash="c32e5a827c708eda95613eeec5e54baf" data-text-hash="0909cdf0b48cb49c0c152c081e5a6b63" id="14" refid="14">
<div class="code-area-container">
<pre class="code-area">$ kubectl apply -f SETUP/ --recursive</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="cba1915aca236b765e2ae6659c3a8ee5" data-text-hash="e0f34b4c31c714e00940a1be36127798" id="15" refid="15">
<p>You may recall from the previous chapter that this command applies all manifests in the specified directory and its subdirectories. After applying these manifests, you should have multiple pods in your current Kubernetes namespace.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" data-hash="411a7daf7324fcbe2ffb6a6595708b46" data-text-hash="42e1cda66289301ed11576595a1583ae" id="16" refid="16">
<h5>Understanding how pods communicate</h5>
</div>
<div class="readable-text" data-hash="61b5be6b5bce1ddc1c9e185c2407a43c" data-text-hash="26a6097573d0b79b09c3e085b9d8632d" id="17" refid="17">
<p>You learned in chapter 5 what pods are, when to combine multiple containers into a pod, and how those containers communicate. But how do containers from different pods communicate?</p>
</div>
<div class="readable-text" data-hash="fc70b9b7085b3e7c631cccd110acf8f3" data-text-hash="44cc81075e66759bc53168e687dcf9f7" id="18" refid="18">
<p>Each pod has its own network interface with its own IP address. All pods in the cluster are connected by a single private network with a flat address space. As shown in the following figure, even if the nodes hosting the pods are geographically dispersed with many network routers in between, the pods can communicate over their own flat network where no <i>NAT</i> (Network Address Translation) is required. This pod network is typically a software-defined network that&#8217;s layered on top of the actual network that connects the nodes.</p>
</div>
<div class="browsable-container figure-container" data-hash="4457fb95deaa76c988653f13e2a74206" data-text-hash="c0763c97650b863fed6d3406fedd87b0" id="19" refid="19">
<h5>Figure 11.2 Pods communicate via their own computer network</h5>
<img alt="" data-processed="true" height="288" id="Picture_1" loading="lazy" src="EPUB/images/11image003.png" width="803">
</div>
<div class="readable-text" data-hash="217f6fdb00847c85d0db55922b930858" data-text-hash="9c0e88f3ae0a0f8c9aff30301559e634" id="20" refid="20">
<p>When a pod sends a network packet to another pod, neither SNAT (Source NAT) nor DNAT (Destination NAT) is performed on the packet. This means that the source IP and port, and the destination IP and port, of packets exchanged directly between pods are never changed. If the sending pod knows the IP address of the receiving pod, it can send packets to it. The receiving pod can see the sender&#8217;s IP as the source IP address of the packet.</p>
</div>
<div class="readable-text" data-hash="0dbea431f0a5bf0aa31305c033483585" data-text-hash="b1e58aaa15bcbad52ea26230f4b148b5" id="21" refid="21">
<p>Although there are many Kubernetes network plugins, they must all behave as described above. Therefore, the communication between two pods is always the same, regardless of whether the pods are running on the same node or on nodes located in different geographic regions. The containers in the pods can communicate with each other over the flat NAT-less network, like computers on a local area network (LAN) connected to a single network switch. From the perspective of the applications, the actual network topology between the nodes isn&#8217;t important.</p>
</div>
</div>
<div class="readable-text" data-hash="428e20266d6acfbad0972b32cfb7a56c" data-text-hash="adbf1a54fdcd4b9dda181ace610a4c55" id="22" refid="22">
<h2 id="sigil_toc_id_189">11.1&#160;Exposing pods via services</h2>
</div>
<div class="readable-text" data-hash="da2247ef545bc5c7e1f473b7b1254fd4" data-text-hash="7ee553017e8ff6fc91346129baaf2479" id="23" refid="23">
<p>If an application running in one pod needs to connect to another application running in a different pod, it needs to know the address of the other pod. This is easier said than done for the following reasons:</p>
</div>
<ul>
<li class="readable-text" data-hash="8a78ada8140d843224a1065513f57f37" data-text-hash="c92837d466b734a39875847c75130394" id="24" refid="24">Pods are <i>ephemeral</i>. A pod can be removed and replaced with a new one at any time. This happens when the pod is evicted from a node to make room for other pods, when the node fails, when the pod is no longer needed because a smaller number of pod replicas can handle the load, and for many other reasons.</li>
<li class="readable-text" data-hash="f80861c35aef6d9c18c690918a67f46c" data-text-hash="f80861c35aef6d9c18c690918a67f46c" id="25" refid="25">A pod gets its IP address when it&#8217;s assigned to a node. You don&#8217;t know the IP address of the pod in advance, so you can&#8217;t provide it to the pods that will connect to it.</li>
<li class="readable-text" data-hash="7bbf17b880f0ea9c502e61f661852629" data-text-hash="7bbf17b880f0ea9c502e61f661852629" id="26" refid="26">In horizontal scaling, multiple pod replicas provide the same service. Each of these replicas has its own IP address. If another pod needs to connect to these replicas, it should be able to do so using a single IP or DNS name that points to a load balancer that distributes the load across all replicas.</li>
</ul>
<div class="readable-text" data-hash="679307e76bef2b1bc5bf6c723fafa541" data-text-hash="50de39d6489cbb7eb0d3509d428bed4f" id="27" refid="27">
<p>Also, some pods need to be exposed to clients outside the cluster. Until now, whenever you wanted to connect to an application running in a pod, you used port forwarding, which is for development only. The right way to make a group of pods externally accessible is to use a Kubernetes Service.</p>
</div>
<div class="readable-text" data-hash="e5bf23394f85da6399adfc6bb6c85187" data-text-hash="7154416ffae10aec91bc220d93f6f263" id="28" refid="28">
<h3 id="sigil_toc_id_190">11.1.1&#160;Introducing services</h3>
</div>
<div class="readable-text" data-hash="853239ec327c26eb8f1fad0139491f3d" data-text-hash="cff18f25e902784763805d4fd585a3d8" id="29" refid="29">
<p>A Kubernetes Service is an object you create to provide a single, stable access point to a set of pods that provide the same service. Each service has a stable IP address that doesn&#8217;t change for as long as the service exists. Clients open connections to that IP address on one of the exposed network ports, and those connections are then forwarded to one of the pods that back that service. In this way, clients don&#8217;t need to know the addresses of the individual pods providing the service, so those pods can be scaled out or in and moved from one cluster node to the other at will. A service acts as a load balancer in front of those pods.</p>
</div>
<div class="readable-text" data-hash="568f04947769b357cca2eba2704fb4e1" data-text-hash="0d0e7ef2d8cf65eb61fe8f60fdc6d42d" id="30" refid="30">
<h4>Understanding why you need services</h4>
</div>
<div class="readable-text" data-hash="8a70a7c8957da726083a8faf4568e5e5" data-text-hash="4ed61a784748f1b7cefb3800a7871184" id="31" refid="31">
<p>The Kiada suite is an excellent example to explain services. It contains three sets of pods that provide three different services. The Kiada service calls the Quote service to retrieve a quote from the book, and the Quiz service to retrieve a quiz question.</p>
</div>
<div class="readable-text" data-hash="dd3b135313ee6a4595b6f97e72b8d4bf" data-text-hash="0749ba72e9e6ff24e9b4d8d7071ce851" id="32" refid="32">
<p>I&#8217;ve made the necessary changes to the Kiada application in version 0.5. You can find the updated source code in the <code>Chapter11/</code> directory in the book&#8217;s code repository. You&#8217;ll use this new version throughout this chapter. You&#8217;ll learn how to configure the Kiada application to connect to the other two services, and you&#8217;ll make it visible to the outside world. Since both the number of pods in each service and their IP addresses can change, you&#8217;ll expose them via Service objects, as shown in the following figure.</p>
</div>
<div class="browsable-container figure-container" data-hash="2cca906485bed10c8d98703097c815c9" data-text-hash="0c378e2613a77b022f910df6051db1c4" id="33" refid="33">
<h5>Figure 11.3 Exposing pods with Service objects</h5>
<img alt="" data-processed="true" height="343" id="Picture_2" loading="lazy" src="EPUB/images/11image004.png" width="843">
</div>
<div class="readable-text" data-hash="2a7f368251aa2b45e63e21fef877d258" data-text-hash="e14faca2a8dfc186af7e2f2eeff5d62a" id="34" refid="34">
<p>By creating a service for the Kiada pods and configuring it to be reachable from outside the cluster, you create a single, constant IP address through which external clients can connect to the pods. Each connection is forwarded to one of the kiada pods.</p>
</div>
<div class="readable-text" data-hash="d35ac889244bd346ce7f2887600c2cc5" data-text-hash="f136274858a71d1defc6886344c6d9b5" id="35" refid="35">
<p>By creating a service for the Quote pods, you create a stable IP address through which the Kiada pods can reach the Quote pods, regardless of the number of pod instances behind the service and their location at any given time.</p>
</div>
<div class="readable-text" data-hash="eba0952f570f2ebe18c10ac2d7452ea8" data-text-hash="cd4df6e28a1427f6a6858f1bbc12f788" id="36" refid="36">
<p>Although there&#8217;s only one instance of the Quiz pod, it too must be exposed through a service, since the pod&#8217;s IP address changes every time the pod is deleted and recreated. Without a service, you&#8217;d have to reconfigure the Kiada pods each time or make the pods get the Quiz pod&#8217;s IP from the Kubernetes API. If you use a service, you don&#8217;t have to do that because its IP address never changes.</p>
</div>
<div class="readable-text" data-hash="567b2f7a64453fb9fdfdce4149d6a1fa" data-text-hash="f85436649635b050ce9862b8cfa922cc" id="37" refid="37">
<h4>Understanding how pods become part of a service</h4>
</div>
<div class="readable-text" data-hash="c486dea7d4cef208acc1d27b2baf4243" data-text-hash="81edf59b316012222df9624df758b063" id="38" refid="38">
<p>A service can be backed by more than one pod. When you connect to a service, the connection is passed to one of the backing pods. But how do you define which pods are part of the service and which aren&#8217;t?</p>
</div>
<div class="readable-text" data-hash="ebabd72776ba8a9ed3c4e6754ac800b5" data-text-hash="32e135b56db3ecdfdf385f1be260f353" id="39" refid="39">
<p>In the previous chapter, you learned about labels and label selectors and how they&#8217;re used to organize a set of objects into subsets. Services use the same mechanism. As shown in the next figure, you add labels to Pod objects and specify the label selector in the Service object. The pods whose labels match the selector are part of the service.</p>
</div>
<div class="browsable-container figure-container" data-hash="3d9e7dd2e78d30708a042ecac5586b6f" data-text-hash="6e36900206493c2d7fc3722ed1436862" id="40" refid="40">
<h5>Figure 11.4 Label selectors determine which pods are part of the Service.</h5>
<img alt="" data-processed="true" height="248" id="Picture_3" loading="lazy" src="EPUB/images/11image005.png" width="836">
</div>
<div class="readable-text" data-hash="3f5c0fbc71b1aa5039403fa47a8c7ad0" data-text-hash="f27b84d1bc99d074304b23c0cc29f987" id="41" refid="41">
<p>The label selector defined in the <code>quote</code> service is <code>app=quote</code>, which means that it selects all <code>quote</code> pods, both stable and canary instances, since they all contain the label key <code>app</code> with the value <code>quote</code>. Other labels on the pods don&#8217;t matter.</p>
</div>
<div class="readable-text" data-hash="d8480e3cd38843f05cf4363688df5085" data-text-hash="944e2e5e7c399f894c285d6f5b52076c" id="42" refid="42">
<h3 id="sigil_toc_id_191">11.1.2&#160;Creating and updating services</h3>
</div>
<div class="readable-text" data-hash="6dc9bf105fea8a2de5bc1033a068a24c" data-text-hash="0603f404d5f0678fee8233ce0194f9c3" id="43" refid="43">
<p>Kubernetes supports several types of services: <code>ClusterIP</code>, <code>NodePort</code>, <code>LoadBalancer</code>, and <code>ExternalName</code>. The <code>ClusterIP</code> type, which you&#8217;ll learn about first, is only used internally, within the cluster. If you create a Service object without specifying its type, that&#8217;s the type of service you get. The services for the Quiz and Quote pods are of this type because they&#8217;re used by the Kiada pods within the cluster. The service for the Kiada pods, on the other hand, must also be accessible to the outside world, so the <code>ClusterIP</code> type isn&#8217;t sufficient.</p>
</div>
<div class="readable-text" data-hash="82259b2c1a8aa4203136931d8c27a4e3" data-text-hash="f317ba181a564386e5d285b5831e1080" id="44" refid="44">
<h4>Creating a service YAML manifest</h4>
</div>
<div class="readable-text" data-hash="ea9c14fa5826eb82bd030c5972d9d9bb" data-text-hash="6e59a16f7d416c7869750a3f86f71ae7" id="45" refid="45">
<p>The following listing shows the minimal YAML manifest for the <code>quote</code> Service object.</p>
</div>
<div class="browsable-container listing-container" data-hash="3996c41c52ed84437f0f06105747530e" data-text-hash="fc3e6b5b57cfd6b025e8442bec6cd3a8" id="46" refid="46">
<h5>Listing 11.1 YAML manifest for the quote service</h5>
<div class="code-area-container">
<pre class="code-area">apiVersion: v1    #A
kind: Service    #A
metadata:
  name: quote    #B
spec:
  type: ClusterIP    #C
  selector:    #D
    app: quote    #D
  ports:    #E
  - name: http    #E
    port: 80    #E
    targetPort: 80    #E
    protocol: TCP    #E</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhpcyBtYW5pZmVzdCBkZXNjcmliZXMgYSBTZXJ2aWNlIG9iamVjdC4KI0IgVGhlIG5hbWUgb2YgdGhpcyBzZXJ2aWNlLgojQyBDbHVzdGVySVAgc2VydmljZXMgYXJlIGFjY2Vzc2libGUgb25seSB3aXRoaW4gdGhlIGNsdXN0ZXIuCiNEIFRoZSBsYWJlbCBzZWxlY3RvciB0aGF0IHNwZWNpZmllcyB3aGljaCBwb2RzIGFyZSBwYXJ0IG9mIHRoaXMgc2VydmljZS4KI0UgUG9ydCA4MCBvZiB0aGlzIHNlcnZpY2UgaXMgbWFwcGVkIHRvIHBvcnQgODAgaW4gdGhlIHBvZHMgdGhhdCBiYWNrIHRoaXMgc2VydmljZS4="></div>
</div>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="47" refid="47">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="8b84a08f1b52d2e309d2c10c1a48070b" data-text-hash="f4cc96383b53afec7374930f598fe291" id="48" refid="48">
<p> Since the <code>quote</code> Service object is one of the objects that make up the Quote application, you could also add the <code>app: quote</code> label to this object. However, because this label isn&#8217;t required for the service to function, it&#8217;s omitted in this example.</p>
</div>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="49" refid="49">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="0094539de69cce2b3d488ee4b5b13459" data-text-hash="ca95a5d44a3fbcc8503a4aedf7a6c0c9" id="50" refid="50">
<p>&#8195;If you create a service with multiple ports, you must specify a name for each port. It&#8217;s best to do the same for services with a single port.</p>
</div>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="51" refid="51">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="30b98025c15c229a5f45b62b4892b5cf" data-text-hash="3c39ce2a8561856bf4f92d304dce352c" id="52" refid="52">
<p> Instead of specifying the port number in the <code>targetPort</code> field, you can also specify the name of the port as defined in the container&#8217;s port list in the pod definition. This allows the service to use the correct target port number even if the pods behind the service use different port numbers.</p>
</div>
</div>
<div class="readable-text" data-hash="1955aa59dc8564836b3d898a47b33b48" data-text-hash="90a6f8cd7064e8e60c7629ded6a6e535" id="53" refid="53">
<p>The manifest defines a <code>ClusterIP</code> Service named <code>quote</code>. The service accepts connections on port <code>80</code> and forwards each connection to port <code>80</code> of a randomly selected pod matching the <code>app=quote</code> label selector, as shown in the following figure.</p>
</div>
<div class="browsable-container figure-container" data-hash="44a8161ea07c444404b6c979f337be7f" data-text-hash="3699a8effa59a2f83cc819063261a80b" id="54" refid="54">
<h5>Figure 11.5 The quote service and the pods that it forwards traffic to</h5>
<img alt="" data-processed="true" height="266" id="Picture_4" loading="lazy" src="EPUB/images/11image006.png" width="749">
</div>
<div class="readable-text" data-hash="6dc7d03bf9de886b2b4f60e7f1177610" data-text-hash="51de7d14cdb6bf78836e6cc976497166" id="55" refid="55">
<p>To create the service, apply the manifest file to the Kubernetes API using <code>kubectl apply</code>.</p>
</div>
<div class="readable-text" data-hash="f0614a08d0d356f036dcb0e3abebcbb2" data-text-hash="5b28249c54dd765308884e08d2db4405" id="56" refid="56">
<h4>Creating a service with kubectl expose</h4>
</div>
<div class="readable-text" data-hash="9be2c721f2afbfd66b957cb90fd6e3a5" data-text-hash="c676c1642e445a3db416721f2872c212" id="57" refid="57">
<p>Normally, you create services like you create other objects, by applying an object manifest using <code>kubectl apply</code>. However, you can also create services using the <code>kubectl expose</code> command, as you did in chapter 3 of this book.</p>
</div>
<div class="readable-text" data-hash="c1f835b1f4c6c008b06af351a245012a" data-text-hash="7cc81d006da09510ea3433eca39519b1" id="58" refid="58">
<p>Create the service for the Quiz pod as follows:</p>
</div>
<div class="browsable-container listing-container" data-hash="142a1a6b8271fdbd94f2fc67c1d9feac" data-text-hash="350c921c24b3c51f40fea9cc0be25954" id="59" refid="59">
<div class="code-area-container">
<pre class="code-area">$ kubectl expose pod quiz --name quiz
service/quiz exposed</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="bbb2b0c2710b8cf33e5cf68d186989e2" data-text-hash="0fcdeb1a0c2fd24d66a02f2c296e5a19" id="60" refid="60">
<p>This command creates a service named <code>quiz</code> that exposes the <code>quiz</code> pod. To do this, it checks the pod&#8217;s labels and creates a Service object with a label selector that matches all the pod&#8217;s labels.</p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="61" refid="61">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="df3f3c7ae8293620d8d4868b93e4274e" data-text-hash="e1635f18c03d8d3342e6ec9569d0d8ce" id="62" refid="62">
<p> In chapter 3, you used the <code>kubectl expose</code> command to expose a Deployment object. In this case, the command took the selector from the Deployment and used it in the Service object to expose all its pods. You&#8217;ll learn about Deployments in chapter 13.</p>
</div>
</div>
<div class="readable-text" data-hash="90531e8c7136dcfe8d4e970f95bb3bb1" data-text-hash="1605def94aae5d8e1595f59fca310ee6" id="63" refid="63">
<p>You&#8217;ve now created two services. You&#8217;ll learn how to connect to them in section 11.1.3, but first let&#8217;s see if they&#8217;re configured correctly.</p>
</div>
<div class="readable-text" data-hash="cac00de749b2763d49c9987db0391c6a" data-text-hash="e2de3b670151bf7ec8c943125e55b246" id="64" refid="64">
<h4>Listing services</h4>
</div>
<div class="readable-text" data-hash="c655e96349ba3bb151cb8596b7e92e5c" data-text-hash="836a4794de3da4f6be661dc053d58999" id="65" refid="65">
<p>When you create a service, it&#8217;s assigned an internal IP address that any workload running in the cluster can use to connect to the pods that are part of that service. This is the cluster IP address of the service. You can see it by listing services with the <code>kubectl get services</code> command. If you want to see the label selector of each service, use the <code>-o wide</code> option as follows:</p>
</div>
<div class="browsable-container listing-container" data-hash="d6ce26bb185b60eec76577dc0a54a73c" data-text-hash="788eeb756992d072fdbd3ac16ec38b89" id="66" refid="66">
<div class="code-area-container">
<pre class="code-area">$ kubectl get svc -o wide
NAME    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE   SELECTOR
quiz    ClusterIP   10.96.136.190   &lt;none&gt;        8080/TCP   15s   app=quiz,rel=stable
quote   ClusterIP   10.96.74.151    &lt;none&gt;        80/TCP     23s   app=quote</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="67" refid="67">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="3a0aebbf35fe284e30f4ad7cfc75ac05" data-text-hash="48e066232d07660574ac3455a2f20cce" id="68" refid="68">
<p> The shorthand for <code>services</code> is <code>svc</code>.</p>
</div>
</div>
<div class="readable-text" data-hash="cc8012597f5c28213398ceb82954d537" data-text-hash="d98c0e92481f33832f5c8955a035ee05" id="69" refid="69">
<p>The output of the command shows the two services you created. For each service, the type, IP addresses, exposed ports, and label selector are printed.</p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="70" refid="70">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="4b6ee78e6488a4d7fe13e43848be4f2d" data-text-hash="fd53e8cd66e996549b1926db781a5bd9" id="71" refid="71">
<p> You can also view the details of each service with the <code>kubectl describe svc</code> command.</p>
</div>
</div>
<div class="readable-text" data-hash="64cf0a4dcd7dbd7ef57e87fb9d2c6850" data-text-hash="18017bef20827e612b67f44867963808" id="72" refid="72">
<p>You&#8217;ll notice that the <code>quiz</code> service uses a label selector that selects pods with the labels <code>app: quiz</code> and <code>rel: stable</code>. This is because these are the labels of the <code>quiz</code> pod from which the service was created using the <code>kubectl expose</code> command.</p>
</div>
<div class="readable-text" data-hash="a2698a3394eb2452510f3f479c5d9d7d" data-text-hash="9cece1915c3c2cb0f50685f9dfdd2a31" id="73" refid="73">
<p>Let&#8217;s think about this. Do you want the <code>quiz</code> service to include only the stable pods? Probably not. Maybe later you decide to deploy a canary release of the quiz service in parallel with the stable version. In that case, you want traffic to be directed to both pods.</p>
</div>
<div class="readable-text" data-hash="542d20dc7810d97d6f48ab21714accbc" data-text-hash="fa3e897eddfe57a69a6c7baebbe21c05" id="74" refid="74">
<p>Another thing I don&#8217;t like about the <code>quiz</code> service is the port number. Since the service uses HTTP, I&#8217;d prefer it to use port 80 instead of 8080. Fortunately, you can change the service after you create it.</p>
</div>
<div class="readable-text" data-hash="fcf9b0ccbf2c3973efa7206a6f96ae98" data-text-hash="180a9a50602a2668c59ff67f5c8d6d7e" id="75" refid="75">
<h4>Changing the service&#8217;s label selector</h4>
</div>
<div class="readable-text" data-hash="a46d49477dfe1aa19c8154abfeb814b0" data-text-hash="3055e06e8e0e93ec9071167e2e5f91cd" id="76" refid="76">
<p>To change the label selector of a service, you can use the <code>kubectl set selector</code> command. To fix the selector of the <code>quiz</code> service, run the following command:</p>
</div>
<div class="browsable-container listing-container" data-hash="42b57ddcc8c55c67b15dc478f0491b0a" data-text-hash="e3f6b06adb7575d48db0c42bea0c6636" id="77" refid="77">
<div class="code-area-container">
<pre class="code-area">$ kubectl set selector service quiz app=quiz
service/quiz selector updated</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="31888b47e364d93ce15b6a66bce4c851" data-text-hash="abbf60cd4acd7d613bea943131e30873" id="78" refid="78">
<p>List the services again with the <code>-o wide</code> option to confirm the selector change. This method of changing the selector is useful if you&#8217;re deploying multiple versions of an application and want to redirect clients from one version to another.</p>
</div>
<div class="readable-text" data-hash="aaf102b586b3daa9bf08b0f23a4df381" data-text-hash="06b40b4ffa800a0bb4b62f0edca0fbaa" id="79" refid="79">
<h4>Changing the ports exposed by the service</h4>
</div>
<div class="readable-text" data-hash="5e56b23162fee02d11ae586439eb1132" data-text-hash="fcae22ddbb548fa994498450d5eaf826" id="80" refid="80">
<p>To change the ports that the service forwards to pods, you can edit the Service object with the <code>kubectl edit</code> command or update the manifest file and then apply it to the cluster.</p>
</div>
<div class="readable-text" data-hash="ebbcb6d724ccc631754203d9f7bff1d6" data-text-hash="b907826bd6342c66cd9380d8f1fe8e57" id="81" refid="81">
<p>Before continuing, run <code>kubectl edit svc quiz</code> and change the port from <code>8080</code> to <code>80</code>, making sure to only change the <code>port</code> field and leaving the <code>targetPort</code> set to <code>8080</code>, as this is the port that the <code>quiz</code> pod listens on.</p>
</div>
<div class="readable-text" data-hash="5ef6e9aa2aa3654b712091d934d4c986" data-text-hash="f3cd6c06c0f2faaab81618d48a09769e" id="82" refid="82">
<h4>Configuring basic service properties</h4>
</div>
<div class="readable-text" data-hash="4a7e6f141d343914858b1db5720b52fb" data-text-hash="c23e3a73b9c379e81f4c12e52b8c12cd" id="83" refid="83">
<p>The following table lists the basic fields you can set in the Service object.</p>
</div>
<div class="browsable-container" data-hash="b37fc57940909082628ff29b34f2f203" data-text-hash="5c45d72839a5fed5a5a046af4d3115e8" id="84" refid="84">
<h5>Table 11.1 Fields in the Service object&#8217;s spec for configuring the service&#8217;s basic properties</h5>
<table border="1" cellpadding="0" cellspacing="0" width="100%">
<tbody>
<tr>
<td> <p>Field</p> </td>
<td> <p>Field type</p> </td>
<td> <p>Description</p> </td>
</tr>
<tr>
<td> <p></p><pre>type
</pre> </td>
<td> <p></p><pre>string
</pre> </td>
<td> <p>Specifies the type of this Service object. Allowed values are <code>ClusterIP</code>, <code>NodePort</code>, <code>LoadBalancer</code>, and <code>ExternalName</code>. The default value is <code>ClusterIP</code>. The differences between these types are explained in the following sections of this chapter.</p> </td>
</tr>
<tr>
<td> <p></p><pre>clusterIP
</pre> </td>
<td> <p></p><pre>string
</pre> </td>
<td> <p>The internal IP address within the cluster where the service is available. Normally, you leave this field blank and let Kubernetes assign the IP. If you set it to <code>None</code>, the service is a headless service. These are explained in section 11.4.</p> </td>
</tr>
<tr>
<td> <p></p><pre>selector
</pre> </td>
<td> <p></p><pre>map[string]string
</pre> </td>
<td> <p>Specifies the label keys and values that the pod must have in order for this service to forward traffic to it. If you you don&#8217;t set this field, you are responsible for managing the service endpoints. This is explained in section 11.3.</p> </td>
</tr>
<tr>
<td> <p></p><pre>ports
</pre> </td>
<td> <p></p><pre>[]Object
</pre> </td>
<td> <p>List of ports exposed by this service. Each entry can specify the <code>name</code>, <code>protocol</code>, <code>appProtocol</code>, <code>port</code>, <code>nodePort</code>, and <code>targetPort</code>.</p> </td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" data-hash="44dfacc339a2c4ab8ac2ae8abf1f8cad" data-text-hash="54adc37de84a5bacc28f341efab9e584" id="85" refid="85">
<p>Other fields are explained throughout the remainder of this chapter.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" data-hash="65b45e2fd31eb0d8a0519e75a1d0630d" data-text-hash="7b1f93e9c3a367287472a75a33747bdf" id="86" refid="86">
<h5>IPv4/IPv6 dual-stack support</h5>
</div>
<div class="readable-text" data-hash="90117f89240e08dc20c76c3ab5bab3b3" data-text-hash="3373e3da6c20ae1a09e711530f434646" id="87" refid="87">
<p>Kubernetes supports both IPv4 and IPv6. Whether dual-stack networking is supported in your cluster depends on whether the <code>IPv6DualStack</code> feature gate is enabled for the cluster components to which it applies.</p>
</div>
<div class="readable-text" data-hash="e4db3518187c8c81a4a37eb44c4600f7" data-text-hash="827e8bdc1b64445902846ff49a25e466" id="88" refid="88">
<p>When you create a Service object, you can specify whether you want the service to be a single- or dual-stack service through the <code>ipFamilyPolicy</code> field. The default value is <code>SingleStack</code>, which means that only a single IP family is assigned to the service, regardless of whether the cluster is configured for single-stack or dual-stack networking. Set the value to <code>PreferDualStack</code> if you want the service to receive both IP families when the cluster supports dual-stack, and one IP family when it supports single-stack networking. If your service requires both an IPv4 and an IPv6 address, set the value to <code>RequireDualStack</code>. The creation of the service will be successful only on dual-stack clusters.</p>
</div>
<div class="readable-text" data-hash="67becc85b2b57c3305b503204143a240" data-text-hash="6cb9d6f17e49136c1a85f90fb678b38f" id="89" refid="89">
<p>After you create the Service object, its <code>spec.ipFamilies</code> array indicates which IP families have been assigned to it. The two valid values are <code>IPv4</code> and <code>IPv6</code>. You can also set this field yourself to specify which IP family to assign to the service in clusters that provide dual-stack networking. The <code>ipFamilyPolicy</code> must be set accordingly or the creation will fail.</p>
</div>
<div class="readable-text" data-hash="e5c2cb788e2886cec76d15c345a3e956" data-text-hash="3fd6f567e1c5f2becf8ed87f93c27bc1" id="90" refid="90">
<p>For dual-stack services, the <code>spec.clusterIP</code> field contains only one of the IP addresses, but the <code>spec.clusterIPs</code> field contains both the IPv4 and IPv6 addresses. The order of the IPs in the <code>clusterIPs</code> field corresponds to the order in the <code>ipFamilies</code> field.</p>
</div>
</div>
<div class="readable-text" data-hash="e01b8917a42dfe2eb0ff37dfd27fcb6b" data-text-hash="b91858d1ab2e7b47667f1c5ec03f7768" id="91" refid="91">
<h3 id="sigil_toc_id_192">11.1.3&#160;Accessing cluster-internal services</h3>
</div>
<div class="readable-text" data-hash="b72725e901ab391ba23d81a1d6296e06" data-text-hash="c073c839e1d03bd8b48b9acff8d0fd38" id="92" refid="92">
<p>The <code>ClusterIP</code> services you created in the previous section are accessible only within the cluster, from other pods and from the cluster nodes. You can&#8217;t access them from your own machine. To see if a service is actually working, you must either log in to one of the nodes with <code>ssh</code> and connect to the service from there, or use the <code>kubectl exec</code> command to run a command like <code>curl</code> in an existing pod and get it to connect to the service.</p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="93" refid="93">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="822a99aafd3b8da544189ef01df904a9" data-text-hash="4ea45db46a79a0fb31f902642fb6a11f" id="94" refid="94">
<p> You can also use the <code>kubectl port-forward svc/my-service</code> command to connect to one of the pods backing the service. However, this command doesn&#8217;t connect to the service. It only uses the Service object to find a pod to connect to. The connection is then made directly to the pod, bypassing the service.</p>
</div>
</div>
<div class="readable-text" data-hash="3141b9a3c0f00c0486e18eb92613f584" data-text-hash="b3c3691123d38ffb23266fb82b438727" id="95" refid="95">
<h4>Connecting to services from pods</h4>
</div>
<div class="readable-text" data-hash="adb1df52e8838a220cf6dfbfac7f5d0e" data-text-hash="2160c8103fcc2d6c7fc06b0c11427be2" id="96" refid="96">
<p>To use the service from a pod, run a shell in the <code>quote-001</code> pod as follows:</p>
</div>
<div class="browsable-container listing-container" data-hash="f10aa2e5b816b62702c9d58dbe514e26" data-text-hash="28323bbe5309175b7a68788b5b2bd405" id="97" refid="97">
<div class="code-area-container">
<pre class="code-area">$ kubectl exec -it quote-001 -c nginx -- sh
/ #</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="c8373118f9a0ccab011d6f80cde3f4a3" data-text-hash="0580b45d87ab90ca5a7c1e7c5ee69c87" id="98" refid="98">
<p>Now check if you can access the two services. Use the cluster IP addresses of the services that <code>kubectl get services</code> displays. In my case, the <code>quiz</code> service uses cluster IP <code>10.96.136.190</code>, whereas the <code>quote</code> service uses IP <code>10.96.74.151</code>. From the <code>quote-001</code> pod, I can connect to the two services as follows:</p>
</div>
<div class="browsable-container listing-container" data-hash="5ee2483c9bda56205a7277c3b635a207" data-text-hash="4c631d7dd53ae8d221770f686ba6cdf1" id="99" refid="99">
<div class="code-area-container">
<pre class="code-area">/ # curl http://10.96.136.190    #A
This is the quiz service running in pod quiz
 
/ # curl http://10.96.74.151    #B
This is the quote service running in pod quote-canary</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhpcyBpcyB0aGUgY2x1c3RlciBJUCBvZiB0aGUgcXVpeiBzZXJ2aWNlLCBhcyBzaG93biBieSBrdWJlY3RsIGdldCBzZXJ2aWNlcy4KI0IgVGhpcyBpcyB0aGUgY2x1c3RlciBJUCBvZiB0aGUgcXVvdGUgc2VydmljZS4="></div>
</div>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="100" refid="100">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="69dc86c909ad297c62daa9ea9ce9320d" data-text-hash="0d68c3aa87c8cd499d8cc005e49e03a9" id="101" refid="101">
<p> You don&#8217;t need to specify the port in the curl command, because you set the service port to 80, which is the default for HTTP.</p>
</div>
</div>
<div class="readable-text" data-hash="335d8eecd6c8fffe201c5a7104756e14" data-text-hash="5f52f025021ceccd6286bb6a62581785" id="102" refid="102">
<p>If you repeat the last command several times, you&#8217;ll see that the service forwards the request to a different pod each time:</p>
</div>
<div class="browsable-container listing-container" data-hash="b0dfcc09d39f276a6604f97f19506fbb" data-text-hash="982ac5c21825553ce961e4cd3d24dcc6" id="103" refid="103">
<div class="code-area-container">
<pre class="code-area">/ # while true; do curl http://10.96.74.151; done
This is the quote service running in pod quote-canary
This is the quote service running in pod quote-003
This is the quote service running in pod quote-001
...</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="0f3d1994333eb5fdc927ebfe658462df" data-text-hash="854e420550e05b8eb1c7b37bcf394508" id="104" refid="104">
<p>The service acts as a load balancer. It distributes requests to all the pods that are behind it.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" data-hash="033b9bc8153abd8567724506d8205d94" data-text-hash="44014883c211ba53d228dbb87e6bd21b" id="105" refid="105">
<h5>Configuring session affinity on services</h5>
</div>
<div class="readable-text" data-hash="ea0790b687f6b7952aff61ac0fb02dd2" data-text-hash="8b8e149dabd1b994e0ba6987eb7590c0" id="106" refid="106">
<p>You can configure whether the service should forward each connection to a different pod, or whether it should forward all connections from the same client to the same pod. You do this via the <code>spec.sessionAffinity</code> field in the Service object. Only two types of service session affinity are supported: <code>None</code> and <code>ClientIP</code>.</p>
</div>
<div class="readable-text" data-hash="186428726d8e72f86f5914aa51b9af8c" data-text-hash="547a2ee6866109ae5250bd19b379f3e7" id="107" refid="107">
<p>The default type is <code>None</code>, which means there&#8217;s no guarantee to which pod each connection will be forwarded. However, if you set the value to <code>ClientIP</code>, all connections originating from the same IP will be forwarded to the same pod. In the <code>spec.sessionAffinityConfig.clientIP.timeoutSeconds</code> field, you can specify how long the session will persist. The default value is 3 hours.</p>
</div>
<div class="readable-text" data-hash="43690c2bc34f2a2835134f3405105613" data-text-hash="88e806433ce3333a504bd377670bccf9" id="108" refid="108">
<p>It may surprise you to learn that Kubernetes doesn&#8217;t provide cookie-based session affinity. However, considering that Kubernetes services operate at the transport layer of the OSI network model (UDP and TCP) not at the application layer (HTTP), they don&#8217;t understand HTTP cookies at all.</p>
</div>
</div>
<div class="readable-text" data-hash="950b3693b526f79899491ac179855743" data-text-hash="882267e355556bb7bfe579813b2db9c2" id="109" refid="109">
<h4>Resolving services via DNS</h4>
</div>
<div class="readable-text" data-hash="0e104d2e803f29c86ef09aecc0d5ac42" data-text-hash="d75983a698d70404a19f009572dc0820" id="110" refid="110">
<p>Kubernetes clusters typically run an internal DNS server that all pods in the cluster are configured to use. In most clusters, this internal DNS service is provided by CoreDNS, whereas some clusters use kube-dns. You can see which one is deployed in your cluster by listing the pods in the <code>kube-system</code> namespace.</p>
</div>
<div class="readable-text" data-hash="fee6b5fb2bbcf90b469818c38ced9106" data-text-hash="edbb91d1ff4d1e3bc0997de0edb7f4db" id="111" refid="111">
<p>No matter which implementation runs in your cluster, it allows pods to resolve the cluster IP address of a service by name. Using the cluster DNS, pods can therefore connect to the <code>quiz</code> service like so:</p>
</div>
<div class="browsable-container listing-container" data-hash="fe9dab2fbcdf9569c13061aaceeb2779" data-text-hash="5aba4db2600e9344afaa97dedd0e1dc5" id="112" refid="112">
<div class="code-area-container">
<pre class="code-area">/ # curl http://quiz    #A
This is the quiz service running in pod quiz</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIHNlcnZpY2UgbmFtZSBpcyB1c2VkIGluc3RlYWQgb2YgaXRzIGNsdXN0ZXIgSVAu"></div>
</div>
</div>
<div class="readable-text" data-hash="7b6c79e15175266e9885e9881682834c" data-text-hash="41ac32a108787dba94dfd934d5586dd5" id="113" refid="113">
<p>A pod can resolve any service defined in the same namespace as the pod by simply pointing to the name of the service in the URL. If a pod needs to connect to a service in a different namespace, it must append the namespace of the Service object to the URL. For example, to connect to the <code>quiz</code> service in the <code>kiada</code> namespace, a pod can use the URL <code>http://quiz.kiada/</code> regardless of which namespace it&#8217;s in.</p>
</div>
<div class="readable-text" data-hash="a7b49dba2182e93184b049bfbb70f625" data-text-hash="aed715875e612217fdeb42623b960754" id="114" refid="114">
<p>From the <code>quote-001</code> pod where you ran the shell command, you can also connect to the service as follows:</p>
</div>
<div class="browsable-container listing-container" data-hash="0334021ba0683a61b7a8d35316ccc691" data-text-hash="17dab7db5cf31b3574bd97029627dc57" id="115" refid="115">
<div class="code-area-container">
<pre class="code-area">/ # curl http://quiz.kiada    #A
This is the quiz service running in pod quiz</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIG5hbWUgb2YgdGhlIHNlcnZpY2UgaXMgcXVpejsga2lhZGEgaXMgdGhlIG5hbWVzcGFjZS4="></div>
</div>
</div>
<div class="readable-text" data-hash="3ecfe98dcbb6695bd2ca4e29c96654b6" data-text-hash="88ce5a9deab281a43756bbc9edc95464" id="116" refid="116">
<p>A service is resolvable under the following DNS names:</p>
</div>
<ul>
<li class="readable-text" data-hash="be2c868c8b4b90a9e36ff1bf305f587a" data-text-hash="167aa4e17956ede85c334ef9080c623e" id="117" refid="117"><code class="codechar">&lt;service-name&gt;</code>, if the service is in the same namespace as the pod performing the DNS lookup,</li>
<li class="readable-text" data-hash="a4dde1ce9b76f1dfdf210b18dc16ba85" data-text-hash="17dd23453d47c5af504ecaa8aa1bc8c6" id="118" refid="118"><code>&lt;service-name&gt;.&lt;service-namespace&gt;</code> from any namespace, but also under</li>
<li class="readable-text" data-hash="fe3f9c288a66820553df7e05c42ee457" data-text-hash="1561f96d79ab3a2c58c6014891e7569c" id="119" refid="119"><code>&lt;service-name&gt;.&lt;service-namespace&gt;.svc</code>, and</li>
<li class="readable-text" data-hash="b4f147fe77f08b39c623c9ac5d9a4157" data-text-hash="57ac08b4789257caac471e5b787792ce" id="120" refid="120"><code>&lt;service-name&gt;.&lt;service-namespace&gt;.svc.cluster.local</code>.</li>
</ul>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="121" refid="121">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="ca345fa29a46e3ee3401b646cf614d98" data-text-hash="2003cdf13962f1c54558a2aaaf9c4c90" id="122" refid="122">
<p> The default domain suffix is <code>cluster.local</code> but can be changed at the cluster level.</p>
</div>
</div>
<div class="readable-text" data-hash="7054828c8f75a4491e03177db9653dea" data-text-hash="33b8b9b35a22ef73a5d647defa02af9b" id="123" refid="123">
<p>The reason you don&#8217;t need to specify the fully qualified domain name (FQDN) when resolving the service through DNS is because of the <code>search</code> line in the pod&#8217;s <code>/etc/resolv.conf</code> file. For the <code>quote-001</code> pod, the file looks like this:</p>
</div>
<div class="browsable-container listing-container" data-hash="7f1d208cc76133ac36dcb7db511be94e" data-text-hash="d2cd9d7036b04d4ec73ce3d5b88cbca6" id="124" refid="124">
<div class="code-area-container">
<pre class="code-area">/ # cat /etc/resolv.conf
search kiada.svc.cluster.local svc.cluster.local cluster.local localdomain
nameserver 10.96.0.10
options ndots:5</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="108c9ec51fa544486e9531766362dc81" data-text-hash="71b7f3122e31b81fc783186b6aabba02" id="125" refid="125">
<p>When you try to resolve a service, the domain names specified in the <code>search</code> field are appended to the name until a match is found. If you&#8217;re wondering what the IP address is in the <code>nameserver</code> line, you can list all the services in your cluster to find out:</p>
</div>
<div class="browsable-container listing-container" data-hash="ad04e25fe253956710f791a3e168ac7d" data-text-hash="bf642d36017ceab6ad3c0edb3f24bea4" id="126" refid="126">
<div class="code-area-container">
<pre class="code-area">$ kubectl get svc -A
NAMESPACE     NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  
default       kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP                  
kiada         quiz         ClusterIP   10.96.136.190   &lt;none&gt;        80/TCP                   
kiada         quote        ClusterIP   10.96.74.151    &lt;none&gt;        80/TCP 
kube-system   kube-dns     ClusterIP   10.96.0.10      &lt;none&gt;        53/UDP...    #A</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgSGVyZeKAmXMgdGhlIElQIGFkZHJlc3MgeW914oCZcmUgbG9va2luZyBmb3Iu"></div>
</div>
</div>
<div class="readable-text" data-hash="f62a0e511b1e00a6a6e82655ab77d716" data-text-hash="b50a16cca4ab05e1215134b3f14c22ba" id="127" refid="127">
<p>The nameserver in the pod&#8217;s <code>resolv.conf</code> file points to the <code>kube-dns</code> service in the <code>kube-system</code> namespace. This is the cluster DNS service that the pods use. As an exercise, try to figure out which pod(s) this service forwards traffic to.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" data-hash="b69231c61bdffa25fe324c6c28ead019" data-text-hash="42ff7416abe637c151137f7eab537843" id="128" refid="128">
<h5>Configuring the pod&#8217;s DNS policy</h5>
</div>
<div class="readable-text" data-hash="1d21100a7079285c3ea996b38ddb0dd7" data-text-hash="0f3e2a5b8d5a52752dc3a42b65082b85" id="129" refid="129">
<p>Whether or not a pod uses the internal DNS server can be configured using the <code>dnsPolicy</code> field in the pod&#8217;s <code>spec</code>. The default value is <code>ClusterFirst</code>, which means that the pod uses the internal DNS first and then the DNS configured for the cluster node. Other valid values are <code>Default</code> (uses the DNS configured for the node), <code>None</code> (no DNS configuration is provided by Kubernetes; you must configure the pod&#8217;s DNS settings using the <code>dnsConfig</code> field explained in the next paragraph), and <code>ClusterFirstWithHostNet</code> (for special pods that use the host&#8217;s network instead of their own - this is explained later in the book).</p>
</div>
<div class="readable-text" data-hash="1a61c7238b6ec833b158a39dc7e6deff" data-text-hash="dd0700560904fcfff39f6dcc0c947cb9" id="130" refid="130">
<p>Setting the <code>dnsPolicy</code> field affects how Kubernetes configures the pod&#8217;s <code>resolv.conf</code> file. You can further customize this file through the pod&#8217;s <code>dnsConfig</code> field. The <code>pod-with-dns-options.yaml</code> file in the book&#8217;s code repository demonstrates the use of this field.</p>
</div>
</div>
<div class="readable-text" data-hash="3e8f46dfe5a8d6a0a78a581afa107739" data-text-hash="6058241dab7596ee386dc02848806df2" id="131" refid="131">
<h4>Discovering services through environment variables</h4>
</div>
<div class="readable-text" data-hash="847961d9dc1625936333c5ae6831cc86" data-text-hash="3257df0461456d4ee5ca295d04a1eec9" id="132" refid="132">
<p>Nowadays, virtually every Kubernetes cluster offers the cluster DNS service. In the early days, this wasn&#8217;t the case. Back then, the pods found the IP addresses of the services using environment variables. These variables still exist today.</p>
</div>
<div class="readable-text" data-hash="9bd7a79968951e5cbd7f6afb190583be" data-text-hash="3960aba39821987a8f346e81b46988c3" id="133" refid="133">
<p>When a container is started, Kubernetes initializes a set of environment variables for each service that exists in the pod&#8217;s namespace. Let&#8217;s see what these environment variables look like by looking at the environment of one of your running pods.</p>
</div>
<div class="readable-text" data-hash="4b5e63a357b9054d3502586becde417b" data-text-hash="f6c8eb30bb5d4b196a804e9060622ac7" id="134" refid="134">
<p>Since you created your pods before the services, you won&#8217;t see any environment variables related to the services except those for the <code>kubernetes</code> service, which exists in the <code>default</code> namespace.</p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="135" refid="135">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="b42293ef0c68baaced81ddbe6c4c6429" data-text-hash="c452d1c88a54356903ce786595a2ea8b" id="136" refid="136">
<p> The <code>kubernetes</code> service forwards traffic to the API server. You&#8217;ll use it in chapter 16.</p>
</div>
</div>
<div class="readable-text" data-hash="cdff38f9b1f146725b965d70ae4d9c1b" data-text-hash="893824363e3bbb6fdd444b114555ff65" id="137" refid="137">
<p>To see the environment variables for the two services that you created, you must restart the container as follows:</p>
</div>
<div class="browsable-container listing-container" data-hash="fed4d28989af922af1e78894e6fc1ee3" data-text-hash="c05f7482aa92c63769800e58fbef9304" id="138" refid="138">
<div class="code-area-container">
<pre class="code-area">$ kubectl exec quote-001 -c nginx -- kill 1</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="27f6649089b0eb3b90b2dffa6a5c5e45" data-text-hash="c615c3a671875a0ef8c2e3589ce20e64" id="139" refid="139">
<p>When the container is restarted, its environment variables contain the entries for the <code>quiz</code> and <code>quote</code> services. Display them with the following command:</p>
</div>
<div class="browsable-container listing-container" data-hash="20e34a41df94859332099fcf29b5a395" data-text-hash="a1e7c458b2f8a628ab74464d7add0549" id="140" refid="140">
<div class="code-area-container">
<pre class="code-area">$ kubectl exec -it quote-001 -c nginx -- env | sort
...
QUIZ_PORT_80_TCP_ADDR=10.96.136.190    #A
QUIZ_PORT_80_TCP_PORT=80    #A
QUIZ_PORT_80_TCP_PROTO=tcp    #A
QUIZ_PORT_80_TCP=tcp://10.96.136.190:80    #A
QUIZ_PORT=tcp://10.96.136.190:80    #A
QUIZ_SERVICE_HOST=10.96.136.190    #A
QUIZ_SERVICE_PORT=80    #A
QUOTE_PORT_80_TCP_ADDR=10.96.74.151    #B
QUOTE_PORT_80_TCP_PORT=80    #B
QUOTE_PORT_80_TCP_PROTO=tcp    #B
QUOTE_PORT_80_TCP=tcp://10.96.74.151:80    #B
QUOTE_PORT=tcp://10.96.74.151:80    #B
QUOTE_SERVICE_HOST=10.96.74.151    #B
QUOTE_SERVICE_PORT=80    #B</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIGVudmlyb25tZW50IHZhcmlhYmxlcyBkZXNjcmliaW5nIHRoZSBxdWl6IHNlcnZpY2UKI0IgVGhlIGVudmlyb25tZW50IHZhcmlhYmxlcyBkZXNjcmliaW5nIHRoZSBxdW90ZSBzZXJ2aWNl"></div>
</div>
</div>
<div class="readable-text" data-hash="3d5237424494bda866afbc7f9c8fd13e" data-text-hash="251936abf679a3b6e754d0e1d2b73a4b" id="141" refid="141">
<p>Quite a handful of environment variables, wouldn&#8217;t you say? For services with multiple ports, the number of variables is even larger. An application running in a container can use these variables to find the IP address and port(s) of a particular service.</p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="260cc6dcef2c22785feb4596e3fe5a61" data-text-hash="10de4bc81f754b19b0d27246a0589c05" id="142" refid="142">
<h5>NOTE</h5>
</div>
<div class="readable-text" data-hash="7491e52c67fa11424ab7c2c937c627cc" data-text-hash="ca2fd27929f3d7067e0ff49e8a024d4a" id="143" refid="143">
<p>&#8195;In the environment variable names, the hyphens in the service name are converted to underscores and all letters are uppercased.</p>
</div>
</div>
<div class="readable-text" data-hash="eb849201a4bceb3ce948b8e3689a219b" data-text-hash="e5a225e910857627f1b7d36068f74935" id="144" refid="144">
<p>Nowadays, applications usually get this information through DNS, so these environment variables aren&#8217;t as useful as in the early days. They can even cause problems. If the number of services in a namespace is too large, any pod you create in that namespace will fail to start. The container exits with exit code 1 and you see the following error message in the container&#8217;s log:</p>
</div>
<div class="browsable-container listing-container" data-hash="31b244d3e3ac3f4712a18cc1a9ee81b7" data-text-hash="9349dea3b95caa3a3882b15ad4900f62" id="145" refid="145">
<div class="code-area-container">
<pre class="code-area">standard_init_linux.go:228: exec user process caused: argument list too long</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="fbb84976d71edfc1d1dc159f452c7b10" data-text-hash="0c69fc0ab2adcaea4952c723784ca310" id="146" refid="146">
<p>To prevent this, you can disable the injection of service information into the environment by setting the <code>enableServiceLinks</code> field in the pod&#8217;s <code>spec</code> to <code>false</code>.</p>
</div>
<div class="readable-text" data-hash="54379827ef4ce5b0e0a3caccfb98c4e3" data-text-hash="cc437a09f979011ab4d2ce3d65bd5e34" id="147" refid="147">
<h4>Understanding why you can&#8217;t ping a service IP</h4>
</div>
<div class="readable-text" data-hash="3d80bb30e9a8618349c299a3efca3c87" data-text-hash="dabce3c55668c761d9e5dc1899d1624e" id="148" refid="148">
<p>You&#8217;ve learned how to verify that a service is forwarding traffic to your pods. But what if it doesn&#8217;t? In that case, you might want to try pinging the service&#8217;s IP. Why don&#8217;t you try that right now? Ping the <code>quiz</code> service from the <code>quote-001</code> pod as follows:</p>
</div>
<div class="browsable-container listing-container" data-hash="d493b2680775c65ff174692345954c62" data-text-hash="662a12afa1f9dea4dfcbd29757258f57" id="149" refid="149">
<div class="code-area-container">
<pre class="code-area">$ kubectl exec -it quote-001 -c nginx -- ping quiz
PING quiz (10.96.136.190): 56 data bytes
^C
--- quiz ping statistics ---
15 packets transmitted, 0 packets received, 100% packet loss
command terminated with exit code 1</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="639998594ee002b62d35727fc20c8aea" data-text-hash="f8a28b79be6a97a198badfd7890a2e8f" id="150" refid="150">
<p>Wait a few seconds and then interrupt the process by pressing Control-C. As you can see, the IP address was resolved correctly, but none of the packets got through. This is because the IP address of the service is virtual and has meaning only in conjunction with one of the ports defined in the service. This is explained in chapter 18, which explains the internal workings of services. For now, remember that you can&#8217;t ping services.</p>
</div>
<div class="readable-text" data-hash="42b54a9a7709a143900cf078d726ffcc" data-text-hash="4764f49f6a6b131a0a4ebaebd2d40416" id="151" refid="151">
<h4>Using services in a pod</h4>
</div>
<div class="readable-text" data-hash="1ee33e57af6d349b3b6f1f443468c417" data-text-hash="53fb365a67382d97fb37d5f71ab7fbaf" id="152" refid="152">
<p>Now that you know that the Quiz and Quote services are accessible from pods, you can deploy the Kiada pods and configure them to use the two services. The application expects the URLs of these services in the environment variables <code>QUIZ_URL</code> and <code>QUOTE_URL</code>. These aren&#8217;t environment variables that Kubernetes adds on its own, but variables that you set manually so that the application knows where to find the two services. Therefore, the <code>env</code> field of the <code>kiada</code> container must be configured as in the following listing.</p>
</div>
<div class="browsable-container listing-container" data-hash="0609fbfcc2271f6074e03cbc4567ecca" data-text-hash="4e460ecafa60b0ccea05a299b1a5393d" id="153" refid="153">
<h5>Listing 11.2 Configuring the service URLs in the kiada pod</h5>
<div class="code-area-container">
<pre class="code-area">...
    env:
    - name: QUOTE_URL    #A
      value: http://quote/quote    #A
    - name: QUIZ_URL    #B
      value: http://quiz    #B
    - name: POD_NAME
      ....</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIFVSTCB3aGVyZSB0aGUgUXVvdGUgc2VydmljZSByZXR1cm5zIGEgcXVvdGUgZnJvbSB0aGUgYm9vay4KI0IgVGhlIGJhc2UgVVJMIG9mIHRoZSBRdWl6IHNlcnZpY2Uu"></div>
</div>
</div>
<div class="readable-text" data-hash="d3cc8a6b3b3bf5bdf4fdad8743baf173" data-text-hash="9734641d24f6fcdb8bd2e29b7d4fbdfc" id="154" refid="154">
<p>The environment variable <code>QUOTE_URL</code> is set to <code>http://quote/quote</code>. The hostname is the same as the name of the service you created in the previous section. Similarly, <code>QUIZ_URL</code> is set to <code>http://quiz</code>, where <code>quiz</code> is the name of the other service you created.</p>
</div>
<div class="readable-text" data-hash="841b09fc9c2839e73027c46a2fa41b16" data-text-hash="5a8be52c6c4a9760779c309d70ce535d" id="155" refid="155">
<p>Deploy the Kiada pods by applying the manifest file <code>kiada-stable-and-canary.yaml</code> to your cluster using <code>kubectl apply</code>. Then run the following command to open a tunnel to one of the pods you just created:</p>
</div>
<div class="browsable-container listing-container" data-hash="971a375a2fd4644b3651901c87475b11" data-text-hash="3509da3b0da09899eb5937e7ce421dd6" id="156" refid="156">
<div class="code-area-container">
<pre class="code-area">$ kubectl port-forward kiada-001 8080 8443</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="ef1df721675aeadcc8d29eb57c1ed459" data-text-hash="aab8a27a6e29d9529e5b2571e09845c8" id="157" refid="157">
<p>You can now test the application at <a href=".html">http://localhost:8080</a> or <a href=".html">https://localhost:8443</a>. If you use <code>curl</code>, you should see a response like the following:</p>
</div>
<div class="browsable-container listing-container" data-hash="b1252a5206fe5b1fb2ee0e48936b0bc9" data-text-hash="a17d9a02eecf574c6e88b227569b0282" id="158" refid="158">
<div class="code-area-container">
<pre class="code-area">$ curl http://localhost:8080
==== TIP OF THE MINUTE
Kubectl options that take a value can be specified with an equal sign or with a space. Instead of -tail=10, you can also type --tail 10.
 
==== POP QUIZ
First question
0) First answer
1) Second answer
2) Third answer
 
Submit your answer to /question/1/answers/&lt;index of answer&gt; using the POST method.
 
==== REQUEST INFO
Request processed by Kubia 1.0 running in pod "kiada-001" on node "kind-worker2".
Pod hostname: kiada-001; Pod IP: 10.244.1.90; Node IP: 172.18.0.2; Client IP: ::ffff:127.0.0.1
 
HTML version of this content is available at /html</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="931f1aa4ce2a0dc6b48ba72dced497df" data-text-hash="bfbfe744c53afa4d55376a17c20670f3" id="159" refid="159">
<p>If you open the URL in your web browser, you get the web page shown in the following figure.</p>
</div>
<div class="browsable-container figure-container" data-hash="78b902a2ae2ae2992a1ef3f12f600dfa" data-text-hash="3fa60e91874f333d5c48f12d6612f352" id="160" refid="160">
<h5>Figure 11.6 The Kiada application when accessed with a web browser</h5>
<img alt="" data-processed="true" height="472" id="Picture_5" loading="lazy" src="EPUB/images/11image007.png" width="855">
</div>
<div class="readable-text" data-hash="2372a922b549a04b224cccda76eb4469" data-text-hash="6a9253787f56cb5ee25582011ca28842" id="161" refid="161">
<p>If you can see the quote and quiz question, it means that the <code>kiada-001</code> pod is able to communicate with the <code>quote</code> and <code>quiz</code> services. If you check the logs of the pods that back these services, you&#8217;ll see that they are receiving requests. In the case of the <code>quote</code> service, which is backed by multiple pods, you&#8217;ll see that each request is sent to a different pod.</p>
</div>
<div class="readable-text" data-hash="b5497475f914c0a0d069db1a8bcf5369" data-text-hash="ed17ba65892db5e20382bb7f8cf7aec9" id="162" refid="162">
<h2 id="sigil_toc_id_193">11.2&#160;Exposing services externally</h2>
</div>
<div class="readable-text" data-hash="aa9f291dceb2c54488958cb4e558a67b" data-text-hash="fe4ab19f84c30bc9f35942b80d94051c" id="163" refid="163">
<p>ClusterIP services like the ones you created in the previous section are only accessible within the cluster. Because clients must be able to access the Kiada service from outside the cluster, as shown in the next figure, creating a ClusterIP service won&#8217;t suffice.</p>
</div>
<div class="browsable-container figure-container" data-hash="c97fdfd9c59b7082cc6035d33426ba40" data-text-hash="eb72fc0216e685a12fc73076593a8c0e" id="164" refid="164">
<h5>Figure 11.7 Exposing a service externally</h5>
<img alt="" data-processed="true" height="283" id="Picture_6" loading="lazy" src="EPUB/images/11image008.png" width="859">
</div>
<div class="readable-text" data-hash="cf7bb7d81c6ee2159b8e903a15ece905" data-text-hash="e6c758c262cb5c0bb3bbf97c5e46e3b8" id="165" refid="165">
<p>If you need to make a service available to the outside world, you can do one of the following:</p>
</div>
<ul>
<li class="readable-text" data-hash="7c9928b55176a15413501ab2065eea75" data-text-hash="8cdce75f4ea0b1458cca0f12425dce60" id="166" refid="166">assign an additional IP to a node and set it as one of the service&#8217;s <code class="codechar">externalIPs</code>,</li>
<li class="readable-text" data-hash="93c3666b74fe16c765837d98c7060402" data-text-hash="c07630ebbe1994a11f321162db9360eb" id="167" refid="167">set the service&#8217;s type to <code>NodePort</code> and access the service through the node&#8217;s port(s),</li>
<li class="readable-text" data-hash="db4e9d3a13797d297d197850cd4bf37c" data-text-hash="2ed966990471d152757759d694d832a2" id="168" refid="168">ask Kubernetes to provision a load balancer by setting the type to <code>LoadBalancer</code>, or</li>
<li class="readable-text" data-hash="e91b84b37abf26709a20bc4f3aef7c13" data-text-hash="e91b84b37abf26709a20bc4f3aef7c13" id="169" refid="169">expose the service through an Ingress object.</li>
</ul>
<div class="readable-text" data-hash="7bc3a0923c00cbdf50cba7f98aab816f" data-text-hash="23a6b4bc721dc96c8ffcb84b529dfd6b" id="170" refid="170">
<p>A rarely used method is to specify an additional IP in the <code>spec.externalIPs</code> field of the Service object. By doing this, you&#8217;re telling Kubernetes to treat any traffic directed to that IP address as traffic to be processed by the service. When you ensure that this traffic arrives at a node with the service&#8217;s external IP as its destination, Kubernetes forwards it to one of the pods that back the service.</p>
</div>
<div class="readable-text" data-hash="999218bfb123b7fc1cf724b272204351" data-text-hash="c28bf4326507e4901a4de1ddfcb6e3c7" id="171" refid="171">
<p>A more common way to make a service available externally is to set its type to <code>NodePort</code>. Kubernetes makes the service available on a network port on all cluster nodes (the so-called node port, from which this service type gets its name). Like <code>ClusterIP</code> services, the service gets an internal cluster IP, but is also accessible through the node port on each of the cluster nodes. Usually, you then provision an external load balancer that redirects traffic to these node ports. The clients can connect to your service via the load balancer&#8217;s IP address.</p>
</div>
<div class="readable-text" data-hash="1d69f3a5358e4c8e72fc4eb944d832c2" data-text-hash="130e6edc317933dae1a2558d4991ee4b" id="172" refid="172">
<p>Instead of using a <code>NodePort</code> service and manually setting up the load balancer, Kubernetes can also do this for you if you set the service type to <code>LoadBalancer</code>. However, not all clusters support this service type, as the provisioning of the load balancer depends on the infrastructure the cluster is running on. Most cloud providers support LoadBalancer services in their clusters, whereas clusters deployed on premises require an add-on such as MetalLB, a load-balancer implementation for bare-metal Kubernetes clusters.</p>
</div>
<div class="readable-text" data-hash="53e45772d0f2eb90764dc3995dc4e0b7" data-text-hash="cfaa610237659ab6fbe4bd309b6d186b" id="173" refid="173">
<p>The final way to expose a group of pods externally is radically different. Instead of exposing the service externally via node ports and load balancers, you can use an Ingress object. How this object exposes the service depends on the underlying ingress controller, but it allows you to expose many services through a single externally reachable IP address. You&#8217;ll learn more about this in the next chapter.</p>
</div>
<div class="readable-text" data-hash="197387bca2cabda721bad37c479881c9" data-text-hash="3a603e6ac6d58c8754bd4577fdf756ad" id="174" refid="174">
<h3 id="sigil_toc_id_194">11.2.1&#160;Exposing pods through a NodePort service</h3>
</div>
<div class="readable-text" data-hash="0ebc9e53a8de661689c35731465b2113" data-text-hash="a665897aa52261c0a06db1c021633b0c" id="175" refid="175">
<p>One way to make pods accessible to external clients is to expose them through a <code>NodePort</code> service. When you create such a service, the pods that match its selector are accessible through a specific port on all nodes in the cluster, as shown in the following figure. Because this port is open on the nodes, it&#8217;s called a node port.</p>
</div>
<div class="browsable-container figure-container" data-hash="0a6ab85ce9eb08782d669a81b3c69e8c" data-text-hash="47304e6adf8350cd91c2198f1aacb53f" id="176" refid="176">
<h5>Figure 11.8 Exposing pods through a NodePort service</h5>
<img alt="" data-processed="true" height="329" id="Picture_7" loading="lazy" src="EPUB/images/11image009.png" width="851">
</div>
<div class="readable-text" data-hash="977c2485f9f85ea52286dab4ca44fd14" data-text-hash="12f59e3069dd5ef9615b80e0193e558d" id="177" refid="177">
<p>Like a <code>ClusterIP</code> service, a <code>NodePort</code> service is accessible through its internal cluster IP, but also through the node port on each of the cluster nodes. In the example shown in the figure, the pods are accessible through port <code>30080</code>. As you can see, this port is open on both cluster nodes.</p>
</div>
<div class="readable-text" data-hash="4b97619cc7d55ff79c5c2592018bd64b" data-text-hash="5f6e3f18a66a2b881f61f3d768d3a573" id="178" refid="178">
<p>It doesn&#8217;t matter which node a client connects to because all the nodes will forward the connection to a pod that belongs to the service, regardless of which node is running the pod. When the client connects to node A, a pod on either node A or B can receive the connection. The same is true when the client connects to the port on node B.</p>
</div>
<div class="readable-text" data-hash="518317ad846ed21efc4533c4fea58b09" data-text-hash="49f156cf1245c6bdc9c06312da125f49" id="179" refid="179">
<h4>Creating a NodePort service</h4>
</div>
<div class="readable-text" data-hash="05c4f5d36a8ee7ebebbf7c515adcb5d2" data-text-hash="c187fbda6b11079b747c7038daf9aa7c" id="180" refid="180">
<p>To expose the kiada pods through a <code>NodePort</code> service, you create the service from the manifest shown in the following listing.</p>
</div>
<div class="browsable-container listing-container" data-hash="de6ed7eb2cbeeac2cea878907ed9794c" data-text-hash="193ca097cb27aaf4d1d3b940eceba532" id="181" refid="181">
<h5>Listing 11.3 A NodePort service exposing the kiada pods on two ports</h5>
<div class="code-area-container">
<pre class="code-area">apiVersion: v1
kind: Service
metadata:
  name: kiada
spec:
  type: NodePort    #A
  selector:
    app: kiada
  ports:
  - name: http    #B
    port: 80    #C
    nodePort: 30080    #D
    targetPort: 8080    #E
  - name: https    #F
    port: 443    #F
    nodePort: 30443    #F
    targetPort: 8443    #F</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIHNlcnZpY2UgdHlwZSBpcyBOb2RlUG9ydC4KI0IgVGhlIHNlcnZpY2UgZXhwb3NlcyB0d28gcG9ydHMuIFRoaXMgaGVyZSBpcyB0aGUgSFRUUCBwb3J0LgojQyBUaGUgcG9ydCBvbiB0aGUgc2VydmljZeKAmXMgY2x1c3RlciBJUC4KI0QgVGhlIHNlcnZpY2UgaXMgYWNjZXNzaWJsZSB0aHJvdWdoIHBvcnQgMzAwODAgb2YgZWFjaCBvZiB5b3VyIGNsdXN0ZXIgbm9kZXMuCiNFIFRoaXMgaXMgdGhlIHBvcnQgdGhhdCB0aGUgcG9kcyBsaXN0ZW4gb24uCiNGIFRoZSBzZXJ2aWNlIGV4cG9zZXMgYW5vdGhlciBwb3J0IGZvciBIVFRQUy4="></div>
</div>
</div>
<div class="readable-text" data-hash="0639d751db268c5fcf4e4d2058b993f1" data-text-hash="23d09d7110c5955e1f61108999617e5d" id="182" refid="182">
<p>Compared to the <code>ClusterIP</code> services you created earlier the type of service in the listing is <code>NodePort</code>. Unlike the previous services, this service exposes two ports and defines the <code>nodePort</code> numbers for each of those ports.</p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="183" refid="183">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="b12824d7e074e264b7e711cbb1bc107a" data-text-hash="0a3f34048142f0e23e8502a34b2d4834" id="184" refid="184">
<p> You can omit the <code>nodePort</code> field to allow Kubernetes to assign the port number. This prevents port conflicts between different NodePort services.</p>
</div>
</div>
<div class="readable-text" data-hash="ef0ee45f1bb6aee7137b0912fa4be715" data-text-hash="a796b1b6d32a3c667fe4297d73b3148d" id="185" refid="185">
<p>The service specifies six different port numbers, which might make it difficult to understand, but the following figure should help you make sense of it.</p>
</div>
<div class="browsable-container figure-container" data-hash="198324786880bff44ecfa3b0144a87c4" data-text-hash="600ea17ae8a6eabe038ccb4afd176597" id="186" refid="186">
<h5>Figure 11.9 Exposing multiple ports through with a NodePort service</h5>
<img alt="" data-processed="true" height="233" id="Picture_8" loading="lazy" src="EPUB/images/11image010.png" width="847">
</div>
<div class="readable-text" data-hash="e4cf2166afcaa8f30350b4376809e19f" data-text-hash="5a5728e057ae42dab5c5131618089501" id="187" refid="187">
<h4>Examining your NodePort service</h4>
</div>
<div class="readable-text" data-hash="73283510cf02ea137cd6597b7e70c9ec" data-text-hash="04a69a352bbc13e1e178d7521617f2fa" id="188" refid="188">
<p>After you create the service, inspect it with the <code>kubectl get</code> command as follows:</p>
</div>
<div class="browsable-container listing-container" data-hash="4e4bb478045d52f11bf56878111450e4" data-text-hash="c8ff1ab94349fbbf538e6858177277e6" id="189" refid="189">
<div class="code-area-container">
<pre class="code-area">$ kubectl get svc
NAME    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
kiada   NodePort    10.96.226.212   &lt;none&gt;        80:30080/TCP,443:30443/TCP   1m    #A
quiz    ClusterIP   10.96.173.186   &lt;none&gt;        80/TCP                       3h
quote   ClusterIP   10.96.161.97    &lt;none&gt;        80/TCP                       3h</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhpcyBpcyB0aGUgc2VydmljZSB5b3UgY3JlYXRlZA=="></div>
</div>
</div>
<div class="readable-text" data-hash="3795ec1e14c0f732009a525c276fb90e" data-text-hash="2df0bf69947f630f3acd04d3f32e1144" id="190" refid="190">
<p>Compare the <code>TYPE</code> and <code>PORT(S)</code> columns of the services you&#8217;ve created so far. Unlike the two <code>ClusterIP</code> services, the <code>kiada</code> service is a <code>NodePort</code> service that exposes node ports <code>30080</code> and <code>30443</code> in addition to ports <code>80</code> and <code>443</code> available on the service&#8217;s cluster IP.</p>
</div>
<div class="readable-text" data-hash="c10122b835567cf0e7d7082168b9513e" data-text-hash="4371c376b3c881d7ef26bb0e60857505" id="191" refid="191">
<h4>Accessing a NodePort service</h4>
</div>
<div class="readable-text" data-hash="6ccf74b5d57c6099f06e299ffa8cd770" data-text-hash="a22905ed9721bbc4a151cd532ca94604" id="192" refid="192">
<p>To find out all <code>IP:port</code> combinations over which the service is available, you need not only the node port number(s), but also the IPs of the nodes. You can get these by running <code>kubectl get nodes -o wide</code> and looking at the <code>INTERNAL-IP</code> and <code>EXTERNAL-IP</code> columns. Clusters running in the cloud usually have the external IP set for the nodes, whereas clusters running on bare metal may set only the internal IP of the nodes. You should be able to reach the node ports using these IPs, if there are no firewalls in the way.</p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="193" refid="193">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="43d1a8821bdd65502d665739a1471641" data-text-hash="c149621aca2c8dd715b1aa2690585dc2" id="194" refid="194">
<p> To allow traffic to node ports when using GKE, run <code>gcloud compute firewall-rules create gke-allow-nodeports --allow=tcp:30000-32767</code>. If your cluster is running on a different cloud provider, check the provider&#8217;s documentation on how to configure the firewall to allow access to node ports.</p>
</div>
</div>
<div class="readable-text" data-hash="94c3fcffa11ecbe52f1ccffd6794b810" data-text-hash="d822e1c421d520423524e7381bc84647" id="195" refid="195">
<p>In the cluster I provisioned with the kind tool, the internal IPs of the nodes are as follows:</p>
</div>
<div class="browsable-container listing-container" data-hash="eafb797c9c80a178785e128f0d7dfdf0" data-text-hash="de2e248e1f7d5c49e73ade6565a4455e" id="196" refid="196">
<div class="code-area-container">
<pre class="code-area">$ kubectl get nodes -o wide
NAME                 STATUS   ROLES                  ...   INTERNAL-IP   EXTERNAL-IP   
kind-control-plane   Ready    control-plane,master   ...   172.18.0.3    &lt;none&gt; 
kind-worker          Ready    &lt;none&gt;                 ...   172.18.0.4    &lt;none&gt;
kind-worker2         Ready    &lt;none&gt;                 ...   172.18.0.2    &lt;none&gt;</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="1b55b56195d959ff4f7747e5ec815846" data-text-hash="33d4e606590239da351178236a0c5191" id="197" refid="197">
<p>The <code>kiada</code> service is available on all these IPs, even the IP of the node running the Kubernetes control plane. I can access the service at any of the following URLs:</p>
</div>
<ul>
<li class="readable-text" data-hash="49deb857fead07017efd5329b8308e0a" data-text-hash="367327d9690b0a39bdd57a8850d93647" id="198" refid="198"><code class="codechar">10.96.226.212:80</code> within the cluster (this is the cluster IP and the internal port),</li>
<li class="readable-text" data-hash="4ca00f56b888b51387fcaed2a745242f" data-text-hash="2795495c32e4492edf4645d85f191dd6" id="199" refid="199"><code>172.18.0.3:30080</code> from wherever the node <code>kind-control-plane</code> is reachable, as this is the node&#8217;s IP address; the port is one of the node ports of the <code>kiada</code> service,</li>
<li class="readable-text" data-hash="c0904e1e86d8e9498a0723c0d24285b0" data-text-hash="d1e19d64fdae8830f5b5b70ab495f8c3" id="200" refid="200"><code>172.18.0.4:30080</code> (the second node&#8217;s IP address and the node port), and</li>
<li class="readable-text" data-hash="31e6ebea2e9bb0b30f58fe07cb5847d5" data-text-hash="020dacd46ea2bb7dcf615b2f862c1cd0" id="201" refid="201"><code>172.18.0.2:30080</code> (the third node&#8217;s IP address and the node port).</li>
</ul>
<div class="readable-text" data-hash="c5376f8dfb126ae9963776cff3f7999e" data-text-hash="cb9fc69139fc2aa3f2ce775c7ee26f11" id="202" refid="202">
<p>The service is also accessible via HTTPS on port <code>443</code> within the cluster and via node port <code>30443</code>. If my nodes also had external IPs, the service would also be available through the two node ports on those IPs. If you&#8217;re using Minikube or another single-node cluster, you should use the IP of that node.</p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="5c622e940054ac4ab45712e2d7b5d25d" data-text-hash="12ae2a12586001e30745cb0457586ae3" id="203" refid="203">
<h5>Tip</h5>
</div>
<div class="readable-text" data-hash="cade99f6801cd531fe0b2ca54ef377c6" data-text-hash="639ea44aa435c825733a7f78495a7e9e" id="204" refid="204">
<p>&#8195;If you&#8217;re using Minikube, you can easily access your <code>NodePort</code> services through your browser by running <code>minikube</code> <code>service</code> <code>&lt;service-name&gt;</code> <code>[-n</code> <code>&lt;namespace&gt;]</code>.</p>
</div>
</div>
<div class="readable-text" data-hash="b55debd380b9c6987a9cdd14c0d58d47" data-text-hash="db9519ab7061ea79e90c11bd3b9cd5af" id="205" refid="205">
<p>Use <code>curl</code> or your web browser to access the service. Select one of the nodes and find its IP address. Send the HTTP request to port 30080 of this IP. Check the end of the response to see which pod handled the request and which node the pod is running on. For example, here&#8217;s the response I received to one of my requests:</p>
</div>
<div class="browsable-container listing-container" data-hash="f6e260773ab0bf76051ce01e313f1f2b" data-text-hash="28caa48b0bcf4b8c421c1dfe28f0da67" id="206" refid="206">
<div class="code-area-container">
<pre class="code-area">$ curl 172.18.0.4:30080
...
==== REQUEST INFO
Request processed by Kubia 1.0 running in pod "kiada-001" on node "kind-worker2".
Pod hostname: kiada-001; Pod IP: 10.244.1.90; Node IP: 172.18.0.2; Client IP: ::ffff:172.18.0.4</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="d3dac6088b183876aa70d6eca677b31d" data-text-hash="b8ecfe0a9ee8d2c63101c74aca841796" id="207" refid="207">
<p>Notice that I sent the request to the <code>172.18.0.4</code>, which is the IP of the <code>kind-worker</code> node, but the pod that handled the request was running on the node <code>kind-worker2</code>. The first node forwarded the connection to the second node, as explained in the introduction to NodePort services.</p>
</div>
<div class="readable-text" data-hash="42f54de3bd9b432b005f2fb985cf9b7d" data-text-hash="f7c8647755f37a03d12687ffe425d987" id="208" refid="208">
<p>Did you also notice where the pod thought the request came from? Look at the <code>Client IP</code> at the end of the response. That&#8217;s not the IP of the computer from which I sent the request. You may have noticed that it&#8217;s the IP of the node I sent the request to. I explain why this is and how you can prevent it in section 11.2.3.</p>
</div>
<div class="readable-text" data-hash="0d2b69adb192bc037df5ecbd9a9cd929" data-text-hash="06c2e18d1216b7c6e96252c239e3a264" id="209" refid="209">
<p>Try sending the request to the other nodes as well. You&#8217;ll see that they all forward the requests to a random kiada pod. If your nodes are reachable from the internet, the application is now accessible to users all over the world. You could use round robin DNS to distribute incoming connections across the nodes or put a proper Layer 4 load balancer in front of the nodes and point the clients to it. Or you could just let Kubernetes do this, as explained in the next section.</p>
</div>
<div class="readable-text" data-hash="85fc8974518d00492012052874a4e2f5" data-text-hash="178c5ebdea65f19d01f7247cff78ee84" id="210" refid="210">
<h3 id="sigil_toc_id_195">11.2.2&#160;Exposing a service through an external load balancer</h3>
</div>
<div class="readable-text" data-hash="46d9495aa715074c0fd4a247da94f638" data-text-hash="f574a755f7c71ea3d43e033537d95a26" id="211" refid="211">
<p>In the previous section, you created a service of type <code>NodePort</code>. Another service type is <code>LoadBalancer</code>. As the name suggests, this service type makes your application accessible through a load balancer. While all services act as load balancers, creating a <code>LoadBalancer</code> service causes an actual load balancer to be provisioned.</p>
</div>
<div class="readable-text" data-hash="42ebb7cce984bbec84f9598bd110a2c0" data-text-hash="c28d58084091bbda8d0639ea3e6fa32d" id="212" refid="212">
<p>As shown in the following figure, this load balancer stands in front of the nodes and handles the connections coming from the clients. It routes each connection to the service by forwarding it to the node port on one of the nodes. This is possible because the <code>LoadBalancer</code> service type is an extension of the <code>NodePort</code> type, which makes the service accessible through these node ports. By pointing clients to the load balancer rather than directly to the node port of a particular node, the client never attempts to connect to an unavailable node because the load balancer forwards traffic only to healthy nodes. In addition, the load balancer ensures that connections are distributed evenly across all nodes in the cluster.</p>
</div>
<div class="browsable-container figure-container" data-hash="2a6951eeada869faabd0c1d9dff6124b" data-text-hash="5c220bdb2e9e40a3ce7c43990107c14e" id="213" refid="213">
<h5>Figure 11.10 Exposing a LoadBalancer service</h5>
<img alt="" data-processed="true" height="432" id="Picture_9" loading="lazy" src="EPUB/images/11image011.png" width="847">
</div>
<div class="readable-text" data-hash="5181250597bd099a75997081907e9738" data-text-hash="58dcd344a71739a8f8147ce76b1029a5" id="214" refid="214">
<p>Not all Kubernetes clusters support this type of service, but if your cluster runs in the cloud, it almost certainly does. If your cluster runs on premises, it&#8217;ll support <code>LoadBalancer</code> services if you install an add-on. If the cluster doesn&#8217;t support this type of service, you can still create services of this type, but the service is only accessible through its node ports.</p>
</div>
<div class="readable-text" data-hash="905033a1c9b19ddd2e045f5c2ead8559" data-text-hash="1760fab65ab1bec908ac91d4f64cf8f6" id="215" refid="215">
<h4>Creating a LoadBalancer service</h4>
</div>
<div class="readable-text" data-hash="8aa7022f9c781061d7c816397897541f" data-text-hash="4333a7a2964aa4b189d621f65ec96326" id="216" refid="216">
<p>The manifest in the following listing contains the definition of a <code>LoadBalancer</code> service.</p>
</div>
<div class="browsable-container listing-container" data-hash="4d669d0ea6fa082664aaf7365c98d3a9" data-text-hash="eb63ca26cf8ee42199259c9e79983226" id="217" refid="217">
<h5>Listing 11.4 A <code class="codechar">LoadBalancer</code>-type service</h5>
<div class="code-area-container">
<pre class="code-area">apiVersion: v1
kind: Service
metadata:
  name: kiada
spec:
  type: LoadBalancer    #A
  selector:
    app: kiada
  ports:
  - name: http
    port: 80
    nodePort: 30080
    targetPort: 8080
  - name: https
    port: 443
    nodePort: 30443
    targetPort: 8443</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgS3ViZXJuZXRlcyB3aWxsIHByb3Zpc2lvbiBhIGxvYWQgYmFsYW5jZXIgZm9yIHRoaXMgc2VydmljZS4="></div>
</div>
</div>
<div class="readable-text" data-hash="6f1b5f25803cdf5d1c2f8e42f93b4e93" data-text-hash="00ddc86a1a1d5633a6e6e0841d753b7c" id="218" refid="218">
<p>This manifest differs from the manifest of the <code>NodePort</code> service you deployed earlier in only one line - the line that specifies the service <code>type</code>. The selector and ports are the same as before. The node ports are only specified so that they aren&#8217;t randomly selected by Kubernetes. If you don&#8217;t care about the node port numbers, you can omit the <code>nodePort</code> fields.</p>
</div>
<div class="readable-text" data-hash="3f9b1e86dc8339aa1468cf7611ccb76a" data-text-hash="f02f4900f619452737f2fb1a7dad8253" id="219" refid="219">
<p>Apply the manifest with <code>kubectl apply</code>. You don&#8217;t have to delete the existing <code>kiada</code> service first. This ensures that the internal cluster IP of the service remains unchanged.</p>
</div>
<div class="readable-text" data-hash="89938e6b30307160579eabb892391888" data-text-hash="3e986920f07eaa707445a09450881f57" id="220" refid="220">
<h4>Connecting to the service through the load balancer</h4>
</div>
<div class="readable-text" data-hash="b7b091a651ae4224da8423b2811ba363" data-text-hash="534fd515f29e9eac683629a4445ddea2" id="221" refid="221">
<p>After you create the service, it may take a few minutes for the cloud infrastructure to create the load balancer and update its IP address in the Service object. This IP address will then appear as the external IP address of your service:</p>
</div>
<div class="browsable-container listing-container" data-hash="55743a9dee51da6a95f63c851a6f5a5e" data-text-hash="a378d835fb7fa948cced5b4daeca9b7f" id="222" refid="222">
<div class="code-area-container">
<pre class="code-area">$ kubectl get svc kiada
NAME    TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)                       AGE
kiada   LoadBalancer   10.96.226.212   172.18.255.200   80:30080/TCP,443:30443/TCP    10m</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="b894d9b92f833b5080c6beafe6a114e7" data-text-hash="39a2e0c297596eb18da46631aff3f0e2" id="223" refid="223">
<p>In my case, the IP address of the load balancer is <code>172.18.255.200</code> and I can reach the service through port <code>80</code> and <code>443</code> of this IP. Until the load balancer is created, <code>&lt;pending&gt;</code> is displayed in the <code>EXTERNAL-IP</code> column instead of an IP address. This could be because the provisioning process isn&#8217;t yet complete or because the cluster doesn&#8217;t support <code>LoadBalancer</code> services.</p>
</div>
<div class="readable-text" data-hash="19bfbe9359b57c7e5601f742b1de4f9e" data-text-hash="a3f398ce4bb8ffc9af15be919966068b" id="224" refid="224">
<h4>Adding support for LoadBalancer services with MetalLB</h4>
</div>
<div class="readable-text" data-hash="3018dd3bf1533141f4f6eb532edb3e9a" data-text-hash="ad1d455bbc21748b0c10aab4f7013765" id="225" refid="225">
<p>If your cluster runs on bare metal, you can install MetalLB to support <code>LoadBalancer</code> services. You can find it at <a href=".html"><span>metallb.universe.tf</span></a>. If you created your cluster with the kind tool, you can install MetalLB using the <code>install-metallb-kind.sh</code> script from the book&#8217;s code repository. If you created your cluster with another tool, you can check the MetalLB documentation for how to install it.</p>
</div>
<div class="readable-text" data-hash="e2eb11e5a62396e3eda59cc4243885bf" data-text-hash="fa4154e5cc3a0c2539148d85bb9db1dd" id="226" refid="226">
<p>Adding support for LoadBalancer services is optional. You can always use the node ports directly. The load balancer is just an additional layer.</p>
</div>
<div class="readable-text" data-hash="d4cc11247f39b7715a412645329ec155" data-text-hash="3888e491b2b608f92e2a1346942c01ee" id="227" refid="227">
<h4>Tweaking LoadBalancer services</h4>
</div>
<div class="readable-text" data-hash="62f8ea0a0c20c44035dfc2b66f499cbc" data-text-hash="3ffdad537de94c41638ffa77bc8f3bcf" id="228" refid="228">
<p>LoadBalancer services are easy to create. You just set the <code>type</code> to <code>LoadBalancer</code>. However, if you need more control over the load balancer, you can configure it with the additional fields in the Service object&#8217;s <code>spec</code> explained in the following table.</p>
</div>
<div class="browsable-container" data-hash="3d8318e0e79cfeef178d2431d48fb798" data-text-hash="c29df888e463d89a9bc3861289f4a7f4" id="229" refid="229">
<h5>Table 11.2 Fields in the service spec that you can use to configure LoadBalancer services</h5>
<table border="1" cellpadding="0" cellspacing="0" width="100%">
<tbody>
<tr>
<td> <p>Field</p> </td>
<td> <p>Field type</p> </td>
<td> <p>Description</p> </td>
</tr>
<tr>
<td> <p></p><pre>loadBalancerClass
</pre> </td>
<td> <p></p><pre>string
</pre> </td>
<td> <p>If the cluster supports multiple classes of load balancers, you can specify which one to use for this service. The possible values depend on the load balancer controllers installed in the cluster.</p> </td>
</tr>
<tr>
<td> <p></p><pre>loadBalancerIP
</pre> </td>
<td> <p></p><pre>string
</pre> </td>
<td> <p>If supported by the cloud provider, this field can be used to specify the desired IP for the load balancer.</p> </td>
</tr>
<tr>
<td> <p></p><pre>loadBalancerSourceRanges
</pre> </td>
<td> <p></p><pre>[]string
</pre> </td>
<td> <p>Restricts the client IPs that are allowed to access the service through the load balancer. Not supported by all load balancer controllers.</p> </td>
</tr>
<tr>
<td> <p></p><pre>allocateLoadBalancerNodePorts
</pre> </td>
<td> <p></p><pre>boolean
</pre> </td>
<td> <p>Specifies whether to allocate node ports for this <code>LoadBalancer</code>-type service. Some load balancer implementations can forward traffic to pods without relying on node ports.</p> </td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" data-hash="fe426a41375fbe3ca14f3ecf91ac529a" data-text-hash="ce0512b57c39bac485ef07444613d8cd" id="230" refid="230">
<h3 id="sigil_toc_id_196">11.2.3&#160;Configuring the external traffic policy for a service</h3>
</div>
<div class="readable-text" data-hash="380219bc5e941fd02da564b3f81f4255" data-text-hash="a7698afd12f089830608b9efea42b4db" id="231" refid="231">
<p>You&#8217;ve already learned that when an external client connects to a service through the node port, either directly or through the load balancer, the connection may be forwarded to a pod that&#8217;s on a different node than the one that received the connection. In this case, an additional network hop must be made to reach the pod. This results in increased latency.</p>
</div>
<div class="readable-text" data-hash="0c430b72e662d617c56ee47e14f4bf57" data-text-hash="d1ea519c85ee92f8bc229a54b50f11a2" id="232" refid="232">
<p>Also, as mentioned earlier, when forwarding the connection from one node to another in this manner, the source IP must be replaced with the IP of the node that originally received the connection. This obscures the IP address of the client. Thus, the application running in the pod can&#8217;t see where the connection is coming from. For example, a web server running in a pod can&#8217;t record the true client IP in its access log.</p>
</div>
<div class="readable-text" data-hash="2bc9ec588f60e11a6a7c2df741e07792" data-text-hash="41b7b0f2b51cc3389f65fedfb160fc3c" id="233" refid="233">
<p>The reason the node needs to change the source IP is to ensure that the returned packets are sent back to the node that originally received the connection so that it can return them to the client.</p>
</div>
<div class="readable-text" data-hash="ec581f31a93e122d2a953e91672810a1" data-text-hash="8d29b1d443ecb1821ad67b7b808bf4b3" id="234" refid="234">
<h4>Pros and cons of the Local external traffic policy</h4>
</div>
<div class="readable-text" data-hash="49a4af5c80e488cc20f36974a37ed256" data-text-hash="dd19184187054f7ece1bbd99900757df" id="235" refid="235">
<p>Both the additional network hop problem and the source IP obfuscation problem can be solved by preventing nodes from forwarding traffic to pods that aren&#8217;t running on the same node. This is done by setting the <code>externalTrafficPolicy</code> field in the Service object&#8217;s <code>spec</code> field to <code>Local</code>. This way, a node forwards external traffic only to pods running on the node that received the connection.</p>
</div>
<div class="readable-text" data-hash="4d3d9f5b0e16d67a4a49d24ed25385ef" data-text-hash="12db19697afb90c6ab542cbd3ae97727" id="236" refid="236">
<p>However, setting the external traffic policy to <code>Local</code> leads to other problems. First, if there are no local pods on the node that received the connection, the connection hangs. You must therefore ensure that the load balancer forwards connections only to nodes that have at least one such pod. This is done using the <code>healthCheckNodePort</code> field. The external load balancer uses this node port to check whether a node contains endpoints for the service or not. This allows the load balancer to forward traffic only to nodes that have such a pod.</p>
</div>
<div class="readable-text" data-hash="f3114cb3a5f67be4215b8b1546acca94" data-text-hash="0707d7e9672d9009ab6b7d5c2f4f7671" id="237" refid="237">
<p>The second problem you run into when the external traffic policy is set to <code>Local</code> is the uneven distribution of traffic across pods. If the load balancers distribute traffic evenly among the nodes, but each node runs a different number of pods, the pods on the nodes with fewer pods will receive a higher amount of traffic.</p>
</div>
<div class="readable-text" data-hash="db095e7f9287952d2cb3e2f3e196a4a6" data-text-hash="4166d5548bf7b7b402469982f3845172" id="238" refid="238">
<h4>Comparing the Cluster and the Local external traffic policies</h4>
</div>
<div class="readable-text" data-hash="f53f97aa16c3e6d654f5a02383f0baff" data-text-hash="c39f56fde857a5f0218e7ec3a888d9fd" id="239" refid="239">
<p>Consider the case presented in the following figure. There&#8217;s one pod running on node A and two on node B. The load balancer routes half of the traffic to node A and the other half to node B.</p>
</div>
<div class="browsable-container figure-container" data-hash="1507a2bc2140cf6741faf6ca7144ffb6" data-text-hash="cd6fa90aa8ee3814f190300611eab1eb" id="240" refid="240">
<h5>Figure 11.11 Understanding the two external traffic policies for NodePort and LoadBalancer services</h5>
<img alt="" data-processed="true" height="473" id="Picture_10" loading="lazy" src="EPUB/images/11image012.png" width="858">
</div>
<div class="readable-text" data-hash="b174f9eaaafa0e1bc201b14b94113304" data-text-hash="0a68ee56888a1db2f5cfb38127c304cc" id="241" refid="241">
<p>When <code>externalTrafficPolicy</code> is set to <code>Cluster</code>, each node forwards traffic to all pods in the system. Traffic is split evenly between the pods. Additional network hops are required, and the client IP is obfuscated.</p>
</div>
<div class="readable-text" data-hash="9f74544b39c4c42da29b712ec8b4405d" data-text-hash="43f4d951c2ce5a6cb4cf1db048a6d3b3" id="242" refid="242">
<p>When the <code>externalTrafficPolicy</code> is set to <code>Local</code>, all traffic arriving at node A is forwarded to the single pod on that node. This means that this pod receives 50% of all traffic. Traffic arriving at node B is split between two pods. Each pod receives 25% of the total traffic processed by the load balancer. There are no unnecessary network hops, and the source IP is that of the client.</p>
</div>
<div class="readable-text" data-hash="e05d52b5090029c8d3c4ab2ba39e04a1" data-text-hash="67cd49f9806c1654be3b71de990bb914" id="243" refid="243">
<p>As with most decisions you make as an engineer, which external traffic policy to use in each service depends on what tradeoffs you&#8217;re willing to make.</p>
</div>
<div class="readable-text" data-hash="127b3ca3a205ae31a4d7c76d525ee14a" data-text-hash="a2405c452ea0dfc075e98ffa047527a4" id="244" refid="244">
<h2 id="sigil_toc_id_197">11.3&#160;Managing service endpoints</h2>
</div>
<div class="readable-text" data-hash="37dad92d02d937da9d62b3827348c928" data-text-hash="66e6f8e4220fde62d9382251eecc7702" id="245" refid="245">
<p>So far you&#8217;ve learned that services are backed by pods, but that&#8217;s not always the case. The endpoints to which a service forwards traffic can be anything that has an IP address.</p>
</div>
<div class="readable-text" data-hash="5f472a72eeaf08794f48ea96b280935f" data-text-hash="4ed22d95f6d99bf2ea4e6c9c508177cc" id="246" refid="246">
<h3 id="sigil_toc_id_198">11.3.1&#160;Introducing the Endpoints object</h3>
</div>
<div class="readable-text" data-hash="32a5a32f8337d0c9ebbdeda3b63a4382" data-text-hash="d550331b9e9c04bdc8053ca948d427d2" id="247" refid="247">
<p>A service is typically backed by a set of pods whose labels match the label selector defined in the Service object. Apart from the label selector, the Service object&#8217;s <code>spec</code> or <code>status</code> section doesn&#8217;t contain the list of pods that are part of the service. However, if you use <code>kubectl describe</code> to inspect the service, you&#8217;ll see the IPs of the pods under <code>Endpoints</code>, as follows:</p>
</div>
<div class="browsable-container listing-container" data-hash="63dae721a2c7faf1cb8d346d9d07d744" data-text-hash="d8fd4663faee19f63f794e62740a41ea" id="248" refid="248">
<div class="code-area-container">
<pre class="code-area">$ kubectl describe svc kiada
Name:                     kiada
...
Port:                     http  80/TCP
TargetPort:               8080/TCP
NodePort:                 http  30080/TCP
Endpoints:                10.244.1.7:8080,10.244.1.8:8080,10.244.1.9:8080 + 1 more...    #A
...</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIGxpc3Qgb2YgZW5kcG9pbnRzIChwb2QgSVBzIGFuZCBwb3J0cykgZm9yIHRoaXMgc2VydmljZS4="></div>
</div>
</div>
<div class="readable-text" data-hash="b05aea58b9ad2593642e947f9ae1332f" data-text-hash="1f292706431ed5fa9fa3b786834de606" id="249" refid="249">
<p>The <code>kubectl describe</code> command collects this data not from the Service object, but from an Endpoints object whose name matches that of the service. The endpoints of the <code>kiada</code> service are specified in the <code>kiada</code> Endpoints object.</p>
</div>
<div class="readable-text" data-hash="c3a4171b7dda5ad88542d850703e37ff" data-text-hash="f0b2e9995d63ad79015b79b3e5b74cbd" id="250" refid="250">
<h4>Listing Endpoints objects</h4>
</div>
<div class="readable-text" data-hash="a4e1021c0ee48035d9f9e9a24bb283bc" data-text-hash="85c528abb63c36848aab2523f06d51a5" id="251" refid="251">
<p>You can retrieve Endpoints objects in the current namespace as follows:</p>
</div>
<div class="browsable-container listing-container" data-hash="dfa3c14b04ec19f69c4a7ea5943ad959" data-text-hash="270966a15e64378e305b20a80ab6d9f5" id="252" refid="252">
<div class="code-area-container">
<pre class="code-area">$ kubectl get endpoints
NAME    ENDPOINTS                                                     AGE
kiada   10.244.1.7:8443,10.244.1.8:8443,10.244.1.9:8443 + 5 more...   25m
quiz    10.244.1.11:8080                                              66m
quote   10.244.1.10:80,10.244.2.10:80,10.244.2.8:80 + 1 more...       66m</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="253" refid="253">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="cbcc5d66a1608da6333b0113779338da" data-text-hash="55a32974f6b08fc29f6674dcc3602332" id="254" refid="254">
<p> The shorthand for <code>endpoints</code> is <code>ep</code>. Also, the object kind is Endpoints (plural form) not Endpoint. Running <code>kubectl get endpoint</code> fails with an error.</p>
</div>
</div>
<div class="readable-text" data-hash="c381b7c11bdefd2edf2ed3295824fcaa" data-text-hash="4dc8784ff644236e7a316eff88863748" id="255" refid="255">
<p>As you can see, there are three Endpoints objects in the namespace. One for each service. Each Endpoints object contains a list of IP and port combinations that represent the endpoints for the service.</p>
</div>
<div class="readable-text" data-hash="588dc51998091592a2c5ae8eb9b35466" data-text-hash="00624b95403242ed18e1707b4ef68af3" id="256" refid="256">
<h4>Inspecting an Endpoints object more closely</h4>
</div>
<div class="readable-text" data-hash="97f2f7bf891d49b652f9860da2c21021" data-text-hash="2e8b8b7f03ce4524673d9b368925c94d" id="257" refid="257">
<p>To see which pods represent these endpoints, use <code>kubectl get -o yaml</code> to retrieve the full manifest of the Endpoints object as follows:</p>
</div>
<div class="browsable-container listing-container" data-hash="c293dff551b9275e1bc97ee9543f53d1" data-text-hash="15a1f7878319e96f5ffe25e6f0592353" id="258" refid="258">
<div class="code-area-container">
<pre class="code-area">$ kubectl get ep kiada -o yaml
apiVersion: v1
kind: Endpoints
metadata:
  name: kiada    #A
  namespace: kiada    #A
  ...
subsets:
- addresses:
  - ip: 10.244.1.7    #B
    nodeName: kind-worker    #C
    targetRef:
      kind: Pod
      name: kiada-002    #D
      namespace: kiada    #D
      resourceVersion: "2950"
      uid: 18cea623-0818-4ff1-9fb2-cddcf5d138c3
  ...    #E
  ports:    #F
  - name: https    #F
    port: 8443    #F
    protocol: TCP    #F
  - name: http    #F
    port: 8080    #F
    protocol: TCP    #F</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIG5hbWUgYW5kIG5hbWVzcGFjZSBvZiB0aGlzIEVuZHBvaW50cyBvYmplY3QuIFRoZXNlIGFsd2F5cyBtYXRjaCB0aGUgbmFtZSBhbmQgbmFtZXNwYWNlIG9mIHRoZSBhc3NvY2lhdGVkIFNlcnZpY2Ugb2JqZWN0LgojQiBUaGUgSVAgYWRkcmVzcyBvZiB0aGUgZmlyc3QgZW5kcG9pbnQgKGEgcG9kIHRoYXQgbWF0Y2hlcyB0aGUgbGFiZWwgc2VsZWN0b3IpLgojQyBUaGUgbmFtZSBvZiB0aGUgY2x1c3RlciBub2RlIG9uIHdoaWNoIHRoZSBwb2QgcnVucy4KI0QgVGhlIG5hbWUgYW5kIG5hbWVzcGFjZSBvZiB0aGUgcG9kLgojRSBUaGUgZW50cmllcyBmb3Igb3RoZXIgcG9kcyB0aGF0IG1hdGNoIHRoZSBzZWxlY3RvciBhcmUgbm90IHNob3duLgojRiBUaGUgbGlzdCBvZiBwb3J0cyB0aGF0IHRoZXNlIGVuZHBvaW50cyBleHBvc2UuIE1hdGNoZXMgdGhlIHBvcnRzIGRlZmluZWQgaW4gdGhlIFNlcnZpY2Uu"></div>
</div>
</div>
<div class="readable-text" data-hash="e3b20be75a76d88dd30107f26cf78993" data-text-hash="00daba0b352cd80b953fdd07dd27e085" id="259" refid="259">
<p>As you can see, each pod is listed as an element of the <code>addresses</code> array. In the <code>kiada</code> Endpoints object, all endpoints are in the same endpoint subset, because they all use the same port numbers. However, if one group of pods uses port 8080, for example, and another uses port 8088, the Endpoints object would contain two subsets, each with its own ports.</p>
</div>
<div class="readable-text" data-hash="716cdacddfbd7b3eec2a4defb8363cc8" data-text-hash="4aebfc1c8241dada4c03127719d27487" id="260" refid="260">
<h4>Understanding who manages the Endpoints object</h4>
</div>
<div class="readable-text" data-hash="34916c57abc55344783bfc18536d0f2a" data-text-hash="56628df2c90aacaddb5cbdc16af4f650" id="261" refid="261">
<p>You didn&#8217;t create any of the three Endpoints objects. They were created by Kubernetes when you created the associated Service objects. These objects are fully managed by Kubernetes. Each time a new pod appears or disappears that matches the Service&#8217;s label selector, Kubernetes updates the Endpoints object to add or remove the endpoint associated with the pod. You can also manage a service&#8217;s endpoints manually. You&#8217;ll learn how to do that later.</p>
</div>
<div class="readable-text" data-hash="6a5cc1ff9454775b6fe5d44ccf0419d5" data-text-hash="846289c941fb6325bc7ba747cdd5c756" id="262" refid="262">
<h3 id="sigil_toc_id_199">11.3.2&#160;Introducing the EndpointSlice object</h3>
</div>
<div class="readable-text" data-hash="6595ce2bd48ceef0838b1b9087b89462" data-text-hash="1bb8dff9519c359906c2fdc4d2890767" id="263" refid="263">
<p>As you can imagine, the size of an Endpoints object becomes an issue when a service contains a very large number of endpoints. Kubernetes control plane components need to send the entire object to all cluster nodes every time a change is made. In large clusters, this leads to noticeable performance issues. To counter this, the EndpointSlice object was introduced, which splits the endpoints of a single service into multiple slices.</p>
</div>
<div class="readable-text" data-hash="f62c3718faf526954e38b6453947d73f" data-text-hash="7bcef39d38f7ae32a51fd2a06367000c" id="264" refid="264">
<p>While an Endpoints object contains multiple endpoint subsets, each EndpointSlice contains only one. If two groups of pods expose the service on different ports, they appear in two different EndpointSlice objects. Also, an EndpointSlice object supports a maximum of 1000 endpoints, but by default Kubernetes only adds up to 100 endpoints to each slice. The number of ports in a slice is also limited to 100. Therefore, a service with hundreds of endpoints or many ports can have multiple EndpointSlices objects associated with it.</p>
</div>
<div class="readable-text" data-hash="ad6e471287163d6fa28205173e9647c0" data-text-hash="b27aa0002971a7544c554576eb7b717c" id="265" refid="265">
<p>Like Endpoints, EndpointSlices are created and managed automatically.</p>
</div>
<div class="readable-text" data-hash="16f5db07bdcbf6c53457bd88c90747f6" data-text-hash="399487fbc7779f155f519675da678462" id="266" refid="266">
<h4>Listing EndpointSlice objects</h4>
</div>
<div class="readable-text" data-hash="3fc69b0fedcc3de47234b78163018499" data-text-hash="2c61f0da86012861db6c8ecf1e30cd43" id="267" refid="267">
<p>In addition to the Endpoints objects, Kubernetes creates the EndpointSlice objects for your three services. You can see them with the <code>kubectl get endpointslices</code> command:</p>
</div>
<div class="browsable-container listing-container" data-hash="925b68dc74e5f150cc598aaae64590f6" data-text-hash="5321df2835be236179087796a1dcd07a" id="268" refid="268">
<div class="code-area-container">
<pre class="code-area">$ kubectl get endpointslices
NAME          ADDRESSTYPE   PORTS       ENDPOINTS                                       AGE
kiada-m24zq   IPv4          8080,8443   10.244.1.7,10.244.1.8,10.244.1.9 + 1 more...    80m
quiz-qbckq    IPv4          8080        10.244.1.11                                     79m
quote-5dqhx   IPv4          80          10.244.2.8,10.244.1.10,10.244.2.9 + 1 more...   79m</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="269" refid="269">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="532b335884b27ef21ddee1392912f258" data-text-hash="80164e82e3657bbb6a02fc359f35414b" id="270" refid="270">
<p> As of this writing, there is no shorthand for <code>endpointslices</code>.</p>
</div>
</div>
<div class="readable-text" data-hash="312b21df08acb05a12cbf5e957b6c2ba" data-text-hash="9dbd97c8d321bf85ec7189d86e339150" id="271" refid="271">
<p>You&#8217;ll notice that unlike Endpoints objects, whose names match the names of their respective Service objects, each EndpointSlice object contains a randomly generated suffix after the service name. This way, many EndpointSlice objects can exist for each service.</p>
</div>
<div class="readable-text" data-hash="f5766b15515aa5682c0a50087cde59c8" data-text-hash="ddfe50609855c0c197cfb3d9401bf940" id="272" refid="272">
<h4>Listing EndpointSlices for a particular service</h4>
</div>
<div class="readable-text" data-hash="d9f225aeec35a080ffa7825617bc704f" data-text-hash="1c2533decc43f4b73d9ebbf09933ed37" id="273" refid="273">
<p>To see only the EndpointSlice objects associated with a particular service, you can specify a label selector in the <code>kubectl get</code> command. To list the EndpointSlice objects associated with the <code>kiada</code> service, use the label selector <code>kubernetes.io/service-name=kiada</code> as follows:</p>
</div>
<div class="browsable-container listing-container" data-hash="c03a809d51a961ee2e5202423c39411a" data-text-hash="0ac70e9fbfbed70002e3225ed9c79852" id="274" refid="274">
<div class="code-area-container">
<pre class="code-area">$ kubectl get endpointslices -l kubernetes.io/service-name=kiada
NAME          ADDRESSTYPE   PORTS       ENDPOINTS                                      AGE
kiada-m24zq   IPv4          8080,8443   10.244.1.7,10.244.1.8,10.244.1.9 + 1 more...   88m</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="94e69f20beabdcafd7df51aaa207343e" data-text-hash="f658c06b28b4eb2383496b46c24fcbc7" id="275" refid="275">
<h4>Inspecting an EndpointSlice</h4>
</div>
<div class="readable-text" data-hash="18275a7420743d7f754b9f4fa439fcc2" data-text-hash="808f384d840d704f84861734b12407d9" id="276" refid="276">
<p>To examine an EndpointSlice object in more detail, you use <code>kubectl describe</code>. Since the <code>describe</code> command doesn&#8217;t require the full object name, and all EndpointSlice objects associated with a service begin with the service name, you can see them all by specifying only the service name, as shown here:</p>
</div>
<div class="browsable-container listing-container" data-hash="181368660eadd9ac853ffb37a99ca1c4" data-text-hash="106f43cdd41c56f8392de34649e15cdc" id="277" refid="277">
<div class="code-area-container">
<pre class="code-area">$ kubectl describe endpointslice kiada
Name:         kiada-m24zq
Namespace:    kiada
Labels:       endpointslice.kubernetes.io/managed-by=endpointslice-controller.k8s.io
              kubernetes.io/service-name=kiada
Annotations:  endpoints.kubernetes.io/last-change-trigger-time: 2021-10-30T08:36:21Z
AddressType:  IPv4
Ports:    #A
  Name   Port  Protocol    #A
  ----   ----  --------    #A
  http   8080  TCP    #A
  https  8443  TCP    #A
Endpoints:
  - Addresses:  10.244.1.7    #B
    Conditions:
      Ready:    true
    Hostname:   &lt;unset&gt;
    TargetRef:  Pod/kiada-002    #C
    Topology:   kubernetes.io/hostname=kind-worker    #D
...</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIHBvcnRzIGV4cG9zZWQgYnkgdGhlIGVuZHBvaW50cyBpbiB0aGlzIHNsaWNlLgojQiBUaGUgSVAgYWRkcmVzcyBvZiB0aGUgZmlyc3QgZW5kcG9pbnQuCiNDIFRoZSBraWFkYS0wMDIgcG9kIHJlcHJlc2VudHMgdGhpcyBzZXJ2aWNlIGVuZHBvaW50LgojRCBUb3BvbG9neSBpbmZvcm1hdGlvbiBmb3IgdGhpcyBlbmRwb2ludC4gRXhwbGFpbmVkIGxhdGVyIGluIHRoaXMgY2hhcHRlci4="></div>
</div>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="278" refid="278">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="a3f887dd50b9720634c1ae348cc20a8a" data-text-hash="da325b153fd884678a1bb958e2432433" id="279" refid="279">
<p> If multiple EndpointSlices match the name you provide to <code>kubectl describe</code>, the command will print all of them.</p>
</div>
</div>
<div class="readable-text" data-hash="7321b8b18cad76aad4d824857fa32d10" data-text-hash="ccd5f706b2c48e9da73f14644a65e7b9" id="280" refid="280">
<p>The information in the output of the <code>kubectl describe</code> command isn&#8217;t much different from the information in the Endpoint object you saw earlier. The EndpointSlice object contains a list of ports and endpoint addresses, as well as information about the pods that represent those endpoints. This includes the pod&#8217;s topology information, which is used for topology-aware traffic routing. You&#8217;ll learn about it later in this chapter.</p>
</div>
<div class="readable-text" data-hash="6e0a96358c7bde51f332721498e51c01" data-text-hash="e29290f73d2acd6c4e73f5d7564ed085" id="281" refid="281">
<h3 id="sigil_toc_id_200">11.3.3&#160;Managing service endpoints manually</h3>
</div>
<div class="readable-text" data-hash="d4624f4bb4702100d72f5990b2474e1a" data-text-hash="323bb57d327c41906b3c761ad353219e" id="282" refid="282">
<p>When you create a Service object with a label selector, Kubernetes automatically creates and manages the Endpoints and EndpointSlice objects and uses the selector to determine the service endpoints. However, you can also manage endpoints manually by creating the Service object without a label selector. In this case, you must create the Endpoints object yourself. You don&#8217;t need to create the EndpointSlice objects because Kubernetes mirrors the Endpoints object to create corresponding EndpointSlices.</p>
</div>
<div class="readable-text" data-hash="aebae26930ecb2f4c36595017253e112" data-text-hash="ee4f2793130771f97bcaa8061c82a27a" id="283" refid="283">
<p>Typically, you manage service endpoints this way when you want to make an existing external service accessible to pods in your cluster under a different name. This way, the service can be found through the cluster DNS and environment variables.</p>
</div>
<div class="readable-text" data-hash="c167b808c8a7bc30deda8d61f60b4472" data-text-hash="7c61c1be31e68c02185c306d114eb0c4" id="284" refid="284">
<h4>Creating a service without a label selector</h4>
</div>
<div class="readable-text" data-hash="83a6240a1f210f77295c371378b4e03d" data-text-hash="cd2a4c45e09ecf3b900e04082ac9e5e4" id="285" refid="285">
<p>The following listing shows an example of a Service object manifest that doesn&#8217;t define a label selector. You&#8217;ll manually configure the endpoints for this service.</p>
</div>
<div class="browsable-container listing-container" data-hash="29f07f10a95355630d0507c3478a9295" data-text-hash="53667536cf29fe8d1d51a3c975dcbfe0" id="286" refid="286">
<h5>Listing 11.5 A service with no pod selector</h5>
<div class="code-area-container">
<pre class="code-area">apiVersion: v1
kind: Service
metadata:
  name: external-service    #A
spec:    #B
  ports:    #B
  - name: http    #B
    port: 80    #B</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIG5hbWUgb2YgdGhlIHNlcnZpY2UgbXVzdCBtYXRjaCB0aGUgbmFtZSBvZiB0aGUgRW5kcG9pbnRzIG9iamVjdCAoc2VlIG5leHQgbGlzdGluZykuCiNCIE5vIGxhYmVsIHNlbGVjdG9yIGlzIGRlZmluZWQgZm9yIHRoaXMgc2VydmljZS4="></div>
</div>
</div>
<div class="readable-text" data-hash="fa4bf4960569d36edd0f00ba8abfd97f" data-text-hash="44451babc61737b311670ec1db51efb4" id="287" refid="287">
<p>The manifest in the listing defines a service named <code>external-service</code> that accepts incoming connections on port 80. As explained in the first part of this chapter, pods in the cluster can use the service either through its cluster IP address, which is assigned when you create the service, or through its DNS name.</p>
</div>
<div class="readable-text" data-hash="d856e0ff089df03f9ff3c8982d1de556" data-text-hash="c4822efef68d5a8d537d51915d006192" id="288" refid="288">
<h4>Creating an Endpoints object</h4>
</div>
<div class="readable-text" data-hash="2a72fa4cdefdc3af744a37228bf523b4" data-text-hash="2382ffc9fc447edad4e97c8ae282b297" id="289" refid="289">
<p>If a service doesn&#8217;t define a pod selector, no Endpoints object is automatically created for it. You must do this yourself. The following listing shows the manifest of the Endpoints object for the service you created in the previous section.</p>
</div>
<div class="browsable-container listing-container" data-hash="31ac04cfe630ee50c85b10cba62d1f76" data-text-hash="e8d52cc2a187b8cde5492416c4a2e9d7" id="290" refid="290">
<h5>Listing 11.6 An Endpoints object created by hand</h5>
<div class="code-area-container">
<pre class="code-area">apiVersion: v1
kind: Endpoints
metadata:
  name: external-service    #A
subsets:
- addresses:
  - ip: 1.1.1.1    #B
  - ip: 2.2.2.2    #B
  ports:
  - name: http    #C
    port: 88    #C</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIG5hbWUgb2YgdGhlIEVuZHBvaW50cyBvYmplY3QgbXVzdCBtYXRjaCB0aGUgbmFtZSBvZiB0aGUgc2VydmljZSAoc2VlIHByZXZpb3VzIGxpc3RpbmcpLgojQiBUaGUgSVBzIG9mIHRoZSBlbmRwb2ludHMgdGhhdCB0aGUgc2VydmljZSB3aWxsIGZvcndhcmQgY29ubmVjdGlvbnMgdG8uCiNDIFRoZSBwb3J0IG9uIHdoaWNoIHRoZSBlbmRwb2ludHMgZXhwb3NlIHRoZSBzZXJ2aWNlLg=="></div>
</div>
</div>
<div class="readable-text" data-hash="69d8635cbe374a11ff419975e882d112" data-text-hash="8088abfa0021ab19e1459cf60e661da4" id="291" refid="291">
<p>The Endpoints object must have the same name as the service and contain the list of destination addresses and ports. In the listing, IP addresses 1.1.1.1 and 2.2.2.2 represent the endpoints for the service.</p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="292" refid="292">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="48606a9ae24643cefed2de0bb99bba59" data-text-hash="fe64cedf8077ca1d91358ab9a5a40f33" id="293" refid="293">
<p> You don&#8217;t have to create the EndpointSlice object. Kubernetes creates it from the Endpoints object.</p>
</div>
</div>
<div class="readable-text" data-hash="e772634d7b2e0c39f11e0a69a9e93d5d" data-text-hash="48444e144078ac4c12be91914b8eec2f" id="294" refid="294">
<p>The creation of the Service and its associated Endpoints object allows pods to use this service in the same way as other services defined in the cluster. As shown in the following figure, traffic sent to the service&#8217;s cluster IP is distributed to the service&#8217;s endpoints. These endpoints are outside the cluster but could also be internal.</p>
</div>
<div class="browsable-container figure-container" data-hash="2610ca6c5598353f35500cd815ff48c6" data-text-hash="6a3d22854fca7d1b7e64d2b00db13351" id="295" refid="295">
<h5>Figure 11.12 Pods consuming a service with two external endpoints.</h5>
<img alt="" data-processed="true" height="276" id="Picture_11" loading="lazy" src="EPUB/images/11image013.png" width="846">
</div>
<div class="readable-text" data-hash="3f423da6fc7c28131c1aa7f312e56d26" data-text-hash="79b2c6cf787b7e14ee362d14503fbd2a" id="296" refid="296">
<p>If you later decide to migrate the external service to pods running inside the Kubernetes cluster, you can add a selector to the service to redirect traffic to those pods instead of the endpoints you configured by hand. This is because Kubernetes immediately starts managing the Endpoints object after you add the selector to the service.</p>
</div>
<div class="readable-text" data-hash="cd2948630d774b2b63bcb2440ae6aa23" data-text-hash="dbbc8c35fceade20c5add2afbc53455d" id="297" refid="297">
<p>You can also do the opposite: If you want to migrate an existing service from the cluster to an external location, remove the selector from the Service object so that Kubernetes no longer updates the associated Endpoints object. From then on, you can manage the service&#8217;s endpoints manually.</p>
</div>
<div class="readable-text" data-hash="0a5de55d9d8a1107f4fc027023ddfa19" data-text-hash="8b657b6a8c7a72625176d49afd2229c2" id="298" refid="298">
<p>You don&#8217;t have to delete the service to do this. By changing the existing Service object, the cluster IP address of the service remains constant. The clients using the service won&#8217;t even notice that you&#8217;ve relocated the service.</p>
</div>
<div class="readable-text" data-hash="f56cdeaa0f65d0a813b0a5bc4bcfffca" data-text-hash="85b01e3a376bfe52fac2a8b55bfe9f12" id="299" refid="299">
<h2 id="sigil_toc_id_201">11.4&#160;Understanding DNS records for Service objects</h2>
</div>
<div class="readable-text" data-hash="aa2e72a26fe00228d588d382cba88e9c" data-text-hash="30238f63c8264081ff8e6098b2f939a0" id="300" refid="300">
<p>An important aspect of Kubernetes services is the ability to look them up via DNS. This is something that deserves to be looked at more closely.</p>
</div>
<div class="readable-text" data-hash="6b8cac732b7cb49e4aa0a74f8ec9b7a4" data-text-hash="08fef4ec366d670581144126a76a4269" id="301" refid="301">
<p>You know that a service is assigned an internal cluster IP address that pods can resolve through the cluster DNS. This is because each service gets an <code>A</code> record in DNS (or an <code>AAAA</code> record for IPv6). However, a service also receives an <code>SRV</code> record for each of the ports it makes available.</p>
</div>
<div class="readable-text" data-hash="afdad35cb070c24b27e9b45c54561818" data-text-hash="f3a201e3d05b12ca6d7259c2bfa6b980" id="302" refid="302">
<p>Let&#8217;s take a closer look at these DNS records. First, run a one-off pod like this:</p>
</div>
<div class="browsable-container listing-container" data-hash="dfe2b48570f33ac62d3964e8c88690f3" data-text-hash="112517d6c0ab25ed86c2d697c86bb3d8" id="303" refid="303">
<div class="code-area-container">
<pre class="code-area">$ kubectl run -it --rm dns-test --image=giantswarm/tiny-tools
/ #</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="0af76663b6f20fdddef914d6122bb541" data-text-hash="7a72b4b21a36aa3862e9c3cfee68ed19" id="304" refid="304">
<p>This command runs a pod named <code>dns-test</code> with a container based on the container image <code>giantswarm/tiny-tools</code>. This image contains the <code>host</code>, <code>nslookup</code>, and <code>dig</code> tools that you can use to examine DNS records. When you run the <code>kubectl run</code> command, your terminal will be attached to the shell process running in the container (the <code>-it</code> option does this). When you exit the shell, the pod will be removed (by the <code>--rm</code> option).</p>
</div>
<div class="readable-text" data-hash="76fa80636d44955048bfc08a62e81e36" data-text-hash="a0a22eddc80a2b0f9b5258e8e61a991b" id="305" refid="305">
<h3 id="sigil_toc_id_202">11.4.1&#160;Inspecting a service&#8217;s A and SRV records in DNS</h3>
</div>
<div class="readable-text" data-hash="3b50ad5b92af12053f9e03cc87f960e1" data-text-hash="bd02f951b9d8c17984516d8d0edce6ff" id="306" refid="306">
<p>You start by inspecting the <code>A</code> and <code>SRV</code> records associated with your services.</p>
</div>
<div class="readable-text" data-hash="127658a69a372c946dbd6427e8b2a705" data-text-hash="d971552d456debf8384044e6ec3d3ac7" id="307" refid="307">
<h4>Looking up a service&#8217;s A record</h4>
</div>
<div class="readable-text" data-hash="4be1754a0cee0acb22983674a550c3d6" data-text-hash="2adb9686b3450c0ebaf95d76f043cb52" id="308" refid="308">
<p>To determine the IP address of the <code>quote</code> service, you run the <code>nslookup</code> command in the shell running in the container of the <code>dns-test</code> pod like so:</p>
</div>
<div class="browsable-container listing-container" data-hash="220214d57a614b2b9540ec8108f59e83" data-text-hash="218f2fc65fb7747e23cf12ae82e0e2a6" id="309" refid="309">
<div class="code-area-container">
<pre class="code-area">/ # nslookup quote
Server:         10.96.0.10
Address:        10.96.0.10#53 //
 
Name:   quote.kiada.svc.cluster.local    #A
Address: 10.96.161.97    #B</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIHNlcnZpY2XigJlzIGZ1bGx5IHF1YWxpZmllZCBkb21haW4gbmFtZQojQiBUaGUgc2VydmljZeKAmXMgY2x1c3RlciBJUA=="></div>
</div>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="310" refid="310">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="0d14a7114834e3d342276a2004d6484f" data-text-hash="fd591fa573aee80d2fce762471a7ea26" id="311" refid="311">
<p> You can use <code>dig</code> instead of <code>nslookup</code>, but you must either use the <code>+search</code> option or specify the fully qualified domain name of the service for the DNS lookup to succeed (run either <code>dig +search quote</code> or <code>dig quote.kiada.svc.cluster.local</code>).</p>
</div>
</div>
<div class="readable-text" data-hash="11213845ee412132d647e655e663960b" data-text-hash="efee23bc12e4ca3e30b39ed94534f9b7" id="312" refid="312">
<p>Now look up the IP address of the <code>kiada</code> service. Although this service is of type <code>LoadBalancer</code> and thus has both an internal cluster IP and an external IP (that of the load balancer), the DNS returns only the cluster IP. This is to be expected since the DNS server is internal and is only used within the cluster.</p>
</div>
<div class="readable-text" data-hash="a43ba317d52b7e27d2f39aaede652725" data-text-hash="ca50ffff0047a814a0a302ce5f02ba69" id="313" refid="313">
<h4>Looking up SRV records</h4>
</div>
<div class="readable-text" data-hash="53a6afa85a9ce96ad5c92d2fa01eb553" data-text-hash="523734d67b1e1ae54218c9baed28995b" id="314" refid="314">
<p>A service provides one or more ports. Each port is given an <code>SRV</code> record in DNS. Use the following command to retrieve the <code>SRV</code> records for the <code>kiada</code> service:</p>
</div>
<div class="browsable-container listing-container" data-hash="266ad20bbd235c10686c2037cde8ce9d" data-text-hash="38fc77122a2601cde1dd3586c7822aad" id="315" refid="315">
<div class="code-area-container">
<pre class="code-area">/ # nslookup -query=SRV kiada
Server:         10.96.0.10
Address:        10.96.0.10#53 // //
 
kiada.kiada.svc.cluster.local   service = 0 50 80 kiada.kiada.svc.cluster.local.    #A
kiada.kiada.svc.cluster.local   service = 0 50 443 kiada.kiada.svc.cluster.local.    #B</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgU1JWIHJlY29yZCBmb3IgdGhlIGh0dHAgcG9ydCA4MAojQiBTUlYgcmVjb3JkIGZvciB0aGUgaHR0cHMgcG9ydCA0NDM="></div>
</div>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="316" refid="316">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="121e59404ae82618c99fc540c1826f5c" data-text-hash="a61991b97ed52c1cfb2cb6c8ba540388" id="317" refid="317">
<p> As of this writing, GKE still runs kube-dns instead of CoreDNS. Kube-dns doesn&#8217;t support all the DNS queries shown in this section.</p>
</div>
</div>
<div class="readable-text" data-hash="0ef1a868c1f3d610c766a0fa123d9852" data-text-hash="2a5c31651596a540aecf99a9cd753771" id="318" refid="318">
<p>A smart client running in a pod could look up the <code>SRV</code> records of a service to find out what ports are provided by the service. If you define the names for those ports in the Service object, they can even be looked up by name. The <code>SRV</code> record has the following form:</p>
</div>
<div class="browsable-container listing-container" data-hash="31a26dd7fec9d05c0e0809af1da019cf" data-text-hash="0de5d56e361486aaeca3cbe449188be7" id="319" refid="319">
<div class="code-area-container">
<pre class="code-area">_port-name._port-protocol.service-name.namespace.svc.cluster.local</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="87b78368cc0aa3c8fb4d146ffea377b8" data-text-hash="b786f8f00fb2b9fcfd304cb1045ca7cf" id="320" refid="320">
<p>The names of the two ports in the <code>kiada</code> service are <code>http</code> and <code>https</code>, and both define TCP as the protocol. To get the <code>SRV</code> record for the <code>http</code> port, run the following command:</p>
</div>
<div class="browsable-container listing-container" data-hash="d398fa453dbfb5869c58eacb5c96d5de" data-text-hash="4034ca41450c99664a1f548eee305a64" id="321" refid="321">
<div class="code-area-container">
<pre class="code-area">/ # nslookup -query=SRV _http._tcp.kiada
Server:         10.96.0.10
Address:        10.96.0.10#53 //
 
_http._tcp.kiada.kiada.svc.cluster.local        service = 0 100 80 kiada.kiada.svc.cluster.local.</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="5c622e940054ac4ab45712e2d7b5d25d" data-text-hash="12ae2a12586001e30745cb0457586ae3" id="322" refid="322">
<h5>Tip</h5>
</div>
<div class="readable-text" data-hash="8727699b462f80e4674b65bed1334a1e" data-text-hash="166e5cee77f5c7a9f5633a678c788c6f" id="323" refid="323">
<p> To list all services and the ports they expose in the <code>kiada</code> namespace, you can run the command <code>nslookup -query=SRV any.kiada.svc.cluster.local</code>. To list all services in the cluster, use the name <code>any.any.svc.cluster.local</code>.</p>
</div>
</div>
<div class="readable-text" data-hash="e43cb483b53bdb71ab96c3320245b29f" data-text-hash="56cdb1672d647dafd0f6bc4e81e5e062" id="324" refid="324">
<p>You&#8217;ll probably never need to look for <code>SRV</code> records, but some Internet protocols, such as SIP and XMPP, depend on them to work.</p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="325" refid="325">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="249bd4ae7c08b51ff2834165a613a538" data-text-hash="e0981e8a2e6e7dcd33522d1b510f0162" id="326" refid="326">
<p> Please leave the shell in the <code>dns-test</code> pod running, because you&#8217;ll need it in the exercises in the next section when you learn about headless services.</p>
</div>
</div>
<div class="readable-text" data-hash="598e0727662caf0b102dc199405b46db" data-text-hash="117ae44aa75ac8546544ea5a0ee6ab6f" id="327" refid="327">
<h3 id="sigil_toc_id_203">11.4.2&#160;Using headless services to connect to pods directly</h3>
</div>
<div class="readable-text" data-hash="706ba97657e34c5ce05e032bcbc343ec" data-text-hash="78fb37ba8f760e9ba6d5478a400af649" id="328" refid="328">
<p>Services expose a set of pods at a stable IP address. Each connection to that IP address is forwarded to a random pod or other endpoint that backs the service. Connections to the service are automatically distributed across its endpoints. But what if you want the client to do the load balancing? What if the client needs to decide which pod to connect to? Or what if it needs to connect to all pods that back the service? What if the pods that are part of a service all need to connect directly to each other? Connecting via the service&#8217;s cluster IP clearly isn&#8217;t the way to do this. What then?</p>
</div>
<div class="readable-text" data-hash="a0b42644162bdd3af72dc866aeea4049" data-text-hash="338810f96b42974ecb65f8d733f97aa7" id="329" refid="329">
<p>Instead of connecting to the service IP, clients could get the pod IPs from the Kubernetes API, but it&#8217;s better to keep them Kubernetes-agnostic and use standard mechanisms like DNS. Fortunately, you can configure the internal DNS to return the pod IPs instead of the service&#8217;s cluster IP by creating a <i>headless</i> service.</p>
</div>
<div class="readable-text" data-hash="dab3f1e9bad9cc0eb56f5d1270a4c53c" data-text-hash="4dd2a50c6975bbfe374bbe3efa1fd5c4" id="330" refid="330">
<p>For headless services, the cluster DNS returns not just a single <code>A</code> record pointing to the service&#8217;s cluster IP, but multiple <code>A</code> records, one for each pod that&#8217;s part of the service. Clients can therefore query the DNS to get the IPs of all the pods in the service. With this information, the client can then connect directly to the pods, as shown in the next figure.</p>
</div>
<div class="browsable-container figure-container" data-hash="b8d451647456eb0418f94a6e0a614325" data-text-hash="a90769ac3b115ddc7631c07420bd74e0" id="331" refid="331">
<h5>Figure 11.13 With headless services, clients connect directly to the pods</h5>
<img alt="" data-processed="true" height="244" id="Picture_12" loading="lazy" src="EPUB/images/11image014.png" width="858">
</div>
<div class="readable-text" data-hash="74c5e10c54f41a7fa2297ce537269acf" data-text-hash="ffca6e8649ee7760344b84297244c0a2" id="332" refid="332">
<h4>Creating a headless service</h4>
</div>
<div class="readable-text" data-hash="8da8d50a5171bdee560238227510a718" data-text-hash="59a191d381de0d004bc62e42466f0660" id="333" refid="333">
<p>To create a headless service, you set the <code>clusterIP</code> field to <code>None</code>. Create another service for the quote pods but make this one headless. The following listing shows its manifest:</p>
</div>
<div class="browsable-container listing-container" data-hash="412bd78d8c55c6f932bc71dd54fa3a2f" data-text-hash="cdc44193b70ddfe3ec47e91f070b0c53" id="334" refid="334">
<h5>Listing 11.7 A headless service</h5>
<div class="code-area-container">
<pre class="code-area">apiVersion: v1
kind: Service
metadata:
  name: quote-headless
spec:
  clusterIP: None    #A
  selector:
    app: quote
  ports:
  - name: http
    port: 80
    targetPort: 80
    protocol: TCP</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgU2V0dGluZyB0aGUgY2x1c3RlcklQIHRvIE5vbmUgbWFrZXMgdGhpcyBhIGhlYWRsZXNzIHNlcnZpY2Uu"></div>
</div>
</div>
<div class="readable-text" data-hash="e6a77dc7836b031c2ef72f15136730bc" data-text-hash="75f3b6ed1ba9e3d9842e51371413a70a" id="335" refid="335">
<p>After you create the service with <code>kubectl</code> <code>apply</code>, you can check it with <code>kubectl</code> <code>get</code>. You&#8217;ll see that it has no cluster IP:</p>
</div>
<div class="browsable-container listing-container" data-hash="6fd98a6e55af2352bfa1785939e9957a" data-text-hash="f8ae127ebaf9bf71c840c00a132ba857" id="336" refid="336">
<div class="code-area-container">
<pre class="code-area">$ kubectl get svc quote-headless -o wide
NAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR
quote-headless   ClusterIP   None         &lt;none&gt;        80/TCP    2m    app=quote</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="16fda584702a4283c07172a238ac8e6b" data-text-hash="b4c007f20224aa133076f0f8485630ee" id="337" refid="337">
<p>Because the service doesn&#8217;t have a cluster IP, the DNS server can&#8217;t return it when you try to resolve the service name. Instead, it returns the IP addresses of the pods. Before you continue, list the IPs of the pods that match the service&#8217;s label selector as follows:</p>
</div>
<div class="browsable-container listing-container" data-hash="c7f98121815c3bedd5b9ef792e104511" data-text-hash="ca523e39882012c82d9726a68e01c253" id="338" refid="338">
<div class="code-area-container">
<pre class="code-area">$ kubectl get po -l app=quote -o wide
NAME           READY   STATUS    RESTARTS   AGE   IP            NODE
quote-canary   2/2     Running   0          3h    10.244.2.9    kind-worker2
quote-001      2/2     Running   0          3h    10.244.2.10   kind-worker2
quote-002      2/2     Running   0          3h    10.244.2.8    kind-worker2
quote-003      2/2     Running   0          3h    10.244.1.10   kind-worker</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="212025c9aa3f1dc5afd03c98f6601ed9" data-text-hash="e296c7cdf6ae85603198cbd98833e935" id="339" refid="339">
<p>Note the IP addresses of these pods.</p>
</div>
<div class="readable-text" data-hash="c03a654e1ef3ff933ef544bddb932940" data-text-hash="fd10a246402f5aa5b5d4b6959ef9e939" id="340" refid="340">
<h4>Understanding DNS A records returned for a headless service</h4>
</div>
<div class="readable-text" data-hash="c87fdf91e775b33a4247d46b3ce95ab5" data-text-hash="02072e028b9060b0f552deb98b4d2310" id="341" refid="341">
<p>To see what the DNS returns when you resolve the service, run the following command in the <code>dns-test</code> pod you created in the previous section:</p>
</div>
<div class="browsable-container listing-container" data-hash="e8e71d29da472a0c1e929d30351dc5b1" data-text-hash="fbd0505fed3d246a384d0b9081b6ebef" id="342" refid="342">
<div class="code-area-container">
<pre class="code-area">/ # nslookup quote-headless
Server:         10.96.0.10
Address:        10.96.0.10#53 //
 
Name:   quote-headless.kiada.svc.cluster.local
Address: 10.244.2.9    #A
Name:   quote-headless.kiada.svc.cluster.local
Address: 10.244.2.8    #B
Name:   quote-headless.kiada.svc.cluster.local
Address: 10.244.2.10    #C
Name:   quote-headless.kiada.svc.cluster.local
Address: 10.244.1.10    #D</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIElQIG9mIHRoZSBxdW90ZS1jYW5hcnkgcG9kCiNCIFRoZSBJUCBvZiB0aGUgcXVvdGUtMDAyIHBvZAojQyBUaGUgSVAgb2YgdGhlIHF1b3RlLTAwMSBwb2QKI0QgVGhlIElQIG9mIHRoZSBxdW90ZS0wMDMgcG9k"></div>
</div>
</div>
<div class="readable-text" data-hash="7973f9a27c92d29f9f4c548338bef079" data-text-hash="37319dd439e0775d2b8424164f46a373" id="343" refid="343">
<p>The DNS server returns the IP addresses of the four pods that match the service&#8217;s label selector. This is different from what DNS returns for regular (non-headless) services such as the <code>quote</code> service, where the name resolves to the cluster IP of the service:</p>
</div>
<div class="browsable-container listing-container" data-hash="ebee0a355b6fe4ec3c1f9fe48f3a616c" data-text-hash="46710f5d58551b6162f932d50bf729df" id="344" refid="344">
<div class="code-area-container">
<pre class="code-area">/ # nslookup quote
Server:         10.96.0.10
Address:        10.96.0.10#53 //
 
Name:   quote.kiada.svc.cluster.local
Address: 10.96.161.97    #A</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIGNsdXN0ZXIgSVAgb2YgdGhlIHF1b3RlIHNlcnZpY2U="></div>
</div>
</div>
<div class="readable-text" data-hash="65af6e757c7c7972d89a58a07dc6adbf" data-text-hash="036fbd499622772dc1a04b598d8c32e1" id="345" refid="345">
<h4>Understanding how clients use headless services</h4>
</div>
<div class="readable-text" data-hash="2d6f30ceb7d965446b48c455f57f117b" data-text-hash="e8737576cf8b380cba6b8fe815d4d028" id="346" refid="346">
<p>Clients that wish to connect directly to pods that are part of a service, can do so by retrieving the <code>A</code> (or <code>AAAA</code>) records from the DNS. The client can then connect to one, some, or all the returned IP addresses.</p>
</div>
<div class="readable-text" data-hash="d24b1838ecbe82bb405e0daa8ee8279c" data-text-hash="f1579748d7964d2d2555eba1e78263af" id="347" refid="347">
<p>Clients that don&#8217;t perform the DNS lookup themselves, can use the service as they&#8217;d use a regular, non-headless service. Because the DNS server rotates the list of IP addresses it returns, a client that simply uses the service&#8217;s FQDN in the connection URL will get a different pod IP each time. Therefore, client requests are distributed across all pods.</p>
</div>
<div class="readable-text" data-hash="51b3d9a359b9c4dc802f2ba9dc2aa4c7" data-text-hash="0db0609f454c68f7190d9f9f394c0fcc" id="348" refid="348">
<p>You can try this by sending multiple requests the <code>quote-headless</code> service with <code>curl</code> from the <code>dns-test</code> pod as follows:</p>
</div>
<div class="browsable-container listing-container" data-hash="c347643bb3024464bb8fc97f9519b4e5" data-text-hash="a82b8c0ad0cce70321386132ae8f30e7" id="349" refid="349">
<div class="code-area-container">
<pre class="code-area">/ # while true; do curl http://quote-headless; done
This is the quote service running in pod quote-002
This is the quote service running in pod quote-001
This is the quote service running in pod quote-002
This is the quote service running in pod quote-canary
...</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="6ada923a1ce5f33c9cf329eda53b3861" data-text-hash="37a93f9fbacd0a15cf07e72327561b41" id="350" refid="350">
<p>Each request is handled by a different pod, just like when you use the regular service. The difference is that with a headless service you connect directly to the pod IP, while with regular services you connect to the cluster IP of the service, and your connection is forwarded to one of the pods. You can see this by running <code>curl</code> with the <code>--verbose</code> option and examining the IP it connects to:</p>
</div>
<div class="browsable-container listing-container" data-hash="48cecea654bc012f1559f7544458f213" data-text-hash="872cbe55998ac182323a79d35966a68c" id="351" refid="351">
<div class="code-area-container">
<pre class="code-area">/ # curl --verbose http://quote-headless   #A
*   Trying 10.244.1.10:80...    #A
* Connected to quote-headless (10.244.1.10) port 80 (#0)
...
 
/ # curl --verbose http://quote     #B
*   Trying 10.96.161.97:80...    #B
* Connected to quote (10.96.161.97) port 80 (#0)
...</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgV2hlbiB5b3UgY29ubmVjdCB0byB0aGUgaGVhZGxlc3Mgc2VydmljZSwgeW91IGNvbm5lY3QgZGlyZWN0bHkgdG8gb25lIG9mIHRoZSBwb2RzLgojQiBXaGVuIHlvdSBjb25uZWN0IHRvIHRoZSByZWd1bGFyIHNlcnZpY2UsIHlvdSBjb25uZWN0IHRvIGl0cyBjbHVzdGVyIElQLg=="></div>
</div>
</div>
<div class="readable-text" data-hash="d3b58857c894a6369956cd79b11e2916" data-text-hash="ca1bd24063e07e2e7c560e926692712f" id="352" refid="352">
<h4>Headless services with no label selector</h4>
</div>
<div class="readable-text" data-hash="bddf98ca57e07b9c30ff68eb9569791b" data-text-hash="fbfe30080e628e98ce52b08cfac64099" id="353" refid="353">
<p>To conclude this section on headless services, I&#8217;d like to mention that services with manually configured endpoints (services without a label selector) can also be headless. If you omit the label selector and set the <code>clusterIP</code> to <code>None</code>, the DNS will return an <code>A</code>/<code>AAAA</code> record for each endpoint, just as it does when the service endpoints are pods. To test this yourself, apply the manifest in the <code>svc.external-service-headless.yaml</code> file and run the following command in the <code>dns-test</code> pod:</p>
</div>
<div class="browsable-container listing-container" data-hash="7732795c5da5c38a4b369dc0bcb69906" data-text-hash="ca545161b8e6f93e2b6b28191713c788" id="354" refid="354">
<div class="code-area-container">
<pre class="code-area">/ # nslookup external-service-headless</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="473f8671721ebbf7b93e0a242907552f" data-text-hash="a064130d9bdded91d35d27e33dffa0e6" id="355" refid="355">
<h3 id="sigil_toc_id_204">11.4.3&#160;Creating a CNAME alias for an existing service</h3>
</div>
<div class="readable-text" data-hash="d1d691e0e3af13ad1bb0f391fa2b48ae" data-text-hash="9d1c18875b09a4b09ae74847611064a9" id="356" refid="356">
<p>In the previous sections, you learned how to create <code>A</code> and <code>AAAA</code> records in the cluster DNS. To do this, you create Service objects that either specify a label selector to find the service endpoints, or you define them manually using the Endpoints and EndpointSlice objects.</p>
</div>
<div class="readable-text" data-hash="7e9d7e4d924f57163aa9bb33aa6e6133" data-text-hash="1c8685cfb63d8dbab32026c4e6d466cf" id="357" refid="357">
<p>There&#8217;s also a way to add <code>CNAME</code> records to the cluster DNS. In Kubernetes, you add <code>CNAME</code> records to DNS by creating a Service object, just as you do for <code>A</code> and <code>AAAA</code> records.</p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="358" refid="358">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="d5fd91f47e54b49947e3c3620cc75881" data-text-hash="0d52bceb53d34f1ad69727feec622eb9" id="359" refid="359">
<p> A <code>CNAME</code> record is a DNS record that maps an alias to an existing DNS name instead of an IP address.</p>
</div>
</div>
<div class="readable-text" data-hash="4a302e46ff0c0c1b271e1edbe34c0d1e" data-text-hash="a0d13f8ccbea97752e11fa0f583c7b6c" id="360" refid="360">
<h4>Creating an ExternalName service</h4>
</div>
<div class="readable-text" data-hash="3c9bfd021e2b22710ee1578259784497" data-text-hash="b38614b08fa5b4192d2f4b6a18f5a277" id="361" refid="361">
<p>To create a service that serves as an alias for an existing service, whether it exists inside or outside the cluster, you create a Service object whose <code>type</code> field is set to <code>ExternalName</code>. The following listing shows an example of this type of service.</p>
</div>
<div class="browsable-container listing-container" data-hash="897197c877e9ae94a76d9c1ce3e24bbe" data-text-hash="084df9df933a4c327154636a357ebbd3" id="362" refid="362">
<h5>Listing 11.8 An <code class="codechar">ExternalName</code>-type service</h5>
<div class="code-area-container">
<pre class="code-area">apiVersion: v1
kind: Service
metadata:
  name: time-api
spec:
  type: ExternalName    #A
  externalName: worldtimeapi.org    #B</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgU2VydmljZSB0eXBlIGlzIHNldCB0byBFeHRlcm5hbE5hbWUuCiNCIFRoaXMgaXMgdGhlIGZ1bGx5IHF1YWxpZmllZCBkb21haW4gbmFtZSB0aGF0IHRoZSBDTkFNRSByZWNvcmQgd2lsbCBwb2ludCB0by4="></div>
</div>
</div>
<div class="readable-text" data-hash="0dd805a215d934905178dec8e9dddc3f" data-text-hash="07da0b34d934e4ce11f8eedc7373fa58" id="363" refid="363">
<p>In addition to setting the <code>type</code> to <code>ExternalName</code>, the service manifest must also specify in the <code>externalName</code> field external name to which this service resolves. No Endpoints or EndpointSlice object is required for ExternalName services.</p>
</div>
<div class="readable-text" data-hash="6a7a250359bea151f7b647a6aeef6895" data-text-hash="79959a27fb953666f99a079b3380b81d" id="364" refid="364">
<h4>Connecting to an ExternalName service from a pod</h4>
</div>
<div class="readable-text" data-hash="532ec2ad163d2cd3d664b93164d3bbe2" data-text-hash="2b94aebe717d61efd82b2d6014924490" id="365" refid="365">
<p>After the service is created, pods can connect to the external service using the domain name <code>time-api.&lt;namespace&gt;.svc.cluster.local</code> (or <code>time-api</code> if they&#8217;re in the same namespace as the service) instead of using the actual FQDN of the external service, as shown in the following example:</p>
</div>
<div class="browsable-container listing-container" data-hash="844cd75ce997cca90dcdaa63bff9759a" data-text-hash="448f39f0cf516110595e4fa820e5ba77" id="366" refid="366">
<div class="code-area-container">
<pre class="code-area">$ kubectl exec -it kiada-001 -c kiada -- curl http://time-api/api/timezone/CET</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="c16f285c3f8426703d4429bfb89e1119" data-text-hash="44d7d80a1151e50fd1479cdba13477f8" id="367" refid="367">
<h4>Resolving ExternalName services in DNS</h4>
</div>
<div class="readable-text" data-hash="b899837b35fb27881fa21c333f16d74f" data-text-hash="2313b2d79ce0b7fb14fce615b20c4ccd" id="368" refid="368">
<p>Because <code>ExternalName</code> services are implemented at the DNS level (only a <code>CNAME</code> record is created for the service), clients don&#8217;t connect to the service through the cluster IP, as is the case with non-headless ClusterIP services. They connect directly to the external service. Like headless services, <code>ExternalName</code> services have no cluster IP, as the following output shows:</p>
</div>
<div class="browsable-container listing-container" data-hash="42e03d19c89548bb951190bfab9589a4" data-text-hash="a51c735b11e69f573c1409929eb589f8" id="369" refid="369">
<div class="code-area-container">
<pre class="code-area">$ kubectl get svc time-api
NAME       TYPE           CLUSTER-IP   EXTERNAL-IP        PORT(S)   AGE
time-api   ExternalName   &lt;none&gt;       worldtimeapi.org   80/TCP    4m51s    #A</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgRXh0ZXJuYWxOYW1lIHNlcnZpY2VzIGdldCBubyBjbHVzdGVyIElQLg=="></div>
</div>
</div>
<div class="readable-text" data-hash="99e0c0d8e2011d40f7f39a03a090816d" data-text-hash="14518950366cf5d9798020f4bce3fd38" id="370" refid="370">
<p>As a final exercise in this section on DNS, you can try resolving the <code>time-api</code> service in the <code>dns-test</code> pod as follows:</p>
</div>
<div class="browsable-container listing-container" data-hash="9438875880c154fe218b6325d838eedf" data-text-hash="0ca052767b83030668cb1f6a8fa0ceac" id="371" refid="371">
<div class="code-area-container">
<pre class="code-area">/ # nslookup time-api
Server:         10.96.0.10
Address:        10.96.0.10#53 //
 
time-api.kiada.svc.cluster.local        canonical name = worldtimeapi.org.    #A
Name:   worldtimeapi.org    #B
Address: 213.188.196.246    #B
Name:   worldtimeapi.org    #B
Address: 2a09:8280:1::3:e    #B</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIHRpbWUtYXBpIHNlcnZpY2UgbWFwcyB0byB3b3JsZHRpbWVhcGkub3JnCiNCIFRoZSBhZGRyZXNzIHdvcmxkdGltZWFwaS5vcmcgcmVzb2x2ZXMgdG8gYW4gSVB2NCBhbmQgYW4gSVB2NiBhZGRyZXNzLg=="></div>
</div>
</div>
<div class="readable-text" data-hash="96b8088e202e15ecda01ca49d40b1a1a" data-text-hash="b14d75eec31e44c4568d38a576ca94a1" id="372" refid="372">
<p>You can see that <code>time-api.kiada.svc.cluster.local</code> points to <code>worldtimeapi.org</code>. This concludes this section on DNS records for Kubernetes services. You can now exit the shell in the <code>dns-test</code> pod by typing <code>exit</code> or pressing Control-D. The pod is deleted automatically.</p>
</div>
<div class="readable-text" data-hash="874f9da53c814cc0649378761aacc3f8" data-text-hash="74a9ee543a113a0ce559d29530d54452" id="373" refid="373">
<h2 id="sigil_toc_id_205">11.5&#160;Configuring services to route traffic to nearby endpoints</h2>
</div>
<div class="readable-text" data-hash="29a6b619f861c4e0aea0e58edc4ce913" data-text-hash="4a70865075a4b77aeaff5e8f8778ae42" id="374" refid="374">
<p>When you deploy pods, they are distributed across the nodes in the cluster. If cluster nodes span different availability zones or regions and the pods deployed on those nodes exchange traffic with each other, network performance and traffic costs can become an issue. In this case, it makes sense for services to forward traffic to pods that aren&#8217;t far from the pod where the traffic originates.</p>
</div>
<div class="readable-text" data-hash="230631334b7f653b737b95bab9ed78c5" data-text-hash="c73a37f73c35f328080247fb1e9778b0" id="375" refid="375">
<p>In other cases, a pod may need to communicate only with service endpoints on the same node as the pod. Not for performance or cost reasons, but because only the node-local endpoints can provide the service in the proper context. Let me explain what I mean.</p>
</div>
<div class="readable-text" data-hash="54231cfbebc241540414d498fa926049" data-text-hash="0e1e865cd34733dec60296ea759a0dec" id="376" refid="376">
<h3 id="sigil_toc_id_206">11.5.1&#160;Forwarding traffic only within the same node with internalTrafficPolicy</h3>
</div>
<div class="readable-text" data-hash="29729e24e560998f0998f33d899acdd6" data-text-hash="3b8bb1b23eb5e01ee2f1cf8cf923bb8a" id="377" refid="377">
<p>If pods provide a service that&#8217;s tied in some way to the node on which the pod is running, you must ensure that client pods running on a particular node connect only to the endpoints on the same node. You can do this by creating a Service with the <code>internalTrafficPolicy</code> set to <code>Local</code>.</p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="378" refid="378">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="55e2e28099ed3d3f47db66cc225d129f" data-text-hash="d4b7e6fe18a8e5c38e60651c9e6cf48d" id="379" refid="379">
<p> You previously learned about the <code>externalTrafficPolicy</code> field, which is used to prevent unnecessary network hops between nodes when external traffic arrives in the cluster. The service&#8217;s <code>internalTrafficPolicy</code> field is similar, but serves a different purpose.</p>
</div>
</div>
<div class="readable-text" data-hash="3108d99281ab745fe0e1ad07a138cce4" data-text-hash="0f2a2cf1bef65a32167e61e64926615a" id="380" refid="380">
<p>As shown in the following figure, if the service is configured with the <code>Local</code> internal traffic policy, traffic from pods on a given node is forwarded only to pods on the same node. If there are no node-local service endpoints, the connection fails.</p>
</div>
<div class="browsable-container figure-container" data-hash="aad45a36292f14e9443c83a5a86a45a3" data-text-hash="1cf2e7e81db5cca7b0ccd43979ba404f" id="381" refid="381">
<h5>Figure 11.14 The behavior of a service with internalTrafficPolicy set to Local</h5>
<img alt="" data-processed="true" height="392" id="Picture_13" loading="lazy" src="EPUB/images/11image015.png" width="830">
</div>
<div class="readable-text" data-hash="23f009418bfe4c9fc575165a4571b76c" data-text-hash="4d2f1a780b2f3ab2bef8c906fdba0c23" id="382" refid="382">
<p>Imagine a system pod running on each cluster node that manages communication with a device attached to the node. The pods don&#8217;t use the device directly, but communicate with the system pod. Since pod IPs are fungible, while service IPs are stable, pods connect to the system pod through a Service. To ensure that pods connect only to the local system pod and not to those on other nodes, the service is configured to forward traffic only to local endpoints. You don&#8217;t have any such pods in your cluster, but you can use the quote pods to try this feature.</p>
</div>
<div class="readable-text" data-hash="5eaf33709530aedd26fdddd3dfab6cb0" data-text-hash="2d3cdfe3a0ccca8ab8c728261e089664" id="383" refid="383">
<h4>Creating a service with a local internal traffic policy</h4>
</div>
<div class="readable-text" data-hash="dc204af45c189375545ce3832fa6fbb4" data-text-hash="17d98eecec0ab4a28d612621d8dcb62d" id="384" refid="384">
<p>The following listing shows the manifest for a service named <code>quote-local</code>, which forwards traffic only to pods running on the same node as the client pod.</p>
</div>
<div class="browsable-container listing-container" data-hash="bb7f64b7b32759be406573c14e1a1a11" data-text-hash="96f4457ef43854fb37ed2da4fe533387" id="385" refid="385">
<h5>Listing 11.9 A service that only forwards traffic to local endpoints</h5>
<div class="code-area-container">
<pre class="code-area">apiVersion: v1
kind: Service
metadata:
  name: quote-local
spec:
  internalTrafficPolicy: Local    #A
  selector:
    app: quote
  ports:
  - name: http
    port: 80
    targetPort: 80
    protocol: TCP</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhpcyBzZXJ2aWNlIGZvcndhcmQgdHJhZmZpYyBmcm9tIHBvZHMgb25seSB0byBlbmRwb2ludHMgb24gdGhlIHNhbWUgbm9kZSBhcyB0aGUgcG9k"></div>
</div>
</div>
<div class="readable-text" data-hash="4425dbe911af30f268d9c9147d1f8aa5" data-text-hash="d8561519f3bd21b615c849d495aee036" id="386" refid="386">
<p>As you can see in the manifest, the service will forward traffic to all pods with the label <code>app: quote</code>, but since <code>internalTrafficPolicy</code> is set to <code>Local</code>, it won&#8217;t forward traffic to all quote pods in the cluster, only to the pods that are on the same node as the client pod. Create the service by applying the manifest with <code>kubectl apply</code>.</p>
</div>
<div class="readable-text" data-hash="371c104514ba832a77af25ce0a307836" data-text-hash="1e0f9624bd0ed011413c72d1f21cfcfc" id="387" refid="387">
<h4>Observing node-local traffic routing</h4>
</div>
<div class="readable-text" data-hash="4c0b8eb7e79f852fb4977517f19e0239" data-text-hash="0d52ecd65335ac3c5887581ec0d13156" id="388" refid="388">
<p>Before you can see how the service routes traffic, you need to figure out where the client pods and the pods that are the endpoints of the service are located. List the pods with the <code>-o wide</code> option to see which node each pod is running on.</p>
</div>
<div class="readable-text" data-hash="a608834944902fbb22c96be27574c09a" data-text-hash="cca0ec441fd88b3f01b1260df8edc695" id="389" refid="389">
<p>Select one of the <code>kiada</code> pods and note its cluster node. Use <code>curl</code> to connect to the <code>quote-local</code> service from that pod. For example, my <code>kiada-001</code> pod runs on the <code>kind-worker</code> node. If I run <code>curl</code> in it multiple times, all requests are handled by the quote pods on the same node:</p>
</div>
<div class="browsable-container listing-container" data-hash="f7a66e87fc226fda96aba9d023a9896f" data-text-hash="766940cd35fdf109aa8251ffddad8312" id="390" refid="390">
<div class="code-area-container">
<pre class="code-area">$ kubectl exec kiada-001 -c kiada -- sh -c "while :; do curl -s quote-local; done"
This is the quote service running in pod quote-002 on node kind-worker    #A
This is the quote service running in pod quote-canary on node kind-worker    #A
This is the quote service running in pod quote-canary on node kind-worker    #A
This is the quote service running in pod quote-002 on node kind-worker    #A</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgQm90aCB0aGVzZSBwb2RzIHJ1biBvbiB0aGUgc2FtZSBub2RlIGFzIHRoZSBraWFkYS0wMDEgcG9kLg=="></div>
</div>
</div>
<div class="readable-text" data-hash="c5e406857f76e1d44e0b1121feebc4f6" data-text-hash="e10547f9fb7ee5d8bbe04c94be32f99a" id="391" refid="391">
<p>No request is forwarded to the pods on the other node(s). If I delete the two pods on the <code>kind-worker</code> node, the next connection attempt will fail:</p>
</div>
<div class="browsable-container listing-container" data-hash="6f9f9c490d15f39e8b7c76cb58d8a6f4" data-text-hash="92c73a27d4032ecaec624142cb56e613" id="392" refid="392">
<div class="code-area-container">
<pre class="code-area">$ kubectl exec -it kiada-001 -c kiada -- curl http://quote-local
curl: (7) Failed to connect to quote-local port 80: Connection refused</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="55703672c526544c6d134dc146256bff" data-text-hash="ac3906209167553f63d3c21fa5668399" id="393" refid="393">
<p>In this section, you learned how to forward traffic only to node-local endpoints when the semantics of the service require it. In other cases, you may want traffic to be forwarded preferentially to endpoints near the client pod, and only to more distant pods when needed. You&#8217;ll learn how to do this in the next section.</p>
</div>
<div class="readable-text" data-hash="77d0ec1f582b20834676b41e69d14f9f" data-text-hash="be8232d1a39a6cb7564f103ec9662f9c" id="394" refid="394">
<h3 id="sigil_toc_id_207">11.5.2&#160;Topology-aware hints</h3>
</div>
<div class="readable-text" data-hash="29a28f820f800859a950977e1c07a3ae" data-text-hash="adeccac4dccfa51df8eee26a709c5027" id="395" refid="395">
<p>Imagine the Kiada suite running in a cluster with nodes spread across multiple data centers in different zones and regions, as shown in the following figure. You don&#8217;t want a Kiada pod running in one zone to connect to Quote pods in another zone, unless there are no Quote pods in the local zone. Ideally, you want connections to be made within the same zone to reduce network traffic and associated costs.</p>
</div>
<div class="browsable-container figure-container" data-hash="74a5bd2bad719b7ef562121db188bc51" data-text-hash="be815f14e76879c541c60713617d9294" id="396" refid="396">
<h5>Figure 11.15 Routing serviced traffic across availability zones</h5>
<img alt="" data-processed="true" height="378" id="Picture_14" loading="lazy" src="EPUB/images/11image016.png" width="855">
</div>
<div class="readable-text" data-hash="3133c5a3efd23c00359783157bf682a2" data-text-hash="0dd9c620ba67e74c306ad40dacabfd50" id="397" refid="397">
<p>What was just described and illustrated in the figure is called <i>topology-aware traffic routing</i>. Kubernetes supports it by adding topology-aware hints to each endpoint in the EndpointSlice object.</p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="398" refid="398">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="f66da4835838c3628315c2038d137cab" data-text-hash="961d32ce730b051c8d91dcd03e71119d" id="399" refid="399">
<p> As of this writing, topology-aware hints are an alpha-level feature, so this could still change or be removed in the future.</p>
</div>
</div>
<div class="readable-text" data-hash="322c8f5385d96db1c5bf4535a3c0e6b0" data-text-hash="e785b7eb744415b02c7bc9cba7dc9207" id="400" refid="400">
<p>Since this feature is still in alpha, it isn&#8217;t enabled by default. Instead of explaining how to try it, I&#8217;ll just explain how it works.</p>
</div>
<div class="readable-text" data-hash="150e640723a7ff5f8a41d70ede48ba90" data-text-hash="56de9df8d3d6cc4015f7234e8303decc" id="401" refid="401">
<h4>Understanding how topology aware hints are calculated</h4>
</div>
<div class="readable-text" data-hash="5c93077686efe7744d68ed92189f38fa" data-text-hash="85a6b15fd82c235e2c47221c9ff99a5e" id="402" refid="402">
<p>First, all your cluster nodes must contain the <code>kubernetes.io/zone</code> label to indicate which zone each node is located in. To indicate that a service should use topology-aware hints, you must set the <code>service.kubernetes.io/topology-aware-hints</code> annotation to <code>Auto</code>. If the service has a sufficient number of endpoints, Kubernetes adds the hints to each endpoint in the EndpointSlice object(s). As you can see in the following listing, the <code>hints</code> field specifies the zones from which this endpoint is to be consumed.</p>
</div>
<div class="browsable-container listing-container" data-hash="ac3098a035c75be1695580d09bccf2ec" data-text-hash="cc8f31ee3067b512134914e29911fba3" id="403" refid="403">
<h5>Listing 11.10 EndpointSlice with topology aware hints</h5>
<div class="code-area-container">
<pre class="code-area">apiVersion: discovery.k8s.io/v1
kind: EndpointSlice
endpoints:
- addresses:
  - 10.244.2.2
  conditions:
    ready: true
  hints:    #A
    forZones:    #A
    - name: zoneA    #A
  nodeName: kind-worker
  targetRef:
    kind: Pod
    name: quote-002
    namespace: default
    resourceVersion: "944"
    uid: 03343161-971d-403c-89ae-9632e7cd0d8d
  zone: zoneA    #B
...</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhpcyBlbmRwb2ludCBzaG91bGQgYmUgY29uc3VtZWQgYnkgY2xpZW50cyBydW5uaW5nIGluIHpvbmVBLgojQiBUaGlzIGVuZHBvaW50IGlzIGxvY2F0ZWQgaW4gem9uZUEu"></div>
</div>
</div>
<div class="readable-text" data-hash="fa83b32b70164063deb8bbe511ff938a" data-text-hash="8030b977e57aab79c5667b7f0a72c679" id="404" refid="404">
<p>The listing shows only a single endpoint. The endpoint represents the pod <code>quote-002</code> running on node <code>kind-worker</code>, which is located in <code>zoneA</code>. For this reason, the <code>hints</code> for this endpoint indicate that it is to be consumed by pods in <code>zoneA</code>. In this particular case, only <code>zoneA</code> should use this endpoint, but the <code>forZones</code> array could contain multiple zones.</p>
</div>
<div class="readable-text" data-hash="78e084be56e0e6da13fbaa924f17154c" data-text-hash="6d14e01d8f62c015a0791f0ee20dcb56" id="405" refid="405">
<p>These hints are computed by the EndpointSlice controller, which is part of the Kubernetes control plane. It assigns endpoints to each zone based on the number of CPU cores that can be allocated in the zone. If a zone has a higher number of CPU cores, it&#8217;ll be assigned a higher number of endpoints than a zone with fewer CPU cores. In most cases, the hints ensure that traffic is kept within a zone, but to ensure a more even distribution, this isn&#8217;t always the case.</p>
</div>
<div class="readable-text" data-hash="ee74d9b4da890baeb8c9c7c23e725c24" data-text-hash="d0539da4af8d4b3a6d302d926a3741d2" id="406" refid="406">
<h4>Understanding where topology aware hints are used</h4>
</div>
<div class="readable-text" data-hash="d705494f074a94ac319c3186cc654179" data-text-hash="43078172922f541d13eba978988b778a" id="407" refid="407">
<p>Each node ensures that traffic sent to the service&#8217;s cluster IP is forwarded to one of the service&#8217;s endpoints. If there are no topology-aware hints in the EndpointSlice object, all endpoints, regardless of the node on which they reside, will receive traffic originating from a particular node. However, if all endpoints in the EndpointSlice object contain hints, each node processes only the endpoints that contain the node&#8217;s zone in the hints and ignores the rest. Traffic originating from a pod on the node is therefore forwarded to only some endpoints.</p>
</div>
<div class="readable-text" data-hash="d5ee1f9135c6ece1ce6a8272c2228ca2" data-text-hash="ce0667fd022b38a8d1dbcd6740744001" id="408" refid="408">
<p>Currently, you can&#8217;t influence topology-aware routing except to turn it on or off, but that may change in the future.</p>
</div>
<div class="readable-text" data-hash="cba94491cb3db882a67ee6892eed52fd" data-text-hash="d3aca31e5329058838a752b6934781f1" id="409" refid="409">
<h2 id="sigil_toc_id_208">11.6&#160;Managing the inclusion of a pod in service endpoints</h2>
</div>
<div class="readable-text" data-hash="4d939e4b7e14c9266b183136a5b86a6f" data-text-hash="6282ba2a7ef4c3a880d459e22ba66183" id="410" refid="410">
<p>There&#8217;s one more thing about services and endpoints that wasn&#8217;t covered yet. You learned that a pod is included as an endpoint of a service if its labels match the service&#8217;s label selector. Once a new pod with matching labels shows up, it becomes part of the service and connections are forwarded to the pod. But what if the application in the pod isn&#8217;t immediately ready to accept connections?</p>
</div>
<div class="readable-text" data-hash="c061cac8ca5a3a4b2f915ac7b7417ffc" data-text-hash="8b60d96adc92483b42f3041e532ad44f" id="411" refid="411">
<p>It may be that the application needs time to load either the configuration or the data, or that it needs to warm up so that the first client connection can be processed as quickly as possible without unnecessary latency caused by the fact that the application has just started. In such cases, you don&#8217;t want the pod to receive traffic immediately, especially if the existing pod instances can handle the traffic. It makes sense not to forward requests to a pod that&#8217;s just starting up until it becomes ready.</p>
</div>
<div class="readable-text" data-hash="0dd7998717edaff62726e55e8be8b918" data-text-hash="75500e56225bec71aac52cf9d0fbe047" id="412" refid="412">
<h3 id="sigil_toc_id_209">11.6.1&#160;Introducing readiness probes</h3>
</div>
<div class="readable-text" data-hash="5269d4379a6be426dfe81b7a6d8f9974" data-text-hash="f61ce38cf6519a161f95bb293294ed29" id="413" refid="413">
<p>In chapter 6, you learned how to keep your applications healthy by letting Kubernetes restart containers that fail their liveness probes. A similar mechanism called <i>readiness probes</i> allows an application to signal that it&#8217;s ready to accept connections.</p>
</div>
<div class="readable-text" data-hash="24e43e9904549bd34e16e0c09f47d6be" data-text-hash="10a3b0b88bb648c5487346ad5a5df792" id="414" refid="414">
<p>Like liveness probes, the Kubelet also calls the readiness probe periodically to determine the readiness status of the pod. If the probe is successful, the pod is considered ready. The opposite is true if it fails. Unlike liveness probes, a container whose readiness probe fails isn&#8217;t restarted; it&#8217;s only removed as an endpoint from the services to which it belongs.</p>
</div>
<div class="readable-text" data-hash="06655f37eb0cca5d36d81a66136defe5" data-text-hash="ceb038b56957e528b087cc4e95827dcf" id="415" refid="415">
<p>As you can see in the following figure, if a pod fails its readiness probe, the service doesn&#8217;t forward connections to the pod even though its labels match the label selector defined in the service.</p>
</div>
<div class="browsable-container figure-container" data-hash="92854b413643c488496001a4113212bd" data-text-hash="7b0ecbe0dee915435df8dd8703d14b1e" id="416" refid="416">
<h5>Figure 11.16 Pods that fail the readiness probe are removed from the service</h5>
<img alt="" data-processed="true" height="341" id="Picture_15" loading="lazy" src="EPUB/images/11image017.png" width="832">
</div>
<div class="readable-text" data-hash="b642a3a422681ffd2535c067dfca0c0b" data-text-hash="b14edea714f2976ccc015ff95e016cb5" id="417" refid="417">
<p>The notion of being ready is specific to each application. The application developer decides what readiness means in the context of their application. To do this, they expose an endpoint through which Kubernetes asks the application whether it&#8217;s ready or not. Depending on the type of endpoint, the correct readiness probe type must be used.</p>
</div>
<div class="readable-text" data-hash="a5bdf19ad1f1c18fe975296ad861bd80" data-text-hash="95c0c019b1ea045ee3b350ea4b15994a" id="418" refid="418">
<h4>Understanding readiness probe types</h4>
</div>
<div class="readable-text" data-hash="814cee8d62f5816cae8d2810514e44fd" data-text-hash="ef54412a8ade8cb0e8cfd884f0529753" id="419" refid="419">
<p>As with liveness probes, Kubernetes supports three types of readiness probes:</p>
</div>
<ul>
<li class="readable-text" data-hash="f3188a78131726da18be90ede78e9dc4" data-text-hash="2288614401b300f28a76cfb21f042824" id="420" refid="420">An <code class="codechar">exec</code> probe executes a process in the container. The exit code used to terminate the process determines whether the container is ready or not.</li>
<li class="readable-text" data-hash="b6b2862bac65ea0ba8548315aae2717d" data-text-hash="8e61417d3e973d860c898dca6e6e9eb2" id="421" refid="421">An <code>httpGet</code> probe sends a <code>GET</code> request to the container via HTTP or HTTPS. The response code determines the container&#8217;s readiness status.</li>
<li class="readable-text" data-hash="0a7791f5081da77dc3f1d55786f1cfbc" data-text-hash="a923aae4e3a4edb7c9adf193825ac1e9" id="422" refid="422">A <code>tcpSocket</code> probe opens a TCP connection to a specified port on the container. If the connection is established, the container is considered ready.</li>
</ul>
<div class="readable-text" data-hash="a88cfb3fa124edfc820c56a3122e44e0" data-text-hash="c9362f2d48723fb46e7bd2933cb34246" id="423" refid="423">
<h4>Configuring how often the probe is executed</h4>
</div>
<div class="readable-text" data-hash="85cf72b575b1c85be45da3a7b08a44f5" data-text-hash="11811cbd77a58ad0a2861aeadf15a8b9" id="424" refid="424">
<p>You may recall that you can configure when and how often the liveness probe runs for a given container using the following properties: <code>initialDelaySeconds</code>, <code>periodSeconds</code>, <code>failureThreshold</code>, and <code>timeoutSeconds</code>. These properties also apply to readiness probes, but they also support the additional <code>successThreshold</code> property, which specifies how many times the probe must succeed for the container to be considered ready.</p>
</div>
<div class="readable-text" data-hash="33e0f503fe9bd121758d73296bf9fa5f" data-text-hash="74b654b1dcf58ee211fbf55aabcd23cd" id="425" refid="425">
<p>These settings are best explained graphically. The following figure shows how the individual properties affect the execution of the readiness probe and the resulting readiness status of the container.</p>
</div>
<div class="browsable-container figure-container" data-hash="0abe7b56b2614b10744aedff7469fe8b" data-text-hash="9434b6ed59a3285ad19ed919f0312dd2" id="426" refid="426">
<h5>Figure 11.17 Readiness probe execution and resulting readiness status of the container</h5>
<img alt="" data-processed="true" height="323" id="Picture_16" loading="lazy" src="EPUB/images/11image018.png" width="881">
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="427" refid="427">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="08535eacd35b6e95add1b62b3e2d9e0f" data-text-hash="3445b64e96c8177b4d4567421a0cf39c" id="428" refid="428">
<p> If the container defines a startup probe, the initial delay for the readiness probe begins when the startup probe succeeds. Startup probes are explained in chapter 6.</p>
</div>
</div>
<div class="readable-text" data-hash="34d7afad4f0ce9f1cf175afc0c5a0afc" data-text-hash="da0d609798296a143b2b58cf2ab42b5e" id="429" refid="429">
<p>When the container is ready, the pod becomes an endpoint of the services whose label selector it matches. When it&#8217;s no longer ready, it&#8217;s removed from those services.</p>
</div>
<div class="readable-text" data-hash="e8413b4326833e327c0edba3086392db" data-text-hash="45c85053833e5bf4f0657485cb85a220" id="430" refid="430">
<h3 id="sigil_toc_id_210">11.6.2&#160;Adding a readiness probe to a pod</h3>
</div>
<div class="readable-text" data-hash="2385ff784bdaee62a1899391e4f43e43" data-text-hash="4720a59f630ce27e789d0e73b2dd0326" id="431" refid="431">
<p>To see readiness probes in action, create a new pod with a probe that you can switch from success to failure at will. This isn&#8217;t a real-world example of how to configure a readiness probe, but it allows you to see how the outcome of the probe affects the pod&#8217;s inclusion in the service.</p>
</div>
<div class="readable-text" data-hash="542e800c0eac71b1b8312069ba48350e" data-text-hash="6147a200a1f2568894a76b1992740dc0" id="432" refid="432">
<p>The following listing shows the relevant part of the pod manifest file <code>pod.kiada-mock-readiness.yaml</code>, which you can find in the book&#8217;s code repository.</p>
</div>
<div class="browsable-container listing-container" data-hash="6051492e57ba771e17a37b6e367fbe0c" data-text-hash="3f6724047c2ef98b8d08f4600d99a6d3" id="433" refid="433">
<h5>Listing 11.11 A readiness probe definition in a pod</h5>
<div class="code-area-container">
<pre class="code-area">apiVersion: v1
kind: Pod
...
spec:
  containers:
  - name: kiada
    ...
    readinessProbe:    #A
      exec:    #B
        command:    #B
        - ls    #B
        - /var/ready    #B
      initialDelaySeconds: 10    #C
      periodSeconds: 5    #C
      failureThreshold: 3    #C
      successThreshold: 2    #C
      timeoutSeconds: 2    #C
  ...</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgQSByZWFkaW5lc3MgcHJvYmUgaXMgZGVmaW5lZCBmb3IgdGhlIGtpYWRhIGNvbnRhaW5lci4KI0IgVGhlIHByb2JlIGV4ZWN1dGVzIHRoZSBscyBjb21tYW5kIGluIHRoZSBjb250YWluZXIuCiNDIFRoaXMgZGVmaW5lcyB3aGVuIGFuZCBob3cgb2Z0ZW4gdGhlIHByb2JlIGlzIGV4ZWN1dGVkLCBhbmQgaG93IG1hbnkgdGltZXMgaXQgbXVzdCBmYWlsIG9yIHN1Y2NlZWQgZm9yIHRoZSBjb250YWluZXLigJlzIHJlYWRpbmVzcyBzdGF0ZSB0byBjaGFuZ2UuIEl0IGFsc28gc2V0cyB0aGUgdGltZW91dCBmb3IgZWFjaCBpbnZvY2F0aW9uIG9mIHRoZSBwcm9iZS4="></div>
</div>
</div>
<div class="readable-text" data-hash="56f7e00361d4c93dc6bfa4f33fa4ddeb" data-text-hash="b2921f1cd59b1b907b90616f3f53b6e4" id="434" refid="434">
<p>The readiness probe periodically runs the <code>ls</code> <code>/var/ready</code> command in the <code>kiada</code> container. The <code>ls</code> command returns the exit code zero if the file exists, otherwise it&#8217;s nonzero. Since zero is considered a success, the readiness probe succeeds if the file is present.</p>
</div>
<div class="readable-text" data-hash="0284e3b614704087bc66fd7cd6c47b3c" data-text-hash="2e6fa05d5364b801d7df6b1bd34f25cb" id="435" refid="435">
<p>The reason to define such a strange readiness probe is so that you can change its outcome by creating or removing the file in question. When you create the pod, the file doesn&#8217;t exist yet, so the pod isn&#8217;t ready. Before you create the pod, delete all other kiada pods except <code>kiada-001</code>. This makes it easier to see the service endpoints change.</p>
</div>
<div class="readable-text" data-hash="4b371a9c1adbad997a7c2475db05a77c" data-text-hash="1b1908ea31fbabf1a30250619ed40b98" id="436" refid="436">
<h4>Observing the pods&#8217; readiness status</h4>
</div>
<div class="readable-text" data-hash="f4bf6b3bce4c32357da1c1c6ac87786a" data-text-hash="2e34c4d78a679feecc4b7f70e0050829" id="437" refid="437">
<p>After you create the pod from the manifest file, check its status as follows:</p>
</div>
<div class="browsable-container listing-container" data-hash="b03a11cb34f152e10a2a555d5a1519d9" data-text-hash="c57ba31fefd61cf6c69e09e0b8795b03" id="438" refid="438">
<div class="code-area-container">
<pre class="code-area">$ kubectl get po kiada-mock-readiness
NAME                   READY   STATUS    RESTARTS   AGE
kiada-mock-readiness   1/2     Running   0          1m    #A</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgT25seSBvbmUgb2YgdGhlIHBvZOKAmXMgY29udGFpbmVycyBpcyByZWFkeS4="></div>
</div>
</div>
<div class="readable-text" data-hash="89f2de349717bea9ddae1fe26709ad1c" data-text-hash="953cebe5e15025b10079a4f794ab5ae3" id="439" refid="439">
<p>The <code>READY</code> column shows that only one of the pod&#8217;s containers is ready. This is the <code>envoy</code> container, which doesn&#8217;t define a readiness probe. Containers without a readiness probe are considered ready as soon as they&#8217;re started.</p>
</div>
<div class="readable-text" data-hash="e6e3e202dab75336b1adce6ca68311b9" data-text-hash="418e1af672b48a8dc60bc61ab4a17e8e" id="440" refid="440">
<p>Since the pod&#8217;s containers aren&#8217;t all ready, the pod shouldn&#8217;t receive traffic sent to the service. You can check this by sending several requests to the kiada service. You&#8217;ll notice that all requests are handled by the <code>kiada-001</code> pod, which is the only active endpoint of the service. This is evident from the Endpoints and EndpointSlice objects associated with the service. For example, the <code>kiada-mock-readiness</code> pod appears in the <code>notReadyAddresses</code> instead of the <code>addresses</code> array in the Endpoints object:</p>
</div>
<div class="browsable-container listing-container" data-hash="3d28d7d9f03bd538c579e9d964306f5d" data-text-hash="6c6a60cd69c1635f406daa51f1572988" id="441" refid="441">
<div class="code-area-container">
<pre class="code-area">$ kubectl get endpoints kiada -o yaml
apiVersion: v1
kind: Endpoints
metadata:
  name: kiada
  ...
subsets:
- addresses:
  - ...
  notReadyAddresses:    #A
  - ip: 10.244.1.36    #A
    nodeName: kind-worker2    #A
    targetRef:    #A
      kind: Pod    #A
      name: kiada-mock-readiness    #A
      namespace: default    #A
    ...</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIGtpYWRhLW1vY2stcmVhZGluZXNzIHBvZCBhcHBlYXJzIGFtb25nIHRoZSBzZXJ2aWNl4oCZcyBub3RSZWFkeUFkZHJlc3Nlcy4="></div>
</div>
</div>
<div class="readable-text" data-hash="90ad4516fcca47d3777d95c605309b56" data-text-hash="3cd32be93c5bf6b81b257e882a81ef96" id="442" refid="442">
<p>In the EndpointSlice object, the endpoint&#8217;s <code>ready</code> condition is <code>false</code>:</p>
</div>
<div class="browsable-container listing-container" data-hash="49f0efdcefbef34a7c2d0973c9b943e5" data-text-hash="c4b6b94514bf726c27a617f0e64cdbe8" id="443" refid="443">
<div class="code-area-container">
<pre class="code-area">$ kubectl get endpointslices -l kubernetes.io/service-name=kiada -o yaml
apiVersion: v1
items:
- addressType: IPv4
  apiVersion: discovery.k8s.io/v1
  endpoints:
  - addresses:
    - 10.244.1.36
    conditions:    #A
      ready: false    #A
    nodeName: kind-worker2
    targetRef:
      kind: Pod
      name: kiada-mock-readiness
      namespace: default
      &#8230;</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIGtpYWRhLW1vY2stcmVhZGluZXNzIHBvZOKAmXMgcmVhZHkgY29uZGl0aW9uIGlzIGZhbHNlLg=="></div>
</div>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="444" refid="444">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="c2733e18c86a7ea13ce10a3e475a620e" data-text-hash="c7c9e1a370e6710c54e4f9c8b62cf329" id="445" refid="445">
<p> In some cases, you may want to disregard the readiness status of pods. This may be the case if you want all pods in a group to get <code>A</code>, <code>AAAA</code>, and <code>SRV</code> records even though they aren&#8217;t ready. If you set the <code>publishNotReadyAddresses</code> field in the Service object&#8217;s <code>spec</code> to <code>true</code>, non-ready pods are marked as ready in both the Endpoints and EndpointSlice objects. Components like the cluster DNS treat them as ready.</p>
</div>
</div>
<div class="readable-text" data-hash="508cad85d5e43a570e38b47ffea29d27" data-text-hash="20c8cb1efd73789446a73a77461f35d3" id="446" refid="446">
<p>For the readiness probe to succeed, create the <code>/var/ready</code> file in the container as follows:</p>
</div>
<div class="browsable-container listing-container" data-hash="318c08f448daab3bcad11ca084e6fcf0" data-text-hash="833159bdca07d80139d62c15baec56fd" id="447" refid="447">
<div class="code-area-container">
<pre class="code-area">$ kubectl exec kiada-mock-readiness -c kiada -- touch /var/ready</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="8647b208a0eb6344b8c7662410a07ea0" data-text-hash="60796a49f21e1e0846dae770849a3b73" id="448" refid="448">
<p>The <code>kubectl</code> <code>exec</code> command runs the <code>touch</code> command in the <code>kiada</code> container of the <code>kiada-mock-readiness</code> pod. The <code>touch</code> command creates the specified file. The container&#8217;s readiness probe will now be successful. All the pod&#8217;s containers should now show as ready. Verify that this is the case as follows:</p>
</div>
<div class="browsable-container listing-container" data-hash="7c2317090736fadd7d14211fac21f7e3" data-text-hash="5789f1c67b55b41ba2e42446e0546fc8" id="449" refid="449">
<div class="code-area-container">
<pre class="code-area">$ kubectl get po kiada-mock-readiness
NAME                   READY   STATUS    RESTARTS   AGE
kiada-mock-readiness   1/2     Running   0          10m</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="2eee1888360d821f0348e6db4986d2c7" data-text-hash="9e0549e4a0f42c85f883b93f594d2f24" id="450" refid="450">
<p>Surprisingly, the pod is still not ready. Is something wrong or is this the expected result? Take a closer look at the pod with <code>kubectl</code> <code>describe</code>. In the output you&#8217;ll find the following line:</p>
</div>
<div class="browsable-container listing-container" data-hash="671e6f48cf21f525e933a1dc0ce5321c" data-text-hash="04845d9645a7479be2baaa6dc2053c20" id="451" refid="451">
<div class="code-area-container">
<pre class="code-area">Readiness:   exec [ls /var/ready] delay=10s timeout=2s period=5s #success=2 #failure=3</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="ea2d0dcc33735c26ff9dd57c8b58e404" data-text-hash="c890b0635b79686fa4c164df15ac0cdf" id="452" refid="452">
<p>The readiness probe defined in the pod is configured to check the status of the container every 5 seconds. However, it&#8217;s also configured to require two consecutive probe attempts to be successful before setting the status of the container to ready. Therefore, it takes about 10 seconds for the pod to be ready after you create the <code>/var/ready</code> file.</p>
</div>
<div class="readable-text" data-hash="c9d04dcc58aa3dbd09f5802188ae5788" data-text-hash="b887db5146d510fad2c348bb27ad4756" id="453" refid="453">
<p>When this happens, the pod should become an active endpoint of the service. You can verify this is the case by examining the Endpoints or EndpointSlice objects associated with the service, or by simply accessing the service a few times and checking to see if the <code>kiada-mock-readiness</code> pod receives any of the requests you send.</p>
</div>
<div class="readable-text" data-hash="0cd06ee441d5dc3902fb8cfd6e02b8d8" data-text-hash="2b983d120cdaa0363b1f55f7a53d9b6c" id="454" refid="454">
<p>If you want to remove the pod from the service again, run the following command to remove the <code>/var/ready</code> file from the container:</p>
</div>
<div class="browsable-container listing-container" data-hash="48baf3229d966d176e3dbac0f5689d91" data-text-hash="63106d86c605042cde3ff8dd52698e71" id="455" refid="455">
<div class="code-area-container">
<pre class="code-area">$ kubectl exec kiada-mock-readiness -c kiada -- rm /var/ready</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="1c4e99d2475f105829774ef08dfda00e" data-text-hash="b99c39cf45b0c376d66105ec69a2c862" id="456" refid="456">
<p>This mockup of a readiness probe is just to show how readiness probes work. In the real world, the readiness probe shouldn&#8217;t be implemented in this way. If you want to manually remove pods from a service, you can do so by either deleting the pod or changing the pod&#8217;s labels rather than manipulating the readiness probe outcome.</p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="5c622e940054ac4ab45712e2d7b5d25d" data-text-hash="12ae2a12586001e30745cb0457586ae3" id="457" refid="457">
<h5>Tip</h5>
</div>
<div class="readable-text" data-hash="2b0dc98db66477083930e27a7cd95eb8" data-text-hash="060e7350b93eef4099a4b91bd46ec072" id="458" refid="458">
<p>&#8195;If you want to manually control whether or not a pod is included in a service, add a label key such as <code>enabled</code> to the pod and set its value to <code>true</code>. Then add the label selector <code>enabled=true</code> to your service. Remove the label from the pod to remove the pod from the service.</p>
</div>
</div>
<div class="readable-text" data-hash="ccd0a5e8475df0b5408931afbb96d023" data-text-hash="b6fab96ad02669e089855f155e144bed" id="459" refid="459">
<h3 id="sigil_toc_id_211">11.6.3&#160;Implementing real-world readiness probes</h3>
</div>
<div class="readable-text" data-hash="9424c0e3741b39d0ee4cffcb2f14d88c" data-text-hash="30cdd217cd469ff81d0d2f4813b7932d" id="460" refid="460">
<p>If you don&#8217;t define a readiness probe in your pod, it becomes a service endpoint as soon as it&#8217;s created. This means that every time you create a new pod instance, connections forwarded by the service to that new instance will fail until the application in the pod is ready to accept them. To prevent this, you should always define a readiness probe for the pod.</p>
</div>
<div class="readable-text" data-hash="47cc01932abf7d18eb51fa67762c269a" data-text-hash="f761466cc4469c5edab8f13c306e6d65" id="461" refid="461">
<p>In the previous section, you learned how to add a mock readiness probe to a container to manually control whether the pod is a service endpoint or not. In the real world, the readiness probe result should reflect the ability of the application running in the container to accept connections.</p>
</div>
<div class="readable-text" data-hash="b77356b572a3d48ba59fa67cf61806bc" data-text-hash="db2d3da7dd558682f2ad1f774237b968" id="462" refid="462">
<h4>Defining a minimal readiness probe</h4>
</div>
<div class="readable-text" data-hash="933e0302e5422e1bd7d135c242c3e7d3" data-text-hash="1f04b6fd56e31361bc3d0c8d138af402" id="463" refid="463">
<p>For containers running an HTTP server, it&#8217;s much better to define a simple readiness probe that checks whether the server responds to a simple <code>GET /</code> request, such as the one in the following snippet, than to have no readiness probe at all.</p>
</div>
<div class="browsable-container listing-container" data-hash="58b00d795c252ad20a5f3b7a02cff31c" data-text-hash="27be4224dffc144307fb800572d6af31" id="464" refid="464">
<div class="code-area-container">
<pre class="code-area">readinessProbe:
  httpGet:    #A
    port: 8080    #A
    path: /    #B
    scheme: HTTP    #B</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIHByb2JlIHNlbmRzIGFuIEhUVFAgR0VUIHJlcXVlc3QgdG8gcG9ydCA4MDgwIG9mIHRoZSBjb250YWluZXIuCiNCIFRoZSBwcm9iZSByZXF1ZXN0cyB0aGUgcm9vdCBVUkwgcGF0aCBvdmVyIEhUVFAgKGFzIG9wcG9zZWQgdG8gSFRUUFMpLg=="></div>
</div>
</div>
<div class="readable-text" data-hash="d8b401ebda29d9cbf3bf652365c8bc21" data-text-hash="f6aa0830595910308c1ce8c7f559314e" id="465" refid="465">
<p>When Kubernetes invokes this readiness probe, it sends the <code>GET /</code> request to port <code>8080</code> of the container and checks the returned HTTP response code. If the response code is greater than or equal to <code>200</code> and less than <code>400</code>, the probe is successful, and the pod is considered ready. If the response code is anything else (for example, <code>404</code> or <code>500</code>) or the connection attempt fails, the readiness probe is considered failed and the pod is marked as not ready.</p>
</div>
<div class="readable-text" data-hash="8bd4732a594b5939e6a6bfb375b05828" data-text-hash="32024af7d0b4656d97548bace5631f70" id="466" refid="466">
<p>This simple probe ensures that the pod only becomes part of the service when it can actually handle HTTP requests, rather than immediately when the pod is started.</p>
</div>
<div class="readable-text" data-hash="a4a21f40ce737773e2226fbf37dfd983" data-text-hash="51dcbfc85f8ebd686be28acd6d7da46f" id="467" refid="467">
<h4>Defining a better readiness probe</h4>
</div>
<div class="readable-text" data-hash="0213f9eef6b8511bf3e2e0590f8a4824" data-text-hash="8aaec4909e0d182a46f00e2909648a2f" id="468" refid="468">
<p>A simple readiness probe like the one shown in the previous section isn&#8217;t always sufficient. Take the Quote pod, for example. You may recall that it runs two containers. The <code>quote-writer</code> container selects a random quote from this book and writes it to a file called <code>quote</code> in the volume shared by the two containers. The <code>nginx</code> container serves files from this shared volume. Thus, the quote itself is available at the URL path <code>/quote</code>.</p>
</div>
<div class="readable-text" data-hash="72730fcbaf479b8cb216e3320d864073" data-text-hash="37ef75fde954d7dfd26a1dc58e05314a" id="469" refid="469">
<p>The purpose of the Quote pod is clearly to provide a random quote from the book. Therefore, it shouldn&#8217;t be marked ready until it can serve this quote. If you direct the readiness probe to the URL path <code>/</code>, it&#8217;ll succeed even if the <code>quote-writer</code> container hasn&#8217;t yet created the <code>quote</code> file. Therefore, the readiness probe in the Quote pod should be configured as shown in the following snippet from the <code>pod.quote-readiness.yaml</code> file:</p>
</div>
<div class="browsable-container listing-container" data-hash="09f971857fd066ed234fb830aff0f4fe" data-text-hash="07cee32b6bb447246afa61d0f3533b39" id="470" refid="470">
<div class="code-area-container">
<pre class="code-area">readinessProbe:
  httpGet: 
    port: 80
    path: /quote    #A
    scheme: HTTP
  failureThreshold: 1   #B</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIFF1b3RlIHBvZCBpcyByZWFkeSB3aGVuIGl0IGNhbiBzZXJ2ZSB0aGUgcXVvdGUuCiNCIFNldCB0aGUgZmFpbHVyZSB0aHJlc2hvbGQgdG8gb25lLCBzbyB0aGF0IHRoZSBwb2QgaXMgaW1tZWRpYXRlbHkgbWFya2VkIGFzIG5vdCByZWFkeSBpZiB0aGUgcHJvYmUgZmFpbHMu"></div>
</div>
</div>
<div class="readable-text" data-hash="ee56c337da10613cab2a7810f87e0819" data-text-hash="a81a77ff538245788b8e9f3d453f2f9a" id="471" refid="471">
<p>If you add this readiness probe to your Quote pod, you&#8217;ll see that the pod is only ready when the <code>quote</code> file exists. Try deleting the file from the pod as follows:</p>
</div>
<div class="browsable-container listing-container" data-hash="7187f45186d55ec6fc4ade0742b37c00" data-text-hash="13d5f0a90d71ad58d8d2e82db5ab9f5d" id="472" refid="472">
<div class="code-area-container">
<pre class="code-area">$ kubectl exec quote-readiness -c quote-writer -- rm /var/local/output/quote</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="d5bb08e6984d34f4231199f4f0d0f9da" data-text-hash="de854cf8b10cf72c89db567842050595" id="473" refid="473">
<p>Now check the pod&#8217;s readiness status with <code>kubectl get pod</code> and you&#8217;ll see that one of the containers is no longer ready. When the <code>quote-writer</code> recreates the file, the container becomes ready again. You can also inspect the endpoints of the <code>quote</code> service with <code>kubectl get endpoints quote</code> to see that the pod is removed and then re-added.</p>
</div>
<div class="readable-text" data-hash="d7933734dbb766d469c2f5492ada491a" data-text-hash="340dad018567a01a821e4d97db4215cb" id="474" refid="474">
<h4>Implementing a dedicated readiness endpoint</h4>
</div>
<div class="readable-text" data-hash="61886cea46a681521b582aa835c8bf08" data-text-hash="cb46e9ed8509e88a46b7628621ce6f16" id="475" refid="475">
<p>As you saw in the previous example, it may be sufficient to point the readiness probe to an existing path served by the HTTP server, but it&#8217;s also common for an application to provide a dedicated endpoint, such as <code>/healthz/ready</code> or <code>/readyz</code>, through which it reports its readiness status. When the application receives a request on this endpoint, it can perform a series of internal checks to determine its readiness status.</p>
</div>
<div class="readable-text" data-hash="61348f88faa8038befc1f12ca46d3808" data-text-hash="7673fdeeb7c18220278e3ed2517bf4a5" id="476" refid="476">
<p>Let&#8217;s take the Quiz service as an example. The Quiz pod runs both an HTTP server and a MongoDB container. As you can see in the following listing, the <code>quiz-api</code> server implements the <code>/healthz/ready</code> endpoint. When it receives a request, it checks if it can successfully connect to MongoDB in the other container. If so, it responds with a <code>200 OK</code>. If not, it returns <code>500 Internal Server Error</code>.</p>
</div>
<div class="browsable-container listing-container" data-hash="4ba8e43068fed50c978ac6a68f85f2ef" data-text-hash="8102105df42e9570a59dee8635946401" id="477" refid="477">
<h5>Listing 11.12: The readiness endpoint in the quiz-api application</h5>
<div class="code-area-container">
<pre class="code-area">func (s *HTTPServer) ListenAndServe(listenAddress string) {
    router := mux.NewRouter()
    router.Methods("GET").Path("/").HandlerFunc(s.handleRoot)
    router.Methods("GET").Path("/healthz/ready").HandlerFunc(s.handleReadiness)    #A
    ...
}
 
func (s *HTTPServer) handleReadiness(res http.ResponseWriter, req *http.Request) {
    conn, err := s.db.Connect()    #B
    if err != nil {    #C
        res.WriteHeader(http.StatusInternalServerError)    #C
        _, _ = fmt.Fprintf(res, &#8220;ERROR: %v\n&#8221;, err.Error())    #C
        return    #C
    }
    defer conn.Close()
 
    res.WriteHeader(http.StatusOK)    #D
    _, _ = res.Write([]byte("Readiness check successful"))    #D
}</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIC9oZWFsdGh6L3JlYWR5IGVuZHBvaW50IGludm9rZXMgdGhlIGhhbmRsZVJlYWRpbmVzcygpIGZ1bmN0aW9uLgojQiBUcnkgdG8gY29ubmVjdCB0byBNb25nb0RCLgojQyBJZiB0aGUgY29ubmVjdGlvbiBmYWlscywgdGhlIDUwMCBJbnRlcm5hbCBTZXJ2ZXIgRXJyb3IgcmVzcG9uc2UgY29kZSBpcyByZXR1cm5lZC4KI0QgSWYgdGhlIGNvbm5lY3Rpb24gc3VjY2VlZHMsIHRoZSAyMDAgT0sgcmVzcG9uc2UgY29kZSBpcyByZXR1cm5lZC4="></div>
</div>
</div>
<div class="readable-text" data-hash="de43d55b01eee72b6f153f3fa5a0b732" data-text-hash="6ecaeef25df2c8cf3108a39e41f1e27f" id="478" refid="478">
<p>The readiness probe defined in the Quiz pod ensures that everything the pod needs to provide its services is present and working. As additional components are added to the quiz-api application, further checks can be added to the readiness check code. An example of this is the addition of an internal cache. The readiness endpoint could check to see if the cache is warmed up, so that only then is the pod exposed to clients.</p>
</div>
<div class="readable-text" data-hash="d21d2b118d3b98362b8a1bf0ae05d0c4" data-text-hash="aa6d1cc7c7b3df7a9a4abeb5b3ab6230" id="479" refid="479">
<h4>Checking dependencies in the readiness probe</h4>
</div>
<div class="readable-text" data-hash="e05124772eeb8a67b018cef898389f54" data-text-hash="d16c2f4cd741fee641d1b170f6a91070" id="480" refid="480">
<p>In the Quiz pod, the MongoDB database is an internal dependency of the <code>quiz-api</code> container. The Kiada pod, on the other hand, depends on the Quiz and Quote services, which are external dependencies. What should the readiness probe check in the Kiada pod? Should it check whether it can reach the Quote and Quiz services?</p>
</div>
<div class="readable-text" data-hash="e94d26dc88eebdda06dc8b7feeca7f30" data-text-hash="1e2a8a8370a66d62d812fb656486293d" id="481" refid="481">
<p>The answer to this question is debatable, but any time you check dependencies in a readiness probe, you must consider what happens if a transient problem, such as a temporary increase in network latency, causes the probe to fail.</p>
</div>
<div class="readable-text" data-hash="16f15e2ae359db876fead08c19a1f2d8" data-text-hash="129571becead5afc66e7af63c09d8435" id="482" refid="482">
<p>Note that the <code>timeoutSeconds</code> field in the readiness probe definition limits the time the probe has to respond. The default timeout is only one second. The container must respond to the readiness probe in this time.</p>
</div>
<div class="readable-text" data-hash="911b1e99904f6a1e0bef80983c0619fd" data-text-hash="7e1ed9bed364dfb36f7c5cfd34535a44" id="483" refid="483">
<p>If the Kiada pod calls the other two services in its readiness check, but their responses are only slightly delayed due to a transient network disruption, its readiness probe fails and the pod is removed from the service endpoints. If this happens to all Kiada pods at the same time, there will be no pods left to handle client requests. The disruption may only last a second, but the pods may not be added back to the service until dozens of seconds later, depending on how the <code>periodSeconds</code> and <code>successThreshold</code> properties are configured.</p>
</div>
<div class="readable-text" data-hash="fb36878e903423cd37866036b5a2668e" data-text-hash="15f18ad70fbcd631196eeb0d309a29f5" id="484" refid="484">
<p>When you check external dependencies in your readiness probes, you should consider what happens when these types of transient network problems occur. Then you should set your periods, timeouts, and thresholds accordingly.</p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="5c622e940054ac4ab45712e2d7b5d25d" data-text-hash="12ae2a12586001e30745cb0457586ae3" id="485" refid="485">
<h5>Tip</h5>
</div>
<div class="readable-text" data-hash="af446c526bcef3966a7fd17dd028ff0a" data-text-hash="3c02fc2caea6df4f1dc246448758fa88" id="486" refid="486">
<p> Readiness probes that try to be too smart can cause more problems than they solve. As a rule of thumb, readiness probes shouldn&#8217;t test external dependencies, but can test dependencies within the same pod.</p>
</div>
</div>
<div class="readable-text" data-hash="42c4bc67b512771ae9020cda83610632" data-text-hash="ea7bac7331c8682ac25d7c185fd1de86" id="487" refid="487">
<p>The Kiada application also implements the <code>/healthz/ready</code> endpoint instead of having the readiness probe use the <code>/</code> endpoint to check its status. This endpoint simply responds with the HTTP response code <code>200 OK</code> and the word <code>Ready</code> in the response body. This ensures that the readiness probe only checks that the application itself is responding, without also connecting to the Quiz or Quote services. You can find the pod manifest in the <code>pod.kiada-readiness.yaml</code> file.</p>
</div>
<div class="readable-text" data-hash="7ce9519c2c0677c81aed115e2a0d299b" data-text-hash="7ee7a137a48d2de06ee056020ea1c36e" id="488" refid="488">
<h4>Understanding readiness probes in the context of pod shutdown</h4>
</div>
<div class="readable-text" data-hash="49c229980d627dfdd5b6998f0dd97f15" data-text-hash="8bccfca22b6f6b22a877f53ceaa94e0c" id="489" refid="489">
<p>One last note before you close this chapter. As you know, readiness probes are most important when the pod starts, but they also ensure that the pod is taken out of service when something causes it to no longer be ready during normal operation. But what about when the pod is terminating? A pod that&#8217;s in the process of shutting down shouldn&#8217;t be part of any services. Do you need to consider that when implementing the readiness probe?</p>
</div>
<div class="readable-text" data-hash="f578f4d74bf5e1ee7c65f7fd99861928" data-text-hash="68acc280d5b79c74af9038b9bf521d7d" id="490" refid="490">
<p>Fortunately, when you delete a pod, Kubernetes not only sends the termination signal to the pod&#8217;s containers, but also removes the pod from all services. This means you don&#8217;t have to make any special provisions for terminating pods in your readiness probes. You don&#8217;t have to make sure that the probe fails when your application receives the termination signal.</p>
</div>
<div class="readable-text" data-hash="30cc3b37b4b529a13cc27ff1f3c5711c" data-text-hash="375e70277eabcaa811df63d384eac7d7" id="491" refid="491">
<h2 id="sigil_toc_id_212">11.7&#160;Summary</h2>
</div>
<div class="readable-text" data-hash="2b8ea4fa48b97782593ce2206dd64445" data-text-hash="45aacda223e6ccf8db2dc760afb3b0cc" id="492" refid="492">
<p>In this chapter, you finally connected the Kiada pods to the Quiz and Service pods. Now you can use the Kiada suite to test the knowledge you&#8217;ve acquired so far and refresh your memory with quotes from this book. In this chapter, you learned that:</p>
</div>
<ul>
<li class="readable-text" data-hash="cb71c054ee29daa79bf619a0efef7d24" data-text-hash="cb71c054ee29daa79bf619a0efef7d24" id="493" refid="493">Pods communicate over a flat network that allows any pod to reach any other pod in the cluster, regardless of the actual network topology connecting the cluster nodes.</li>
<li class="readable-text" data-hash="d5f04ad991203f295f268170fc3f4b13" data-text-hash="d5f04ad991203f295f268170fc3f4b13" id="494" refid="494">A Kubernetes service makes a group of pods available under a single IP address. While the IPs of the pods may change, the IP of the service remains constant.</li>
<li class="readable-text" data-hash="a71782df580aa15dc4aa3d60769c83a5" data-text-hash="a71782df580aa15dc4aa3d60769c83a5" id="495" refid="495">The cluster IP of the service is reachable from inside the cluster, but NodePort and LoadBalancer services are also accessible from outside the cluster.</li>
<li class="readable-text" data-hash="d0398150441c3d1289b1287648678b26" data-text-hash="d0398150441c3d1289b1287648678b26" id="496" refid="496">Service endpoints are either determined by a label selector specified in the Service object or configured manually. These endpoints are stored in the Endpoints and EndpointSlice objects.</li>
<li class="readable-text" data-hash="5df317572d376ada63387e3db86afd96" data-text-hash="36319bf4db70c797da1c4f15bbff57df" id="497" refid="497">Client pods can find services using the cluster DNS or environment variables. Depending on the type of Service, the following DNS records may be created: <code>A</code>, <code>AAAA</code>, <code>SRV</code>, and <code>CNAME</code>.</li>
<li class="readable-text" data-hash="ac422713d5f32a76ea7e5789f93b1423" data-text-hash="ac422713d5f32a76ea7e5789f93b1423" id="498" refid="498">Services can be configured to forward external traffic only to pods on the same node that received the external traffic, or to pods anywhere in the cluster. They can also be configured to route internal traffic only to pods on the same node as the pod from which the traffic originates from. Topology-aware routing ensures that traffic isn&#8217;t routed across availability zones when a local pod can provide the requested service.</li>
<li class="readable-text" data-hash="ef6bf71329e493d69611e0ba207a6f6c" data-text-hash="ef6bf71329e493d69611e0ba207a6f6c" id="499" refid="499">Pods don&#8217;t become service endpoints until they&#8217;re ready. By implementing a readiness probe handler in an application, you can define what readiness means in the context of that particular application.</li>
</ul>
<div class="readable-text" data-hash="56c3818e256b0c0fd09c60164cd95c3f" data-text-hash="ae4b851ff91b73c2e459cfc56d8d49c4" id="500" refid="500">
<p>In the next chapter, you&#8217;ll learn how to use Ingress objects to make multiple services accessible through a single external IP address.</p>
</div></div>

        </body>
        
        