
        <html lang="en">
        <head>
        <meta charset="UTF-8"/>
        </head>
        <body>
        <div><div class="readable-text" data-hash="d70613c6c0fce242497a5497eeefc34d" data-text-hash="98cadc4dfcd5aab0656b2b64949fba4e" id="1" refid="1">
<h1>15 Deploying stateful workloads with StatefulSets</h1>
</div>
<div class="introduction-summary">
<h3 class="intro-header">This chapter covers</h3>
<ul>
<li class="readable-text" data-hash="86e719c87d875c395bbf5a5a94fdffcf" data-text-hash="a0b94e91cd51333393ac9835fe9acc9c" id="2" refid="2"> <span>Managing stateful workloads via StatefulSet objects</span></li>
<li class="readable-text" data-hash="99c3620f8d481e7a1719adb5b1efc1dc" data-text-hash="e276989b0cb4812e5988436ff5535c43" id="3" refid="3"> <span>Exposing individual Pods via headless Services</span></li>
<li class="readable-text" data-hash="6a3e4ffc812f72f9af48cd09b1c6edb1" data-text-hash="1be9cbe424126a75880b86bd09fe340b" id="4" refid="4"> <span>Understanding the difference between Deployments and StatefulSets</span></li>
<li class="readable-text" data-hash="73d1bf61ba03b8d6f4c0c93f82a82f77" data-text-hash="1eb35c7f7c40dd58bef73e7d84a8e3dc" id="5" refid="5"> <span>Automating stateful workload management with Kubernetes Operators</span></li>
</ul>
</div>
<div class="readable-text" data-hash="18775b9e91de1cff09f149947141407d" data-text-hash="148163e98720bae85632b6a192a696a2" id="6" refid="6">
<p><span>Each of the three services in your Kiada suite is now deployed via a Deployment object. The Kiada and Quote services each have three replicas, while the Quiz service has only one because its data doesn&#8217;t allow it to scale easily. In this chapter, you&#8217;ll learn how to properly deploy and scale stateful workloads like the Quiz service with a <i>StatefulSet</i>.</span></p>
</div>
<div class="readable-text" data-hash="b82d48cb64b4566d4692fb70f63f5714" data-text-hash="7fd9816631ce83aae400c736b67c624d" id="7" refid="7">
<p><span>Before you begin, create the</span> <code>kiada</code> <span>Namespace, change to the</span> <code>Chapter15/</code> <span>directory and apply all manifests in the</span> <code>SETUP/</code> <span>directory with the following command:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="986233557cbdd07da49d5c8a01eb07c5" data-text-hash="e783dfd21bd1a0975421ed003b94e106" id="8" refid="8">
<div class="code-area-container">
<pre class="code-area">$ kubectl apply -n kiada -f SETUP -R</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="13f2c6c2da8a5d923d4f021dffdd99c0" data-text-hash="279ac5dd73c3a5783ebeb10e172c4f38" id="9" refid="9">
<h5>IMPORTANT</h5>
</div>
<div class="readable-text" data-hash="dc7f63b8b2e12e7695414d541ca81a22" data-text-hash="02fff787e8c57c7917015545d4876c2d" id="10" refid="10">
<p> <span>The examples in this chapter assume that the objects are created in the</span> <code>kiada</code> <span>Namespace. If you create them in a different location, you must update the DNS domain names in several places.</span></p>
</div>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="260cc6dcef2c22785feb4596e3fe5a61" data-text-hash="10de4bc81f754b19b0d27246a0589c05" id="11" refid="11">
<h5>NOTE</h5>
</div>
<div class="readable-text" data-hash="76622ed893d85b693010fd03b6676b5b" data-text-hash="e2708c810c9d419d516641906821e43d" id="12" refid="12">
<p> <span>You can find the code files for this chapter at <a href="master.html">https://github.com/luksa/kubernetes-in-action-2nd-edition/tree/master/Chapter15</a>.</span></p>
</div>
</div>
<div class="readable-text" data-hash="e71c2bc6a8b1d6a051e766d122e24137" data-text-hash="aa92c95b5d459e704002d703ecd9c64d" id="13" refid="13">
<h2 id="sigil_toc_id_267">15.1&#160;Introducing StatefulSets</h2>
</div>
<div class="readable-text" data-hash="2e048ad1582fb081ad7b4ae59d35c00d" data-text-hash="d5b763739fad8b7e1fc477f071412f06" id="14" refid="14">
<p><span>Before you learn about StatefulSets and how they differ from Deployments, it&#8217;s good to know how the requirements of stateful workloads differ from those of their stateless counterparts.</span></p>
</div>
<div class="readable-text" data-hash="f1e48d106d59a3d021dd36e82736b535" data-text-hash="722fe1f0d8b1861f758b30e08a93d8cc" id="15" refid="15">
<h3 id="sigil_toc_id_268">15.1.1&#160;<span>Understanding</span> <span>stateful workload</span> <span>requirements</span></h3>
</div>
<div class="readable-text" data-hash="682d97d400dfd8ddbea5e87ec11da244" data-text-hash="48154a8620c3e1209e2ad5b46d1af7cd" id="16" refid="16">
<p><span>A stateful workload is a piece of software that must store and maintain state in order to function. This state must be maintained when the workload is restarted or relocated. This makes stateful workloads much more difficult to operate.</span></p>
</div>
<div class="readable-text" data-hash="f82c9a45f6a3727726bfd69aeeebc588" data-text-hash="448841f5eab44cf2b5e6d3021d39694b" id="17" refid="17">
<p><span>Stateful workloads</span> <span>are</span> <span>also</span> <span>much harder to scale</span> <span>because you can&#8217;t simply add and remove replicas without considering their state, as you can with stateless workloads. If the replicas can share state by reading and writing the same files, adding new replicas isn&#8217;t a problem. However, for this to be possible, the underlying storage technology must support it. On the other hand, if each replica stores its state in its own files, you&#8217;ll need to allocate a separate volume for each replica. With the Kubernetes resources you&#8217;ve encountered so far, this is easier said than done. Let&#8217;s look at these two options to understand the issues associated with both.</span></p>
</div>
<div class="readable-text" data-hash="388a4f254070fcd7dd94a001783372ce" data-text-hash="ad36fc52ad0b6ff56732fc1eea53bd18" id="18" refid="18">
<h4>Sharing state across multiple Pod replicas</h4>
</div>
<div class="readable-text" data-hash="d24f99c24976916a328b000e5ef8d52d" data-text-hash="137a6448240f2d761cd27effd62b63eb" id="19" refid="19">
<p><span>In Kubernetes,</span> <span>you can use</span> <span>PersistentVolumes</span> <span>with the</span> <code>ReadWriteMany</code> <span>access mode to</span> <span>share data</span> <span>across</span> <span>multiple Pods</span><span>. However,</span> <span>in most cloud environments, the underlying storage technology</span> <span>typically only supports the</span> <code>ReadWriteOnce</code> <span>and</span> <code>ReadOnlyMany</code> <span>access modes, not</span> <code>ReadWriteMany</code><span>, meaning you can&#8217;t mount the volume on multiple nodes in read/write mode. Therefore, Pods on different nodes can&#8217;t read and write to the same PersistentVolume.</span></p>
</div>
<div class="readable-text" data-hash="04808e5cdea368aba0283708a8379e6d" data-text-hash="5e3da5ae67dfb9137388607947a37d14" id="20" refid="20">
<p><span>Let&#8217;s demonstrate this problem using the Quiz service. Can</span> <span>you scale</span> <span>the</span> <code>quiz</code> <span>Deployment to, say, three replicas? Let&#8217;s see what happens. The</span> <code>kubectl scale</code> <span>command is as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="b152404bbbff27ded8efc98697df7eb3" data-text-hash="4a496fb4cc201a5028b49bf491ef987c" id="21" refid="21">
<div class="code-area-container">
<pre class="code-area">$ kubectl scale deploy quiz --replicas 3
deployment.apps/quiz scaled</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="8fbe310515d0a96749232af7706f04f0" data-text-hash="5af755a19107e44af877d0e4ab4908b9" id="22" refid="22">
<p><span>Now check the Pods like so:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="0e604dedbcff6d3ec7e897e7d0ef50cd" data-text-hash="b57f414808edc5b51812d000cfa510ce" id="23" refid="23">
<div class="code-area-container">
<pre class="code-area">$ kubectl get pods -l app=quiz
NAME                   READY   STATUS             RESTARTS      AGE
quiz-6f4968457-2c8ws   2/2     Running            0             10m    #A
quiz-6f4968457-cdw97   0/2     CrashLoopBackOff   1 (14s ago)   22s    #B
quiz-6f4968457-qdn29   0/2     Error              2 (16s ago)   22s    #B</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIGZpcnN0IFBvZCBydW5zIGZpbmUuCiNCIFRoZSB0d28gUG9kcyB0aGF0IHdlcmUganVzdCBjcmVhdGVkIGFyZSBjcmFzaGluZy4="></div>
</div>
</div>
<div class="readable-text" data-hash="285360455812e4ca5711694a10c0306e" data-text-hash="75078fb2aad66f234d77f5dc76a5ae4d" id="24" refid="24">
<p><span>As you can see, only the Pod that existed before the scale-up is running, while the two new Pods aren&#8217;t. Depending on the type of cluster you&#8217;re using, these two Pods may not start at all, or they may start but immediately terminate with an error message. For example, in a cluster on Google Kubernetes Engine, the containers in the Pods don&#8217;t start because the PersistentVolume can&#8217;t be attached to the new Pods because its access mode is</span> <code>ReadWriteOnce</code> <span>and the volume can&#8217;t be attached to multiple nodes at once. In kind-provisioned clusters, the containers start, but the mongo container fails with an error message, which you can see as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="c6f4550e2859b755abf20e517f8e99b2" data-text-hash="0e1f2db69a82b9ae79cd919cb864134b" id="25" refid="25">
<div class="code-area-container">
<pre class="code-area">$ kubectl logs quiz-6f4968457-cdw97 -c mongo #A
..."msg":"DBException in initAndListen, terminating","attr":{"error":"DBPathInUse: Unable to lock the lock file: /data/db/mongod.lock (Resource temporarily unavailable). Another mongod instance is already running on the /data/db directory"}}</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgUmVwbGFjZSB0aGUgUG9kIG5hbWUgd2l0aCB0aGUgbmFtZSBvZiB5b3VyIFBvZHMu"></div>
</div>
</div>
<div class="readable-text" data-hash="5ecee78369dd8ad0373f2026c7934911" data-text-hash="f825441233c332c559f24f5f6c68cb12" id="26" refid="26">
<p><span>The error message indicates that you can&#8217;t use the same data directory in multiple instances of MongoDB. The three</span> <code>quiz</code> <span>Pods use the same directory because they all use the same PersistentVolumeClaim and therefore the same PersistentVolume, as illustrated in the next figure.</span></p>
</div>
<div class="browsable-container figure-container" data-hash="963dc7e5f81b2cb2c09825498077a177" data-text-hash="7dd841f504d04c1042fad4820719f8b0" id="27" refid="27">
<h5><span>Figure 15.1 All Pods from a Deployment use the same PersistentVolumeClaim and PersistentVolume.</span></h5>
<img alt="" data-processed="true" height="300" id="Picture_2" loading="lazy" src="EPUB/images/15_img_0001.png" width="849">
</div>
<div class="readable-text" data-hash="8069d9df1533d6666ad446b0766a49eb" data-text-hash="7c06c798216468387d7c578b18b7df30" id="28" refid="28">
<p><span>Since this approach doesn&#8217;t work, the alternative is to use a separate PersistentVolume for each Pod replica. Let&#8217;s look at what this means and whether you can do it with a single Deployment object.</span></p>
</div>
<div class="readable-text" data-hash="5131a0e3864bd66961a82e8d2f1bec77" data-text-hash="438fc2f14532526d5b275e8488657862" id="29" refid="29">
<h4>Using a dedicated PersistentVolume for each replica</h4>
</div>
<div class="readable-text" data-hash="6dba2bed9dd7ae789da233866c705e20" data-text-hash="3bc3d35b5c023cbacbbc8db3f04e4c84" id="30" refid="30">
<p><span>As you learned in the previous section, MongoDB only supports a single instance by default. If you want to deploy multiple MongoDB instances with the same data, you must create a MongoDB <i>replica set</i> that replicates the data across those instances (here the term &#8220;replica set&#8221; is a MongoDB-specific term and doesn&#8217;t refer to the Kubernetes ReplicaSet resource). Each instance needs its own storage volume and a stable address that other replicas and clients can use to connect to it. Therefore, to deploy a MongoDB replica set in Kubernetes, you need to ensure that:</span></p>
</div>
<ul>
<li class="readable-text" data-hash="3cb22f682d067bcc766663a26bed93dc" data-text-hash="7f1ed8b1be909abae5b2dc258d11cf95" id="31" refid="31"><span>each Pod has its own PersistentVolume,</span></li>
<li class="readable-text" data-hash="b68b634a944cb7d6a43a2578574f84ad" data-text-hash="6f95f2923d2b26529f3785671f459db6" id="32" refid="32"><span>each Pod is addressable by its own unique address,</span></li>
<li class="readable-text" data-hash="0996ec6bd89d118c555eab7c975b0f11" data-text-hash="a5717fd7d3490142f0b5f3978f3dcc48" id="33" refid="33"><span>when a Pod is deleted and replaced, the new Pod is assigned the same address and PersistentVolume.</span></li>
</ul>
<div class="readable-text" data-hash="324abf7beb2d6f474aa9016b8488b7b3" data-text-hash="931b91fb9fe43053012a7cc129e2fe07" id="34" refid="34">
<p><span>You can&#8217;t do this with a single Deployment and Service, but you can do it by creating a separate Deployment, Service, and PersistentVolumeClaim for each replica, as shown in the following figure.</span></p>
</div>
<div class="browsable-container figure-container" data-hash="3fccce5a11402f8dc597401135405a4b" data-text-hash="8587f7930202a559c00e10bc357276fb" id="35" refid="35">
<h5><span>Figure 15.2 Providing each replica with its own volume and address.</span></h5>
<img alt="" data-processed="true" height="465" id="Picture_10" loading="lazy" src="EPUB/images/15_img_0002.png" width="897">
</div>
<div class="readable-text" data-hash="50742af8ecbfffb073719d3409f56f15" data-text-hash="aef109f0a3bde46c5a685b409f444306" id="36" refid="36">
<p><span>Each Pod has its own Deployment, so the Pod can use its own PersistentVolumeClaim and PersistentVolume. The Service associated with each replica gives it a stable address that always resolves to the IP address of the Pod, even if the Pod is deleted and recreated elsewhere. This is necessary because with MongoDB, as with many other distributed systems, you must specify the address of each replica when you initialize the replica set. In addition to these per-replica Services, you may need yet another Service to make all Pods accessible to clients at a single address. So, the whole system looks daunting.</span></p>
</div>
<div class="readable-text" data-hash="9910c44bac40676579693c7064b545c8" data-text-hash="08b953669cb5a21d1b6972ab146a8c2e" id="37" refid="37">
<p><span>It gets worse from here. If you need to increase the number of replicas, you can&#8217;t use the</span> <code>kubectl scale</code> <span>command; you have to create additional Deployments, Services, and PersistentVolumeClaims, which adds to the complexity.</span></p>
</div>
<div class="readable-text" data-hash="d09cfce73534bbaece96f6cc0938edba" data-text-hash="82755c386a3bd7b86d784f1cd6aa9410" id="38" refid="38">
<p><span>Even though this approach is feasible, it&#8217;s complex and it would be difficult to operate this system. Fortunately, Kubernetes provides a better way to do this with a single Service and a single StatefulSet object.</span></p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="39" refid="39">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="762603d289956069596fc717bbad86db" data-text-hash="6f9bd7da181c863b73d3f368d6c074a0" id="40" refid="40">
<p> <span>You don&#8217;t need the</span> <code>quiz</code> <span>Deployment and the</span> <code>quiz-data</code> <span>PersistentVolumeClaim anymore, so please delete them as follows:</span> <code>kubectl delete deploy/quiz pvc/quiz-data</code><span>.</span></p>
</div>
</div>
<div class="readable-text" data-hash="37515e22ceeb3cb7b54ec72bf2e20ebe" data-text-hash="4073a9873973772e5c7d82a7e53eff68" id="41" refid="41">
<h3 id="sigil_toc_id_269">15.1.2&#160;<span>Comparing StatefulSets with Deployments</span></h3>
</div>
<div class="readable-text" data-hash="b58497552d13911cc3301d87bf71ca33" data-text-hash="fab94f5162c363d7be4753b3ea83f7ec" id="42" refid="42">
<p><span>A StatefulSet is</span> <span>similar to</span> <span>a Deployment, but</span> <span>is specifically tailored to</span> <span>stateful workloads.</span> <span>However, t</span><span>here are significant difference</span><span>s</span> <span>in the behavior of these two objects.</span> <span>This difference is best explained</span> <span>with the <i>Pets vs. Cattle</i> analogy that you may have heard of. If not, let me explain.</span></p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="43" refid="43">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="6ecdf8b23080c789848dd77d4e707473" data-text-hash="fead26f1787f442cd2d2ea2ef0d84229" id="44" refid="44">
<p><span>&#8195;StatefulSets were originally called PetSets</span><span>. The name comes from this Pets vs. Cattle analogy.</span></p>
</div>
</div>
<div class="readable-text" data-hash="71d37d5003e98decf8dda7e4c0110d42" data-text-hash="0962367806e4204e72f3794486b83aed" id="45" refid="45">
<h4>The Pets vs. Cattle analogy</h4>
</div>
<div class="readable-text" data-hash="02379403978c134324214f574f262e02" data-text-hash="4661c3aee176250561e1ec80876d4539" id="46" refid="46">
<p><span>We used to treat our hardware infrastructure and workloads like pets. We gave each server a name and took care of each workload instance individually. However, it turns out that it&#8217;s much easier to manage hardware and software if you treat them like cattle and think of them as indistinguishable entities. That makes it easy to replace each unit without worrying that the replacement isn&#8217;t exactly the unit that was there before, much like a farmer treats cattle.</span></p>
</div>
<div class="browsable-container figure-container" data-hash="021998460e6fd2f18d6cb76ac7317339" data-text-hash="68ed7850986440ecb4e970a3b68b7b2d" id="47" refid="47">
<h5><span>Figure 15.3 Treating entities as pets vs. as cattle</span></h5>
<img alt="" data-processed="true" height="303" id="Picture_8" loading="lazy" src="EPUB/images/15_img_0003.png" width="912">
</div>
<div class="readable-text" data-hash="7ef6967257ce5c669a279a556c18bb31" data-text-hash="628b5cdb8a02dc6fdd6f06ff3b38392e" id="48" refid="48">
<p><span>Stateless workloads deployed via Deployments are like cattle. If a Pod is replaced, you probably won&#8217;t even notice. Stateful workloads, on the other hand, are like pets. If a pet gets lost, you can&#8217;t just replace it with a new one. Even if you give the replacement pet the same name, it won&#8217;t behave exactly like the original. However, in the hardware/software world, this is possible if you can give the replacement the same network identity and state as the replaced instance. And this is exactly what happens when you deploy an application with a StatefulSet.</span></p>
</div>
<div class="readable-text" data-hash="8c387d877787036c5fac5963b358b4fc" data-text-hash="eefd1801e790622cd93cff28b659052a" id="49" refid="49">
<h4>Deploying Pods with a StatefulSet</h4>
</div>
<div class="readable-text" data-hash="ce36676519cf9c8bcd01a39f8504c5f4" data-text-hash="b18c8a98b896be9791f1cef074fc6e23" id="50" refid="50">
<p><span>As with Deployments,</span> <span>in a StatefulSet</span> <span>you specify a Pod template, the desired number of replicas, and a label selector. However, you can also specify a PersistentVolumeClaim template.</span> <span>Each time</span> <span>the StatefulSet controller creates</span> <span>a new replica</span><span>, it creates not</span> <span>only</span> <span>a new Pod object, but also one or more PersistentVolumeClaim objects.</span></p>
</div>
<div class="readable-text" data-hash="64fd1c0c46c25d856110737a213a4830" data-text-hash="de281ad54bf3bb8e1d8d18883be9687e" id="51" refid="51">
<p><span>The Pods created from a StatefulSet aren't exact copies of each other</span><span>, as is the case with Deployments</span><span>,</span> <span>because</span> <span>each Pod</span> <span>points</span> <span>to a different set of PersistentVolumeClaims.</span> <span>In addition</span><span>, the names of the Pods aren't random. Instead, each Pod is given a unique ordinal number,</span> <span>as</span> <span>is each PersistentVolumeClaim. When a StatefulSet Pod is deleted and recreated, it</span><span>&#8217;s given the</span> <span>same name as the Pod it replaced. Also, a Pod with a</span> <span>particular</span> <span>ordinal</span> <span>number</span> <span>is always associated with PersistentVolumeClaims with the same number. This means that the state associated with a</span> <span>particular</span> <span>replica is always the same</span><span>, no matter how often the Pod is recreated</span><span>.</span></p>
</div>
<div class="browsable-container figure-container" data-hash="feaf60b818ac045fcf5d3a33d697b36b" data-text-hash="ea5b3f29b8ec4160726c5cf22be89a58" id="52" refid="52">
<h5><span>Figure 15.4 A StatefulSet, its Pods, and PersistentVolumeClaims</span></h5>
<img alt="" data-processed="true" height="311" id="Picture_11" loading="lazy" src="EPUB/images/15_img_0004.png" width="727">
</div>
<div class="readable-text" data-hash="8e8c6abdb773a2304b1ad9a943ec4d64" data-text-hash="6e564ffae6f7be4dfa97f181db476b42" id="53" refid="53">
<p><span>Another notable difference between Deployments and StatefulSets is that, by default,</span> <span>the Pods of a</span> <span>StatefulSet</span> <span>aren't created</span> <span>concurrently</span><span>. Instead, they</span><span>&#8217;</span><span>re created</span> <span>one at a time</span><span>, similar to a rolling update of a Deployment. When you create a StatefulSet, only the first</span> <span>Pod</span> <span>is created</span> <span>initially</span><span>. Then the StatefulSet controller waits</span> <span>until the Pod is</span> <span>ready before creating the next one.</span></p>
</div>
<div class="readable-text" data-hash="d8bce998e62e6cafe7a695054fe6a8d0" data-text-hash="0de43973909feadf278b2afd8f30fa92" id="54" refid="54">
<p><span>A StatefulSet can be scaled just like a Deployment. When you scale a StatefulSet up, new Pods and PersistentVolumeClaims are created from the</span><span>ir respective</span> <span>template</span><span>s</span><span>. When you scale down</span> <span>the StatefulSet</span><span>, the Pods are deleted, but the PersistentVolumeClaims are either</span> <span>retained</span> <span>or deleted, depending on the policy you configure in the StatefulSet.</span></p>
</div>
<div class="readable-text" data-hash="9e8824ffc3a001483ba3a907019b14a8" data-text-hash="a3481a69902b7e3ad0b3baf4e20b6683" id="55" refid="55">
<h3 id="sigil_toc_id_270">15.1.3&#160;Creating a StatefulSet</h3>
</div>
<div class="readable-text" data-hash="26c76fa7cdecf3f85719c6e4c9fdf3f3" data-text-hash="e41d426fc8a695c9800a458a6272947c" id="56" refid="56">
<p><span>In this section, you&#8217;ll replace the</span> <code>quiz</code> <span>Deployment with a StatefulSet. Each</span> <span>StatefulSet must have an associated headless Service that</span> <span>exposes the Pods individually, so the first thing you must do is</span> <span>create</span> <span>this</span> <span>Service.</span></p>
</div>
<div class="readable-text" data-hash="e4f6a46d5c6bb03601227d916c1683af" data-text-hash="68e044bdf86784b38cf2890b3fd4906a" id="57" refid="57">
<h4>Creating the governing Service</h4>
</div>
<div class="readable-text" data-hash="28e15da78b8e3232bb5e053c5b59e07c" data-text-hash="3b673ba32427ccd4d7add9657d04a427" id="58" refid="58">
<p><span>The headless Service associated with a StatefulSet gives the Pods their network identity. You may recall from chapter 11 that a headless Service</span> <span>doesn&#8217;t have a</span> <span>cluster IP</span> <span>address</span><span>, but you can</span> <span>still</span> <span>use it to communicate with the Pods that match its label selector. Instead of a single</span> <code>A</code> <span>or</span> <code>AAAA</code> <span>DNS record pointing to the</span> <span>Service&#8217;s</span> <span>IP, the DNS record for a headless Service points to the IPs of all the Pods that are part of the Service.</span></p>
</div>
<div class="readable-text" data-hash="e3c773cd02b62b082b799dc979195be2" data-text-hash="9b8dd1cde076eacb476d2acdef5d4ffe" id="59" refid="59">
<p><span>As you can see in the following figure, when using</span> <span>a headless Service with a StatefulSet</span><span>, an additional</span> <span>DNS record</span> <span>is created</span> <span>for</span> <span>each</span> <span>Pod</span> <span>so that the</span> <span>IP address</span> <span>of each Pod can be</span> <span>looked up</span> <span>by its</span> <span>name.</span> <span>This is how stateful Pods maintain their stable</span> <span>network identity.</span> <span>These DNS records don&#8217;t exist when the</span> <span>headless Service</span> <span>isn&#8217;t associated with</span> <span>a StatefulSet.</span></p>
</div>
<div class="browsable-container figure-container" data-hash="0e196c160012ff43402c2062cc53dfb5" data-text-hash="b1cd85c231be09e9e596d7e8352ced28" id="60" refid="60">
<h5><span>Figure 15.5 A headless Service used in combination with a StatefulSet</span></h5>
<img alt="" data-processed="true" height="337" id="Picture_13" loading="lazy" src="EPUB/images/15_img_0005.png" width="862">
</div>
<div class="readable-text" data-hash="2c9ee53cee83bc19813c08b830f177c8" data-text-hash="934df20a45afe828c7d4a606316408bd" id="61" refid="61">
<p><span>You already have a Service called</span> <code>quiz</code> <span>that you created in the previous chapters. You could</span> <span>change it into</span> <span>a headless Service, but let's create an additional Service instead</span><span>, because the new Service will expose all</span> <code>quiz</code> <span>Pods, whether they&#8217;re ready or not.</span></p>
</div>
<div class="readable-text" data-hash="094640f30e24faf81a5857e39089f06d" data-text-hash="2a1b40c1721657ae0f14f6c08f7cacb9" id="62" refid="62">
<p><span>T</span><span>his headless Service will allow you to resolve individual Pods,</span> <span>so let&#8217;s call it</span> <code>quiz-pods</code><span>. Create the service</span> <span>with</span> <span>the</span> <code>kubectl apply</code> <span>command. You can find the Service manifest in the</span> <code>svc.quiz-pods.yaml</code> <span>file,</span> <span>whose</span> <span>contents</span> <span>are</span> <span>shown in the following listing.</span></p>
</div>
<div class="browsable-container listing-container" data-hash="874a66c907440724f2d1b5a6eb8eeb1d" data-text-hash="f758d03dc02bd95be1b14950a79cd0a3" id="63" refid="63">
<h5>Listing 15.1 Headless Service for the quiz StatefulSet</h5>
<div class="code-area-container">
<pre class="code-area">apiVersion: v1
kind: Service
metadata:
  name: quiz-pods    #A
spec:
  clusterIP: None    #B
  publishNotReadyAddresses: true    #C
  selector:    #D
    app: quiz    #D
  ports:    #E
  - name: mongodb    #E
    port: 27017    #E</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIG5hbWUgb2YgdGhpcyBTZXJ2aWNlIGlzIHF1aXotcG9kcyBiZWNhdXNlIGl0IGFsbG93cyB5b3UgdG8gcmVzb2x2ZSBpbmRpdmlkdWFsIHF1aXogUG9kcy4KI0IgQnkgc2V0dGluZyB0aGlzIGZpZWxkLCB0aGUgU2VydmljZSBiZWNvbWVzIGhlYWRsZXNzLgojQyBCeSBzZXR0aW5nIHRoaXMgZmllbGQsIGEgRE5TIHJlY29yZCBpcyBjcmVhdGVkIGZvciBlYWNoIFBvZCwgd2hldGhlciB0aGUgUG9kIGlzIHJlYWR5IG9yIG5vdC4KI0QgVGhlIGxhYmVsIHNlbGVjdG9yIG1hdGNoZXMgYWxsIHF1aXogUG9kcy4KI0UgVGhpcyBTZXJ2aWNlIGFsc28gcHJvdmlkZXMgU1JWIGVudHJpZXMgZm9yIHRoZSBQb2RzLiBUaGUgTW9uZ29EQiBjbGllbnQgdXNlcyB0aGVtIHRvIGNvbm5lY3QgdG8gZWFjaCBpbmRpdmlkdWFsIE1vbmdvREIgc2VydmVyLg=="></div>
</div>
</div>
<div class="readable-text" data-hash="d0b1a1a30dc64f8166987f8679142151" data-text-hash="c84d12c27b72fa638fa9e7816179b8aa" id="64" refid="64">
<p><span>In the listing, the</span> <code>clusterIP</code> <span>field</span> <span>is set to</span> <code>None</code><span>, which makes this a headless Service. If you set</span> <code>publishNotReadyAddresses</code> <span>to</span> <code>true</code><span>, the DNS records for each Pod are created immediately when the Pod is created, rather than only when the Pod is ready. This way, the</span> <code>quiz-pods</code> <span>Service will include all</span> <code>quiz</code> <span>Pods, regardless of their readiness status.</span></p>
</div>
<div class="readable-text" data-hash="68eb7c8de9a00412b5947aa666f1181c" data-text-hash="f5018aec245d93f821939e2b750ca0db" id="65" refid="65">
<h4>Creating the StatefulSet</h4>
</div>
<div class="readable-text" data-hash="5ca3fc092b8a8f6e837f9337711ea7d3" data-text-hash="6113124f46fd8fdf8ff66e3bf5eea4b1" id="66" refid="66">
<p><span>After you create the headless Service, you can create the StatefulSet. You can find the object manifest in the</span> <code>sts.quiz.yaml</code> <span>file. The most important parts of the manifest are shown in the following listing.</span></p>
</div>
<div class="browsable-container listing-container" data-hash="edcd9fb838b0fd74c94b0d8906c7dd2e" data-text-hash="d852c44a1f408b4acc10b257b745c7e8" id="67" refid="67">
<h5>Listing 15.2 The object manifest for a StatefulSet</h5>
<div class="code-area-container">
<pre class="code-area">apiVersion: apps/v1    #A
kind: StatefulSet    #A
metadata:
  name: quiz
spec:
  serviceName: quiz-pods    #B
  podManagementPolicy: Parallel    #C
  replicas: 3    #D
  selector:    #E
    matchLabels:    #E
      app: quiz    #E
  template:    #F
    metadata:
      labels:    #E
        app: quiz    #E
        ver: "0.1"    #E
    spec:
      volumes:    #G
      - name: db-data    #G
        persistentVolumeClaim:    #G
          claimName: db-data    #G
      containers:
      - name: quiz-api
        ...
      - name: mongo
        image: mongo:5
        command:    #H
        - mongod    #H
        - --bind_ip    #H
        - 0.0.0.0    #H
        - --replSet    #H
        - quiz    #H
        volumeMounts:    #I
        - name: db-data    #I
          mountPath: /data/db    #I
  volumeClaimTemplates:    #J
  - metadata:    #J
      name: db-data    #J
      labels:    #J
        app: quiz    #J
    spec:    #J
      resources:    #J
        requests:    #J
          storage: 1Gi    #J
      accessModes:    #J
      - ReadWriteOnce    #J</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgU3RhdGVmdWxTZXRzIGFyZSBpbiB0aGUgYXBwcy92MSBBUEkgZ3JvdXAgYW5kIHZlcnNpb24uCiNCIFRoZSBuYW1lIG9mIHRoZSBoZWFkbGVzcyBTZXJ2aWNlIHRoYXQgZ292ZXJucyB0aGlzIFN0YXRlZnVsU2V0LgojQyBUaGlzIHRlbGxzIHRoZSBTdGF0ZWZ1bFNldCBjb250cm9sbGVyIHRvIGNyZWF0ZSBhbGwgUG9kcyBhdCB0aGUgc2FtZSB0aW1lLgojRCBUaGUgU3RhdGVmdWxTZXQgaXMgY29uZmlndXJlZCB0byBjcmVhdGUgdGhyZWUgcmVwbGljYXMuCiNFIFRoZSBsYWJlbCBzZWxlY3RvciBkZXRlcm1pbmVzIHdoaWNoIFBvZHMgYmVsb25nIHRvIHRoaXMgU3RhdGVmdWxTZXQuIEl0IG11c3QgbWF0Y2ggdGhlIGxhYmVscyBpbiB0aGUgUG9kIHRlbXBsYXRlLgojRiBUaGUgUG9kcyBmb3IgdGhpcyBTdGF0ZWZ1bFNldCBhcmUgY3JlYXRlZCB1c2luZyB0aGlzIHRlbXBsYXRlLgojRyBBIHNpbmdsZSB2b2x1bWUgaXMgZGVmaW5lZCBpbiB0aGUgUG9kLiBUaGUgdm9sdW1lIHJlZmVycyB0byBhIFBlcnNpc3RlbnRWb2x1bWVDbGFpbSB3aXRoIHRoZSBzcGVjaWZpZWQgbmFtZS4KI0ggTW9uZ29EQiBtdXN0IGJlIHN0YXJ0ZWQgd2l0aCB0aGVzZSBvcHRpb25zIHRvIGVuYWJsZSByZXBsaWNhdGlvbi4KI0kgVGhlIFBlcnNpc3RlbnRWb2x1bWVDbGFpbSB2b2x1bWUgaXMgbW91bnRlZCBoZXJlLgojSiBUaGUgdGVtcGxhdGUgdXNlZCB0byBjcmVhdGUgdGhlIFBlcnNpc3RlbnRWb2x1bWVDbGFpbXMu"></div>
</div>
</div>
<div class="readable-text" data-hash="85ca7ee447c8ac14a41046fde67d6450" data-text-hash="b7009125fba5c09fc7cb4597df301d11" id="68" refid="68">
<p><span>The manifest defines an object of kind</span> <code>StatefulSet</code> <span>from the API group</span> <code>apps</code><span>, version</span> <code>v1</code><span>. The name of the StatefulSet is</span> <code>quiz</code><span>. In the StatefulSet</span> <code>spec</code><span>, you&#8217;ll find some fields you know from Deployments and ReplicaSets, such as</span> <code>replicas</code><span>,</span> <code>selector</code><span>, and</span> <code>template</code><span>, explained in the previous chapter, but this manifest contains other fields that are specific to StatefulSets. In the</span> <code>serviceName</code> <span>field, for example, you specify the name of the headless Service that governs this StatefulSet.</span></p>
</div>
<div class="readable-text" data-hash="702828c6d0135f189b92fa7fb9b6cb80" data-text-hash="2774da5125f02f685c3f40548933c39c" id="69" refid="69">
<p><span>By setting</span> <code>podManagementPolicy</code> <span>to</span> <code>Parallel</code><span>, the StatefulSet controller creates all Pods simultaneously. Since some distributed applications can&#8217;t handle multiple instances being launched at the same time, the default behavior of the controller is to create one Pod at a time. However, in this example, the</span> <code>Parallel</code> <span>option makes the initial scale-up less involved.</span></p>
</div>
<div class="readable-text" data-hash="4ea04d763bb8344126df8ac2f31cd188" data-text-hash="7264378786bc18c05df6e44eb4b7d095" id="70" refid="70">
<p><span>In the</span> <code>volumeClaimTemplates</code> <span>field, you specify the templates for the PersistentVolumeClaims that the controller creates for each replica. Unlike the Pod templates, where you omit the</span> <code>name</code> <span>field, you must specify the name in the PersistentVolumeClaim template. This name must match the name in the</span> <code>volumes</code> <span>section of the Pod template.</span></p>
</div>
<div class="readable-text" data-hash="7302a62260e61f9f298b3037ca205e1a" data-text-hash="295be2ac608bce86418ed1344f35fcf1" id="71" refid="71">
<p><span>Create the StatefulSet by applying the manifest file as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="b923c6147a4578508428d14c06ed0ce0" data-text-hash="c49ec2b7594a024992a9c07291fefbc8" id="72" refid="72">
<div class="code-area-container">
<pre class="code-area">$ kubectl apply -f sts.quiz.yaml
statefulset.apps/quiz created</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="b9e91f00ca6d582d94f20d0b939f20f7" data-text-hash="859941986352fdba536bd45da96ca130" id="73" refid="73">
<h3 id="sigil_toc_id_271">15.1.4&#160;<span>Inspecting the StatefulSet, Pods, and PersistentVolumeClaims</span></h3>
</div>
<div class="readable-text" data-hash="e4af95db36397f64bf8954f1d2ccf431" data-text-hash="4d1bdbcce1afe258b9514204bf80c595" id="74" refid="74">
<p><span>After you create the StatefulSet, you can use the</span> <code>kubectl rollout status</code> <span>command to see its status like so:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="2634b6fbf112f77a40425eb3865d468f" data-text-hash="697b064fc6db06678474a64053e7b18f" id="75" refid="75">
<div class="code-area-container">
<pre class="code-area">$ kubectl rollout status sts quiz
Waiting for 3 pods to be ready...</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="76" refid="76">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="b1ef79d803fac02f41c0a3f003035dfa" data-text-hash="cfbcdff745f846210c4fcbe5b1ce9783" id="77" refid="77">
<p> <span>The shorthand for StatefulSets is sts.</span></p>
</div>
</div>
<div class="readable-text" data-hash="d116e121edc4019d534ac37c502ca180" data-text-hash="5c535e0b5dbb02315fb5c9a26fe73936" id="78" refid="78">
<p><span>After</span> <code>kubectl</code> <span>prints this message, it doesn&#8217;t continue. Interrupt its execution by pressing Control-C and check the StatefulSet status with the</span> <code>kubectl get</code> <span>command to investigate why.</span></p>
</div>
<div class="browsable-container listing-container" data-hash="39e7eb49cf7e9fda4561aff546f5a523" data-text-hash="b4656ea69979419d9521c93d1820f054" id="79" refid="79">
<div class="code-area-container">
<pre class="code-area">$ kubectl get sts
NAME   READY   AGE
quiz   0/3     22s</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="80" refid="80">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="defe99fb884c4763b00da2b754b0831d" data-text-hash="4847723f2e163173941dfdd63188fc6d" id="81" refid="81">
<p> <span>As with Deployments and ReplicaSets, you can use the</span> <code>-o wide</code> <span>option to display the names of the containers and images used in the StatefulSet.</span></p>
</div>
</div>
<div class="readable-text" data-hash="8321877acf3109cfc0549ce6784c7894" data-text-hash="d5eefc439710b5225440543b30354afe" id="82" refid="82">
<p><span>The value in the</span> <code>READY</code> <span>column shows that none of the replicas are ready. List the Pods with</span> <code>kubectl get pods</code> <span>as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="66967f7f0294af7fd7652caffed543d8" data-text-hash="63f696509c738f9dbe4109f5b39d44c2" id="83" refid="83">
<div class="code-area-container">
<pre class="code-area">$ kubectl get pods -l app=quiz
NAME     READY   STATUS    RESTARTS   AGE
quiz-0   1/2     Running   0          56s
quiz-1   1/2     Running   0          56s
quiz-2   1/2     Running   0          56s</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="84" refid="84">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="b295f7d8697869bed092efaa829e9e82" data-text-hash="15486f8c82fb224eeb1e150bf1c19764" id="85" refid="85">
<p> <span>Did you notice the Pod names? They don&#8217;t contain a template hash or random characters. the name of each Pod is composed of the StatefulSet name and an ordinal number, as explained in the introduction.</span></p>
</div>
</div>
<div class="readable-text" data-hash="63fce5b53e18f815e1eae9d1de4971bc" data-text-hash="97abda0cacd3ad65da78814f793863ca" id="86" refid="86">
<p><span>You&#8217;ll notice that only one of the two containers in each Pod is ready. If you examine a Pod with the</span> <code>kubectl describe</code> <span>command, you&#8217;ll see that the</span> <code>mongo</code> <span>container is ready, but the</span> <code>quiz-api</code> <span>container isn&#8217;t, because its readiness check fails. This is because the endpoint called by the readiness probe (</span><code>/healthz/ready</code><span>) checks whether the</span> <code>quiz-api</code> <span>process can query the MongoDB server. The failed readiness probe indicates that this isn&#8217;t possible. If you check the logs of the</span> <code>quiz-api</code> <span>container as follows, you&#8217;ll see why:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="d23e2999f938eacfc95545f25d475c61" data-text-hash="a868f291379c190b685aee96fb388d9f" id="87" refid="87">
<div class="code-area-container">
<pre class="code-area">$ kubectl logs quiz-0 -c quiz-api
... INTERNAL ERROR: connected to mongo, but couldn't execute the ping command: server selection error: server selection timeout, current topology: { Type: Unknown, Servers: [{ Addr: 127.0.0.1:27017, Type: RSGhost, State: Connected, Average RTT: 898693 }, ] }</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="4b372e35f922db3e9726867ae67d1c2f" data-text-hash="ce377f6ab80dbfc51413c90af3a76aac" id="88" refid="88">
<p><span>As indicated in the error message, the connection to MongoDB has been established, but the server doesn&#8217;t allow the ping command to be executed. The reason is that the server was started with the</span> <code>--replSet</code> <span>option configuring it to use replication, but the MongoDB replica set hasn&#8217;t been initiated yet. To do this, run the following command:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="f2e1fddb136e02eee08679e6757c03bf" data-text-hash="19286612db2f46eb56cabaf0bdd7f5b0" id="89" refid="89">
<div class="code-area-container">
<pre class="code-area">$ kubectl exec -it quiz-0 -c mongo -- mongosh --quiet --eval 'rs.initiate({
  _id: "quiz",
  members: [
    {_id: 0, host: "quiz-0.quiz-pods.kiada.svc.cluster.local:27017"},
    {_id: 1, host: "quiz-1.quiz-pods.kiada.svc.cluster.local:27017"},
    {_id: 2, host: "quiz-2.quiz-pods.kiada.svc.cluster.local:27017"}]})'</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="90" refid="90">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="7e844d76fcb9af54b8b80ad4a96d85c7" data-text-hash="2c5b5cfbcf27a8abf25a92120686d8f7" id="91" refid="91">
<p> <span>Instead of typing this long command, you can also run the</span> <code>initiate-mongo-replicaset.sh</code> <span>shell script, which you can find in this chapter&#8217;s code directory.</span></p>
</div>
</div>
<div class="readable-text" data-hash="69a70c1bd6b35419de788147775c1163" data-text-hash="6a445bf73803508980d40e6db4d24f1c" id="92" refid="92">
<p><span>If the MongoDB shell gives the following error message, you probably forgot to create the</span> <code>quiz-pods</code> <span>Service beforehand:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="3e09648c918ba49286af3f9d934d9174" data-text-hash="ff73b3bb674dccbd9ef70c1c1f8596d4" id="93" refid="93">
<div class="code-area-container">
<pre class="code-area">MongoServerError: replSetInitiate quorum check failed because not all proposed set members responded affirmatively: ... caused by :: Could not find address for quiz-2.quiz-pods.kiada.svc.cluster.local:27017: SocketException: Host not found</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="a61e0aa32da7d12b51236fc721599ee1" data-text-hash="26be7200390b184727a8d9147793469e" id="94" refid="94">
<p><span>If the initiation of the replica set is successful, the command prints the following message:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="be4cd875ab7daa10063207a3b4aca255" data-text-hash="b770d2765648b1ecf47dac1a44d345cc" id="95" refid="95">
<div class="code-area-container">
<pre class="code-area">{ ok: 1 }</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="8286587f3a21bfd923c51cf8a3b96bc9" data-text-hash="004e48d8a1873f1f5ad7849ee1d517d8" id="96" refid="96">
<p><span>All three</span> <code>quiz</code> <span>Pods should be ready shortly after the replica set is initiated. If you run the</span> <code>kubectl rollout status</code> <span>command again, you&#8217;ll see the following output:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="1548fa04a480e63a2ed7c7fb433e5861" data-text-hash="3b72209cb114a37e00d7333070053f79" id="97" refid="97">
<div class="code-area-container">
<pre class="code-area">$ kubectl rollout status sts quiz
partitioned roll out complete: 3 new pods have been updated...</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="bd0160e86f56282ed3e93d75d5cb6eee" data-text-hash="6b9e3d2c22269203bda75a185b10158d" id="98" refid="98">
<h4>Inspecting the StatefulSet with kubectl describe</h4>
</div>
<div class="readable-text" data-hash="27e3b525e4a2c3f7a1773a81c0843a9b" data-text-hash="b076604e151e862b8d137c3df0b67014" id="99" refid="99">
<p><span>As you know, you can examine an object in detail with the</span> <code>kubectl describe</code> <span>command. Here you can see what it displays for the</span> <code>quiz</code> <span>StatefulSet:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="455cb78a374a60b73edeaee07271deb3" data-text-hash="850161acbf5c9255cdeac35ae88c6652" id="100" refid="100">
<div class="code-area-container">
<pre class="code-area">$ kubectl describe sts quiz
Name:               quiz
Namespace:          kiada
CreationTimestamp:  Sat, 12 Mar 2022 18:05:43 +0100
Selector:           app=quiz    #A
Labels:             app=quiz
Annotations:        &lt;none&gt;
Replicas:           3 desired | 3 total    #B
Update Strategy:    RollingUpdate
  Partition:        0
Pods Status:        3 Running / 0 Waiting / 0 Succeeded / 0 Failed    #C
Pod Template:    #D
  ...    #D
Volume Claims:    #E
  Name:          db-data    #E
  StorageClass:    #E
  Labels:        app=quiz    #E
  Annotations:   &lt;none&gt;    #E
  Capacity:      1Gi    #E
  Access Modes:  [ReadWriteOnce]    #E
Events:    #F
  Type    Reason            Age   From                    Message    #F
  ----    ------            ----  ----                    -------    #F
  Normal  SuccessfulCreate  10m   statefulset-controller  create Claim db-data-quiz-0    #F
                                                          Pod quiz-0 in StatefulSet    #F 
                                                          quiz success    #F
  Normal  SuccessfulCreate  10m   statefulset-controller  create Pod quiz-0 in    #F
                                                          StatefulSet quiz successful #F
  ...    #F</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIGxhYmVsIHNlbGVjdG9yIHRoYXQgZGV0ZXJtaW5lcyB3aGljaCBQb2RzIGJlbG9uZyB0byB0aGlzIFN0YXRlZnVsU2V0LgojQiBUaGUgZGVzaXJlZCBhbmQgdGhlIGFjdHVhbCBudW1iZXIgb2YgcmVwbGljYXMuCiNDIFRoZSBzdGF0dXMgb2YgdGhlIFN0YXRlZnVsU2V04oCZcyBQb2RzLgojRCBUaGUgdGVtcGxhdGUgdXNlZCB0byBjcmVhdGUgdGhlIFBvZHMgb2YgdGhpcyBTdGF0ZWZ1bFNldC4KI0UgVGhlIHRlbXBsYXRlIHVzZWQgdG8gY3JlYXRlIHRoZSBQZXJzaXN0ZW50Vm9sdW1lQ2xhaW1zIG9mIHRoaXMgU3RhdGVmdWxTZXQuCiNGIFRoZSBldmVudHMgc2hvdyB3aGF0IHRoZSBTdGF0ZWZ1bFNldCBjb250cm9sbGVyIGRpZC4gVGhlIGxpc3QgbWF5IGluY2x1ZGUgV2FybmluZyBldmVudHMgaWYgc29tZXRoaW5nIGdvZXMgd3Jvbmcu"></div>
</div>
</div>
<div class="readable-text" data-hash="f197bcdae8433072a5127ed5a38e9e2d" data-text-hash="75f7d241313fc71536eda5c84dfec059" id="101" refid="101">
<p><span>As you can see, the output is very similar to that of a ReplicaSet and Deployment. The most noticeable difference is the presence of the PersistentVolumeClaim template, which you won&#8217;t find in the other two object types. The events at the bottom of the output show you exactly what the StatefulSet controller did. Whenever it creates a Pod or a PersistentVolumeClaim, it also creates an Event that tells you what it did.</span></p>
</div>
<div class="readable-text" data-hash="0d2a5a0bca0d1ec9d706de779663b96c" data-text-hash="9f5805ba505f47afd74a2ebafbe35b94" id="102" refid="102">
<h4>Inspecting the Pods</h4>
</div>
<div class="readable-text" data-hash="6ac030b3480e0ef47c24da0e179d9467" data-text-hash="be279e086544307250bd6a6a71612194" id="103" refid="103">
<p><span>Let&#8217;s</span> <span>take a closer look at the manifest of the first Pod to see how it compares to Pods created by a ReplicaSet. Use the</span> <code>kubectl get</code> <span>command to print the Pod manifest like so:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="e77775185fe845f518b492339b7988e1" data-text-hash="db633bcb0c782bcc1db5214303e4c798" id="104" refid="104">
<div class="code-area-container">
<pre class="code-area">$ kubectl get pod quiz-0 -o yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: quiz    #A
    controller-revision-hash: quiz-7576f64fbc    #A
    statefulset.kubernetes.io/pod-name: quiz-0    #A
    ver: "0.1"    #A
  name: quiz-0
  namespace: kiada
  ownerReferences:    #B
  - apiVersion: apps/v1    #B
    blockOwnerDeletion: true    #B
    controller: true    #B
    kind: StatefulSet    #B
    name: quiz    #B
spec:
  containers:    #C
  ...    #C
  volumes:
  - name: db-data
    persistentVolumeClaim:    #D
      claimName: db-data-quiz-0    #D
status:
  ...</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIGxhYmVscyBpbmNsdWRlIHRoZSBsYWJlbHMgc2V0IGluIHRoZSBTdGF0ZWZ1bFNldOKAmXMgUG9kIHRlbXBsYXRlIGFuZCB0d28gYWRkaXRpb25hbCBsYWJlbHMgYWRkZWQgYnkgdGhlIFN0YXRlZnVsU2V0IGNvbnRyb2xsZXIuCiNCIFRoaXMgUG9kIG9iamVjdCBpcyBvd25lZCBieSB0aGUgU3RhdGVmdWxTZXQuCiNDIFRoZSBjb250YWluZXJzIG1hdGNoIHRob3NlIGluIHRoZSBTdGF0ZWZ1bFNldOKAmXMgUG9kIHRlbXBsYXRlLgojRCBTaW5jZSBlYWNoIFBvZCBpbnN0YW5jZSBnZXRzIGl0cyBvd24gUGVyc2lzdGVudFZvbHVtZUNsYWltLCB0aGUgY2xhaW0gbmFtZSBzcGVjaWZpZWQgaW4gdGhlIFN0YXRlZnVsU2V04oCZcyBQb2QgdGVtcGxhdGUgd2FzIHJlcGxhY2VkIHdpdGggdGhlIG5hbWUgb2YgdGhlIGNsYWltIGFzc29jaWF0ZWQgd2l0aCB0aGlzIHBhcnRpY3VsYXIgUG9kIGluc3RhbmNlLg=="></div>
</div>
</div>
<div class="readable-text" data-hash="bd53d00fd5ec0d8f71c92f233f749518" data-text-hash="2ae5412103ddb1d380f3a42498c9d296" id="105" refid="105">
<p><span>The only label you defined in the Pod template in the StatefulSet manifest was</span> <code>app</code><span>, but the StatefulSet controller added two additional labels to the Pod:</span></p>
</div>
<ul>
<li class="readable-text" data-hash="22fd0d86f2a6c6f38c031da8dbd62b5c" data-text-hash="a2d58aa98c6221d69d01029543694830" id="106" refid="106"><span>The label</span> <code class="codechar">controller-revision-hash</code> <span>serves the same purpose as the label</span> <code class="codechar">pod-template-hash</code> <span>on the Pods of a ReplicaSet. It allows the controller to determine to which revision of the StatefulSet a particular Pod belongs.</span></li>
<li class="readable-text" data-hash="70f397a5ad2422fc8515cec9fbe2267e" data-text-hash="40bc5863bb71c52cc8f8a676610a3d44" id="107" refid="107"><span>The label</span> <code>statefulset.kubernetes.io/pod-name</code> <span>specifies the Pod name and allows you to create a Service for a specific Pod instance by using this label in the Service&#8217;s label selector.</span></li>
</ul>
<div class="readable-text" data-hash="d6f3c2ba29092953e21b4bb384c9e276" data-text-hash="11ea1fcc43b4a7d1edbaabe628ea662d" id="108" refid="108">
<p><span>Since this Pod object is managed by the StatefulSet, the</span> <code>ownerReferences</code> <span>field indicates this fact. Unlike Deployments, where Pods are owned by ReplicaSets, which in turn are owned by the Deployment, StatefulSets own the Pods directly. The StatefulSet takes care of both replication and updating of the Pods.</span></p>
</div>
<div class="readable-text" data-hash="7e458beae08a01669116a3ae434f3b90" data-text-hash="0ee5587284ca63fc0a7bdae1c8506538" id="109" refid="109">
<p><span>The Pod&#8217;s</span> <code>containers</code> <span>match the containers defined in the StatefulSet&#8217;s Pod template, but that&#8217;s not the case for the Pod&#8217;s</span> <code>volumes</code><span>. In the template you specified the</span> <code>claimName</code> <span>as</span> <code>db-data</code><span>, but here in the Pod it&#8217;s been changed to</span> <code>db-data-quiz-0</code><span>. This is because each Pod instance gets its own PersistentVolumeClaim. The name of the claim is made up of the</span> <code>claimName</code> <span>and the name of the Pod.</span></p>
</div>
<div class="readable-text" data-hash="23ed4618e6d0f09bf899f8cfab03abc5" data-text-hash="5b2b27e1e1b69d796b99074f1235ade6" id="110" refid="110">
<h4>Inspecting the PersistentVolumeClaims</h4>
</div>
<div class="readable-text" data-hash="00b214f02c191adf5406dd85c295ba89" data-text-hash="552092fc7613eec518f8c63df7eeb285" id="111" refid="111">
<p><span>Along with the Pods, the StatefulSet controller creates a PersistentVolumeClaim for each Pod. List them as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="60cd6fcbcdca7b0f7e63747060570db9" data-text-hash="d15f8fd975c6ec6517c11992a7231fb8" id="112" refid="112">
<div class="code-area-container">
<pre class="code-area">$ kubectl get pvc -l app=quiz
NAME             STATUS   VOLUME           CAPACITY   ACCESS MODES   STORAGECLASS   AGE
db-data-quiz-0   Bound    pvc...1bf8ccaf   1Gi        RWO            standard       10m
db-data-quiz-1   Bound    pvc...c8f860c2   1Gi        RWO            standard       10m
db-data-quiz-2   Bound    pvc...2cc494d6   1Gi        RWO            standard       10m</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="b497b7f4fd1aa82413da01340e01dc8c" data-text-hash="357be3349e272560142294d493dc2521" id="113" refid="113">
<p><span>You can check the manifest of these PersistentVolumeClaims to make sure they match the template specified in the StatefulSet. Each claim is bound to a PersistentVolume that&#8217;s been dynamically provisioned for it. These volumes don&#8217;t yet contain any data, so the Quiz service doesn&#8217;t currently return anything. You&#8217;ll import the data next.</span></p>
</div>
<div class="readable-text" data-hash="96ed20a96cf1c67feeae981d75b7e51c" data-text-hash="5f10f141ae91d4c23675b50f67245df5" id="114" refid="114">
<h3 id="sigil_toc_id_272">15.1.5&#160;<span>Understanding the role of the</span> <span>headless Service</span></h3>
</div>
<div class="readable-text" data-hash="bed3b5bff49e00abfd774bf5eb02de96" data-text-hash="599d842c307b430333df1b71c2627b8e" id="115" refid="115">
<p><span>An important requirement of distributed applications is peer discovery&#8212;the ability for each cluster member to find the other members. If an application deployed via a StatefulSet needs to find all other Pods in the StatefulSet, it could do so by retrieving the list of Pods from the Kubernetes API. However, since we want applications to remain Kubernetes-agnostic, it&#8217;s better for the application to use DNS and not talk to Kubernetes directly.</span></p>
</div>
<div class="readable-text" data-hash="209bf11ed6cce8e0350af4657f8e8a31" data-text-hash="df4b21e682227833e64cfdafe9439ee9" id="116" refid="116">
<p><span>For example, a client connecting to a MongoDB replica set must know the addresses of all the replicas, so it can find the primary replica when it needs to write data. You must specify the addresses in the connection string you pass to the MongoDB client. For your three</span> <code>quiz</code> <span>Pods, the following connection URI can be used:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="32628ffbaa8110843faaa4a8064ac0f7" data-text-hash="bab1b8aa5b5a281df2e4aa75975245bb" id="117" refid="117">
<div class="code-area-container">
<pre class="code-area">mongodb://quiz-0.quiz-pods.kiada.svc.cluster.local:27017,quiz-1.quiz-pods.kiada.svc.
cluster.local:27017,quiz-2.quiz-pods.kiada.svc.cluster.local:27017</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="5d023be091f9f5a888fdd433547ea1aa" data-text-hash="c6edc957cd1fcff301f60c71193cdcc8" id="118" refid="118">
<p><span>If the StatefulSet was configured with additional replicas, you&#8217;d need to add their addresses to the connection string, too. Fortunately, there&#8217;s a better way.</span></p>
</div>
<div class="readable-text" data-hash="ac2400590f793bea940cd4db761e727f" data-text-hash="2d72c69f84d87215f4e64b95a15f6e72" id="119" refid="119">
<h4>Exposing stateful Pods through DNS individually</h4>
</div>
<div class="readable-text" data-hash="584c8bd53bde6399a2a4a3dd8e558c0b" data-text-hash="a3a0fd35b4108fa047e641129257f326" id="120" refid="120">
<p><span>In chapter 11 you learned that a Service object not only exposes a set of Pods at a stable IP address but also makes the cluster DNS resolve the Service name to this IP address. With a headless Service, on the other hand, the name resolves to the IPs of the Pods that belong to the Service. However, when a headless Service is associated with a StatefulSet, each Pod also gets its own</span> <code>A</code> <span>or</span> <code>AAAA</code> <span>record that resolves directly to the individual Pod&#8217;s IP. For example, because you combined the</span> <code>quiz</code> <span>StatefulSet with the</span> <code>quiz-pods</code> <span>headless Service, the IP of the</span> <code>quiz-0</code> <span>Pod is resolvable at the following address:</span></p>
</div>
<div class="browsable-container figure-container" data-hash="0e15d66e32c93ddd2f94bb566a0769f7" data-text-hash="d41d8cd98f00b204e9800998ecf8427e" id="121" refid="121">
<img alt="" data-processed="true" height="76" id="Picture_1" loading="lazy" src="EPUB/images/15_img_0006.png" width="370">
</div>
<div class="readable-text" data-hash="5d341153b0363f75c3406e7e4d152eff" data-text-hash="2e936d0c23a861e09c2797d5f1ac115f" id="122" refid="122">
<p><span>All the other replicas created by the StatefulSet are resolvable in the same way.</span></p>
</div>
<div class="readable-text" data-hash="5ba3b854bc8ede4e92f206125d82effc" data-text-hash="05c8a1318bec7b8d53222982bebcc7ff" id="123" refid="123">
<h4>Exposing stateful Pods via SRV records</h4>
</div>
<div class="readable-text" data-hash="74565ab564000990cd2943397c406431" data-text-hash="6ba11c1ee922bb8dd3b09ab108a5d77b" id="124" refid="124">
<p><span>In addition to the</span> <code>A</code> <span>and</span> <code>AAAA</code> <span>records, each stateful Pod also gets</span> <code>SRV</code> <span>records. These can be used by the MongoDB client to look up the addresses and port numbers used by each Pod so you don&#8217;t have to specify them manually. However, you must ensure that the</span> <code>SRV</code> <span>record has the correct name. MongoDB expects the</span> <code>SRV</code> <span>record to start with</span> <code>_mongodb</code><span>. To ensure that&#8217;s the case, you must set the port name in the Service definition to</span> <code>mongodb</code> <span>like you did in listing 15.1. This ensures that the</span> <code>SRV</code> <span>record is as follows:</span></p>
</div>
<div class="browsable-container figure-container" data-hash="31978ad0b93f29bba8e61573a9d94f2f" data-text-hash="d41d8cd98f00b204e9800998ecf8427e" id="125" refid="125">
<img alt="" data-processed="true" height="77" id="Picture_3" loading="lazy" src="EPUB/images/15_img_0007.png" width="411">
</div>
<div class="readable-text" data-hash="42dacb180de86ef2059ed37f2dd6e87f" data-text-hash="d328c17ab8ef6313ed330f0ff489096d" id="126" refid="126">
<p><span>Using</span> <code>SRV</code> <span>records allows the MongoDB connection string to be much simpler. Regardless of the number of replicas in the set, the connection string is always as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="41c6e5006d8458ecb87e45bdb677a5e3" data-text-hash="f786e71bf3c070f3aa016994b747aeae" id="127" refid="127">
<div class="code-area-container">
<pre class="code-area">mongodb+srv://quiz-pods.kiada.svc.cluster.local</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="5981feef742eff153dc74958c9ea09a5" data-text-hash="56a6b91ed8ac3fa4e80ad821d276d020" id="128" refid="128">
<p><span>Instead of specifying the addresses individually, the</span> <code>mongodb+srv</code> <span>scheme tells the client to find the addresses by performing an</span> <code>SRV</code> <span>lookup for the domain name</span> <code>_mongodb._tcp.quiz-pods.kiada.svc.cluster.local</code><span>. You&#8217;ll use this connection string to import the quiz data into MongoDB, as explained next.</span></p>
</div>
<div class="readable-text" data-hash="13838c9b35c8ac0e7a6712ccdc18c6d0" data-text-hash="304d46a829e8f95e2aa1fcad5fbc95a9" id="129" refid="129">
<h4>Importing quiz data into MongoDB</h4>
</div>
<div class="readable-text" data-hash="5c6397946dc2e49558f535482688e9fe" data-text-hash="8ac51a9988118329e607b58a66fb34dc" id="130" refid="130">
<p><span>In the previous chapters, an init container was used to import the quiz data into the MongoDB store.</span> <span>The</span> <span>init container</span> <span>approach is no longer valid since</span> <span>the data is now replicated</span><span>, so if</span> <span>you</span> <span>were to use it</span><span>, the data would be imported multiple times</span><span>. Instead, let&#8217;s move the import to a dedicated</span> <span>Pod.</span></p>
</div>
<div class="readable-text" data-hash="e7f1d859eca8d2fd31a24546c8e11029" data-text-hash="0d9047b94d057c1683b98a4cb58999ad" id="131" refid="131">
<p><span>You can find the Pod manifest in the file</span> <code>pod.quiz-data-importer.yaml</code><span>.</span> <span>The file also contains a ConfigMap that contains the data to be imported.</span> <span>The following listing shows the contents of the manifest file.</span></p>
</div>
<div class="browsable-container listing-container" data-hash="b35ca79dc7318ca2d364794eb1339a07" data-text-hash="16b629256dc3236405bd7c89d14b353b" id="132" refid="132">
<h5>Listing 15.3 The manifest of the quiz-data-importer Pod</h5>
<div class="code-area-container">
<pre class="code-area">apiVersion: v1
kind: Pod
metadata:
  name: quiz-data-importer
spec:
  restartPolicy: OnFailure    #A
  volumes:
  - name: quiz-questions
    configMap:
      name: quiz-questions
  containers:
  - name: mongoimport
    image: mongo:5
    command:
    - mongoimport
    - mongodb+srv://quiz-pods.kiada.svc.cluster.local/kiada?tls=false    #B
    - --collection
    - questions
    - --file
    - /questions.json
    - --drop
    volumeMounts:
    - name: quiz-questions
      mountPath: /questions.json
      subPath: questions.json
      readOnly: true
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: quiz-questions
  labels:
    app: quiz
data:
  questions.json: ...</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhpcyBQb2QncyBjb250YWluZXIgb25seSBuZWVkcyB0byBydW4gdG8gY29tcGxldGlvbiBvbmNlLgojQiBUaGUgY2xpZW50IHVzZXMgdGhlIFNSViBsb29rdXAgbWV0aG9kIHRvIGZpbmQgdGhlIE1vbmdvREIgcmVwbGljYXMu"></div>
</div>
</div>
<div class="readable-text" data-hash="00c3d9b88ba5e9e99d825fc719c08758" data-text-hash="4bba6db35aa928aef52d5564d2e9c896" id="133" refid="133">
<p><span>The</span> <code>quiz-questions</code> <span>ConfigMap</span> <span>is mounted into the</span> <code>quiz-data-importer</code> <span>Pod</span> <span>through</span> <span>a</span> <code>configMap</code> <span>volume. When the Pod's container</span> <span>starts</span><span>, it runs the</span> <code>mongoimport</code> <span>command</span><span>, which</span> <span>connects to the primary MongoDB replica and imports the data</span> <span>from the file in the volume</span><span>. The data is then replicated to the secondary replicas.</span></p>
</div>
<div class="readable-text" data-hash="38141e71e0f45082420316f273c3b702" data-text-hash="4bfac06d76402c92c9a19c5183d8e64a" id="134" refid="134">
<p><span>Since the</span> <code>mongoimport</code> <span>container only needs to run once, the Pod's</span> <code>restartPolicy</code> <span>is set to</span> <code>OnFailure</code><span>. If the import fails, the container</span> <span>will be restarted</span> <span>as many times as necessary until the import</span> <span>succeeds</span><span>. Deploy</span> <span>the</span> <span>Pod using the</span> <code>kubectl apply</code> <span>command and verify that it complete</span><span>d successfully</span><span>. You can do this by checking the status</span> <span>of the Pod</span> <span>as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="cf547329112ea15f0a9da93f341edad4" data-text-hash="63e409819ca603dc4e2831ad47ea7c6c" id="135" refid="135">
<div class="code-area-container">
<pre class="code-area">$ kubectl get pod quiz-data-importer
NAME                 READY   STATUS      RESTARTS   AGE
quiz-data-importer   0/1     Completed   0          50s</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="5aa40a8eb066e3eb4566bd2e303bc22b" data-text-hash="380896139549f3e030f67cc3370d6c5c" id="136" refid="136">
<p><span>If the</span> <code>STATUS</code> <span>column</span> <span>displays</span> <span>the value</span> <code>Completed</code><span>,</span> <span>it</span> <span>means</span> <span>that</span> <span>the container</span> <span>exited without errors</span><span>. The</span> <span>logs of the</span> <span>container</span> <span>will show</span> <span>the number of</span> <span>imported</span> <span>documents. You should now be able to access the Kiada suite</span> <span>via</span> <code>curl</code> <span>or your web browser and see</span> <span>that</span> <span>the</span> <span>Quiz service returns the</span> <span>questions</span> <span>you imported</span><span>. You can delete the</span> <code>quiz-data-importer</code> <span>Pod and the</span> <code>quiz-questions</code> <span>ConfigMap at will.</span></p>
</div>
<div class="readable-text" data-hash="6c55985d82c5205c03b9e36d8ee2e12f" data-text-hash="9c1fc406875859d8d57b9fad2a1acacc" id="137" refid="137">
<p><span>Now answer a few</span> <span>q</span><span>uiz questions</span> <span>and</span> <span>use the following command to</span> <span>check if your answers are stored</span> <span>in MongoDB:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="8140893bd184c6c123cec7c94ca0bc08" data-text-hash="de99d41afff7edb9d7df114818f2d0ba" id="138" refid="138">
<div class="code-area-container">
<pre class="code-area">$ kubectl exec quiz-0 -c mongo -- mongosh kiada --quiet --eval 'db.responses.find()'</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="88cf21b879a1fa6dee05d511b24cf04e" data-text-hash="9e7ecebaf93678e1719199335a1af74f" id="139" refid="139">
<p><span>When you run this command, the</span> <code>mongosh</code> <span>shell in pod</span> <code>quiz-0</code> <span>connects to the</span> <code>kiada</code> <span>database and displays all</span> <span>the</span> <span>documents</span> <span>stored</span> <span>in the</span> <code>responses</code> <span>collection in JSON form. Each of these documents</span> <span>represents an answer that you submitted</span><span>.</span></p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="140" refid="140">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="dd17eeaa2f795d05e363197e84670a2b" data-text-hash="dfc5d601b5694f906249d7c889d73143" id="141" refid="141">
<p> <span>This command assumes that</span> <code>quiz-0</code> <span>is the primary MongoDB replica, which should be the case</span> <span>unless</span> <span>you deviate</span><span>d</span> <span>from the instructions</span> <span>for</span> <span>creat</span><span>ing</span> <span>the StatefulSet. If the command fails, try</span> <span>running</span> <span>it in the</span> <code>quiz-1</code> <span>and</span> <code>quiz-2</code> <span>Pods. You can also find the primary replica by</span> <span>running</span> <span>the MongoDB command</span> <code>rs.hello().primary</code> <span>in any</span> <code>quiz</code> <span>Pod.</span></p>
</div>
</div>
<div class="readable-text" data-hash="69be9e8675f407da2d69a18d9c9c81c2" data-text-hash="c77a38812114d57b2494bfcb6a031ec4" id="142" refid="142">
<h2 id="sigil_toc_id_273">15.2&#160;Understanding StatefulSet behavior</h2>
</div>
<div class="readable-text" data-hash="a81a5d801a8226d6dcf87ebaf3f870b9" data-text-hash="6c33dc4cf52821d3a5ae17a47f23e690" id="143" refid="143">
<p><span>In the previous section, you created the StatefulSet and saw how the controller created the Pods. You used the cluster DNS records that were created for the headless Service to import data into the MongoDB replica set. Now you&#8217;ll put the StatefulSet to the test and learn about its behavior. First, you&#8217;ll see how it handles missing Pods and node failures.</span></p>
</div>
<div class="readable-text" data-hash="efbaf43891d940b2d3b0854281bb2a93" data-text-hash="1b0c77279d11ddd5bb28f7a31d6a979a" id="144" refid="144">
<h3 id="sigil_toc_id_274">15.2.1&#160;<span>Understanding how a StatefulSet replaces missing Pods</span></h3>
</div>
<div class="readable-text" data-hash="d4cbd4981786da56f96237ce638c8ac9" data-text-hash="138bb610341ed17cf6c1598450050e03" id="145" refid="145">
<p><span>Unlike the Pods created by a ReplicaSet, the Pods of a StatefulSet are named differently and each has its own PersistentVolumeClaim (or set of PersistentVolumeClaims if the StatefulSet contains multiple claim templates). As mentioned in the introduction, if a StatefulSet Pod is deleted and replaced by the controller with a new instance, the replica retains the same identity and is associated with the same PersistentVolumeClaim. Try deleting the</span> <code>quiz-1</code> <span>Pod as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="4a0bc42ee0e283e53ecce0f76b986631" data-text-hash="6e9bac2cd7770c489730d7b52a0c60b4" id="146" refid="146">
<div class="code-area-container">
<pre class="code-area">$ kubectl delete po quiz-1
pod "quiz-1" deleted</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="3b1ae58dd841822a4dcd441e65eee7f6" data-text-hash="03bf14466723fe287e7281086313e5a8" id="147" refid="147">
<p><span>The pod that&#8217;s created in its place has the same name, as you can see here:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="e39e03aac584451ba4e0e5e4e91ff3eb" data-text-hash="e646a8e7347e252b39e295f3021fac90" id="148" refid="148">
<div class="code-area-container">
<pre class="code-area">$ kubectl get po -l app=quiz
NAME     READY   STATUS    RESTARTS   AGE
quiz-0   2/2     Running   0          94m
quiz-1   2/2     Running   0          5s    #A
quiz-2   2/2     Running   0          94m</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIEFHRSBjb2x1bW4gaW5kaWNhdGVzIHRoYXQgdGhpcyBpcyBhIG5ldyBQb2QsIGJ1dCBpdCBoYXMgdGhlIHNhbWUgbmFtZSBhcyB0aGUgcHJldmlvdXMgcG9kLg=="></div>
</div>
</div>
<div class="readable-text" data-hash="dc68e3724997ed5e855cf3356d10f5c5" data-text-hash="46e4501cece25d2ba0c46ca5e7a2238c" id="149" refid="149">
<p><span>The IP address of the new Pod might be different, but that doesn&#8217;t matter because the DNS records have been updated to point to the new address. Clients using the Pod&#8217;s hostname to communicate with it won&#8217;t notice any difference.</span></p>
</div>
<div class="readable-text" data-hash="d60af80363cf21a20c703df4f0459acd" data-text-hash="ac82740cc8a7b1359a4c479a174b2910" id="150" refid="150">
<p><span>In general, this new Pod can be scheduled to any cluster node if the PersistentVolume bound to the PersistentVolumeClaim represents a network-attached volume and not a local volume. If the volume is local to the node, the Pod is always scheduled to this node.</span></p>
</div>
<div class="readable-text" data-hash="f584e0e7615ca4b705c671deaf20ac5c" data-text-hash="456025d7d9e7125c8076f21d876e4f17" id="151" refid="151">
<p><span>Like the ReplicaSet controller, its StatefulSet counterpart ensures that there are always the desired number of Pods configured in the</span> <code>replicas</code> <span>field. However, there&#8217;s an important difference in the guarantees that a StatefulSet provides compared to a ReplicaSet. This difference is explained next.</span></p>
</div>
<div class="readable-text" data-hash="582ab46bddfa2861c2f5338c8feb594b" data-text-hash="95e7a25ea01590bd65f6a4b71a3453e0" id="152" refid="152">
<h3 id="sigil_toc_id_275">15.2.2&#160;<span>Understanding how a StatefulSet handles node failures</span></h3>
</div>
<div class="readable-text" data-hash="271f1251567a7d7dfca7467d724c4269" data-text-hash="9b13b0eac9a41827209bb190cd36fac8" id="153" refid="153">
<p><span>StatefulSets provide much stricter concurrent Pod execution guarantees than ReplicaSets. This affects how the StatefulSet controller handles node failures and should therefore be explained first.</span></p>
</div>
<div class="readable-text" data-hash="b6e4ee5c19980d705d3c6b3176e4b425" data-text-hash="22e1b83fff4daccf5ad7fcc82dac93c9" id="154" refid="154">
<h4>Understanding the at-most-one semantics of StatefulSets</h4>
</div>
<div class="readable-text" data-hash="58c4dcf58acbc3e4c9787272936bfe84" data-text-hash="85e2a53e0827f2c750ace3748216bc7d" id="155" refid="155">
<p><span>A StatefulSet guarantees at-most-one semantics for its Pods. Since two Pods with the same name can&#8217;t be in the same namespace at the same time, the ordinal-based naming scheme of StatefulSets is sufficient to prevent two Pods with the same identity from running at the same time.</span></p>
</div>
<div class="readable-text" data-hash="ce7c3a9884a29bdf139529a41dea2d96" data-text-hash="84d6346f3c28d74843ce7b55b78d5337" id="156" refid="156">
<p><span>Remember what happens when you run a group of Pods via a ReplicaSet and one of the nodes stops reporting to the Kubernetes control plane? A few minutes later, the ReplicaSet controller determines that the node and the Pods are gone and creates replacement Pods that run on the remaining nodes, even though the Pods on the original node may still be running. If the StatefulSet controller also replaces the Pods in this scenario, you&#8217;d have two replicas with the same identity running concurrently. Let&#8217;s see if that happens.</span></p>
</div>
<div class="readable-text" data-hash="16cc17cc1076a384f66ff7e4b9e48d0d" data-text-hash="66835c6a0b02e46c954af7d92da4fe5d" id="157" refid="157">
<h4>Disconnecting a node from the network</h4>
</div>
<div class="readable-text" data-hash="fdefa066fa70c8a4ae5a5b1df5878238" data-text-hash="d04708a1266b5d44990ea2d03a446ff3" id="158" refid="158">
<p><span>As</span> <span>in the chapter 13, you&#8217;ll cause the network interface of one of the nodes to fail. You can try this exercise if your cluster has more than one node. Find the name of the node running the</span> <code>quiz-1</code> <span>Pod. Suppose it&#8217;s the node</span> <code>kind-worker2</code><span>. If you use a kind-provisioned cluster, turn off the node&#8217;s network interface as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="6c8196c02be14d1e7a892e0f70d4ee84" data-text-hash="db48a29ffad265a2e6d5cd7c41744b9e" id="159" refid="159">
<div class="code-area-container">
<pre class="code-area">$ docker exec kind-worker2 ip link set eth0 down    #A</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgUmVwbGFjZSBraW5kLXdvcmtlcjIgd2l0aCB0aGUgY29ycmVjdCBub2RlIG5hbWUu"></div>
</div>
</div>
<div class="readable-text" data-hash="6efed24b8ed0fe6efa37a8bee60e9a5f" data-text-hash="22d0eee910a75b6841c1534752802d9f" id="160" refid="160">
<p><span>If you&#8217;re using a GKE cluster, use the following command to connect to the node:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="5657ee5b391ed94d9e7c5d7e73231c88" data-text-hash="5adaeed5cb4169dd2a6bfb483ce13194" id="161" refid="161">
<div class="code-area-container">
<pre class="code-area">$ gcloud compute ssh gke-kiada-default-pool-35644f7e-300l    #A</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgUmVwbGFjZSB0aGUgbm9kZSBuYW1lIHdpdGggdGhlIG5hbWUgb2YgeW91ciBub2RlLg=="></div>
</div>
</div>
<div class="readable-text" data-hash="402def220d24793d366a9d289be21678" data-text-hash="8d8b21ce02a481672184fcd5677b6553" id="162" refid="162">
<p><span>Run the following command on the node to shut down its network interface:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="bb9dade00de0e3fcd27d4e8b1954f3f1" data-text-hash="ba5041e782a1c79e01885836136ce513" id="163" refid="163">
<div class="code-area-container">
<pre class="code-area">$ sudo ifconfig eth0 down</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="164" refid="164">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="27f6920dd971f2afa68f148508b349e7" data-text-hash="0c3e2d5eacc2521093eb31c9e7458a9a" id="165" refid="165">
<p> <span>Shutting down the network interface will hang the</span> <code>ssh</code> <span>session. You can end the session by pressing Enter followed by &#8220;~.&#8221; (tilde and dot, without the quotes).</span></p>
</div>
</div>
<div class="readable-text" data-hash="4f73f4dea534d07117ec3beabc16153b" data-text-hash="f1de3b195c4910c2f1dd6c5fcde070be" id="166" refid="166">
<p><span>Because</span> <span>the node&#8217;s network interface is down, the Kubelet running on the node can no longer contact the Kubernetes API server and tell it that the node and all its Pods are still running. The Kubernetes control plane soon marks the node as</span> <code>NotReady</code><span>, as seen here:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="c0da7fec1b08551fa4ebc5529a741fc9" data-text-hash="89aa27e75e775e7982af33b6ef36a17a" id="167" refid="167">
<div class="code-area-container">
<pre class="code-area">$ kubectl get nodes
NAME                 STATUS     ROLES                  AGE   VERSION
kind-control-plane   Ready      control-plane,master   10h   v1.23.4
kind-worker          Ready      &lt;none&gt;                 10h   v1.23.4
kind-worker2         NotReady   &lt;none&gt;                 10h   v1.23.4</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIG5vZGUgaXMgbm8gbG9uZ2VyIHJlYWR5IGJlY2F1c2UgaXQgc3RvcHBlZCBjb21tdW5pY2F0aW5nIHdpdGggdGhlIEt1YmVybmV0ZXMgQVBJLg=="></div>
</div>
</div>
<div class="readable-text" data-hash="50d1c7d3eede4fe52f89bf048ca583c9" data-text-hash="0643c2e58e9247e9b332249bb80fd837" id="168" refid="168">
<p><span>After a few minutes, the status of the</span> <code>quiz-1</code> <span>Pod that was running on this node changes to</span> <code>Terminating</code><span>, as you can see in the Pod list:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="f96fc78e0022790cc67b10d85ddd8c1e" data-text-hash="7cc39e3908f452d95d736246e422ff75" id="169" refid="169">
<div class="code-area-container">
<pre class="code-area">$ kubectl get pods -l app=quiz
NAME     READY   STATUS        RESTARTS   AGE
quiz-0   2/2     Running       0          12m
quiz-1   2/2     Terminating   0          7m39s    #A
quiz-2   2/2     Running       0          12m</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhpcyBQb2QgaXMgYmVpbmcgdGVybWluYXRlZCBiZWNhdXNlIGl0cyBub2RlIGlzIGRvd24u"></div>
</div>
</div>
<div class="readable-text" data-hash="522d9475a6b053ec372623e57acea119" data-text-hash="d3a15cfdd92329c163d22e4c58facb4d" id="170" refid="170">
<p><span>When you inspect the Pod with the</span> <code>kubectl describe</code> <span>command, you see a</span> <code>Warning</code> <span>event with the message &#8220;</span><code>Node is not ready&#8221;</code> <span>as shown here:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="9c1c6e2128e55c341ca991f604cc87d6" data-text-hash="8e08bbecb38856f0bcf0ff803fd5b22b" id="171" refid="171">
<div class="code-area-container">
<pre class="code-area">$ kubectl describe po quiz-1
...
Events:
  Type     Reason                   Age   From                     Message
  ----     ------                   ----  ----                     -------
  Warning  NodeNotReady             11m   node-controller          Node is not ready    #A</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIE5vZGVOb3RSZWFkeSBldmVudCBpbmRpY2F0ZXMgdGhhdCB0aGUgbm9kZSB0aGUgUG9kIGlzIHJ1bm5pbmcgb24gaXMgbm8gbG9uZ2VyIHJlc3BvbmRpbmcu"></div>
</div>
</div>
<div class="readable-text" data-hash="a89d40a493809184edca9667870ef047" data-text-hash="b68cf192018515e94876cb247bfd6fe7" id="172" refid="172">
<h4>Understanding why the StatefulSet controller doesn&#8217;t replace the Pod</h4>
</div>
<div class="readable-text" data-hash="3c4f244af1b980e1256c0a39280a51a2" data-text-hash="72b1ea884441c210ee4eedde616f21c9" id="173" refid="173">
<p><span>At this point I&#8217;d like to point out that the Pod&#8217;s containers are still running. The node isn&#8217;t down, it only lost network connectivity. The same thing happens if the Kubelet process running on the node fails, but the containers keep running.</span></p>
</div>
<div class="readable-text" data-hash="df7809cf488e150c546bfddaf087bcd8" data-text-hash="28e6bd2973bf3490ed4460900eb9658b" id="174" refid="174">
<p><span>This is an important fact because it explains why the StatefulSet controller shouldn&#8217;t delete and recreate the Pod. If the StatefulSet controller deletes and recreates the Pod while the Kubelet is down, the new Pod would be scheduled to another node and the Pod&#8217;s containers would start. There would then be two instances of the same workload running with the same identity. That&#8217;s why the StatefulSet controller doesn&#8217;t do that.</span></p>
</div>
<div class="readable-text" data-hash="ca54519f7a6b4589fe5aa6c8fb2d1e93" data-text-hash="8a88e381eaad882c9a01f24cc52d7250" id="175" refid="175">
<h4>Manually deleting the Pod</h4>
</div>
<div class="readable-text" data-hash="2868645e589abb56ecb778e89fb84b8b" data-text-hash="637b8bf8bb65d209090b1e6b125831c3" id="176" refid="176">
<p><span>If you want the Pod to be recreated elsewhere, manual intervention is required. A cluster operator must confirm that the node has indeed failed and manually delete the Pod object. However, the Pod object is already marked for deletion, as indicated by its status, which shows the Pod as</span> <code>Terminating</code><span>. Deleting the Pod with the usual</span> <code>kubectl delete pod</code> <span>command has no effect.</span></p>
</div>
<div class="readable-text" data-hash="9c193f6518afabafed1177fa34ab5a5d" data-text-hash="97e9c3547a647576138cc864dc7faada" id="177" refid="177">
<p><span>The Kubernetes control plane waits for the Kubelet to report that the Pod&#8217;s containers have terminated. Only then is the deletion of the Pod object complete. However, since the Kubelet responsible for this Pod isn&#8217;t working, this never happens. To delete the Pod without waiting for confirmation, you must delete it as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="94219834f8656dedfcc9598f1f8b93d6" data-text-hash="2c8856c95a3feb1c8e6e5fc094f7ecf3" id="178" refid="178">
<div class="code-area-container">
<pre class="code-area">$ kubectl delete pod quiz-1 --force --grace-period 0
warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
pod "quiz-0" force deleted</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="bf0e490684466ada888c376d87ecce6a" data-text-hash="a6e060367202054767c90d648aaa6c41" id="179" refid="179">
<p><span>Note the warning that the Pod&#8217;s containers may keep running. That&#8217;s the reason why you must make sure that the node has really failed before deleting the Pod in this way.</span></p>
</div>
<div class="readable-text" data-hash="c212c2877700a37ff44156f90a265f86" data-text-hash="2082286814f0354cd081edaada458e4c" id="180" refid="180">
<h4>Recreating the Pod</h4>
</div>
<div class="readable-text" data-hash="4db884da9d0ac99af4faeb0e500d23b4" data-text-hash="6b8942b636dabac627d4bbc2ed2c84ec" id="181" refid="181">
<p><span>After you delete the Pod, it&#8217;s replaced by the StatefulSet controller, but the Pod may not start. There are two possible scenarios. Which one occurs depends on whether the replica&#8217;s PersistentVolume is a local volume, as in <i>kind</i>, or a network-attached volume, as in GKE.</span></p>
</div>
<div class="readable-text" data-hash="5c8380b1e0037fd996e7745a8d1b9a06" data-text-hash="a008627ba55095488420f3fc0c7f439f" id="182" refid="182">
<p><span>If the PersistentVolume is a local volume on the failed node, the Pod can&#8217;t be scheduled and its</span> <code>STATUS</code> <span>remains</span> <code>Pending</code><span>, as shown here:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="96a4e6305d17ec0c180ee3161b991927" data-text-hash="7685c8e915af974a8bed68a69db01d38" id="183" refid="183">
<div class="code-area-container">
<pre class="code-area">$ kubectl get pod quiz-1 -o wide
NAME     READY   STATUS    RESTARTS   AGE     IP       NODE     NOMINATED NODE   
quiz-1   0/2     Pending   0          2m38s   &lt;none&gt;   &lt;none&gt;   &lt;none&gt;           #A</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIFBvZCBoYXNu4oCZdCBiZWVuIHNjaGVkdWxlZCB0byBhbnkgbm9kZS4="></div>
</div>
</div>
<div class="readable-text" data-hash="f0d584fd66c31a2fcc8aed97a77e8fac" data-text-hash="ef28f848ba164fa6484507fac5e60460" id="184" refid="184">
<p><span>The Pod&#8217;s events show why the Pod can&#8217;t be scheduled. Use the</span> <code>kubectl describe</code> <span>command to display them as follows.</span></p>
</div>
<div class="browsable-container listing-container" data-hash="2f41eec1f89bef5b2d307187e5cb4b88" data-text-hash="063ad9776583fbd444f7217e63eea42f" id="185" refid="185">
<div class="code-area-container">
<pre class="code-area">$ kubectl describe pod quiz-1
...
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  21s   default-scheduler  0/3 nodes are available:    #A
1 node had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate,   #B
1 node had taint {node.kubernetes.io/unreachable: }, that the pod didn't tolerate, #C
1 node had volume node affinity conflict.    #D</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIHNjaGVkdWxlciBjb3VsZG7igJl0IGZpbmQgYW55IGFwcHJvcHJpYXRlIG5vZGVzIHRvIHNjaGVkdWxlIHRoZSBQb2QuCiNCIFRoZSBjb250cm9sIHBsYW5lIG5vZGUgYWNjZXB0cyBvbmx5IEt1YmVybmV0ZXMgc3lzdGVtIHdvcmtsb2FkcyBhbmQgbm8gcmVndWxhciB3b3JrbG9hZHMgbGlrZSB0aGlzIFBvZC4KI0MgVGhlIGtpbmQtd29ya2VyMiBub2RlIGlzIHVucmVhY2hhYmxlLgojRCBUaGUgUG9kIGNhbuKAmXQgYmUgc2NoZWR1bGVkIHRvIHRoZSBraW5kLXdvcmtlciBub2RlLCBiZWNhdXNlIHRoZSBQZXJzaXN0ZW50Vm9sdW1lIGNhbuKAmXQgYmUgYXR0YWNoZWQgdGhlcmUu"></div>
</div>
</div>
<div class="readable-text" data-hash="09403165ead94d092739c4e02ed175b2" data-text-hash="9df87ca9418b4cc5d9bf2cad87f8eae4" id="186" refid="186">
<p><span>The event message mentions taints, which you&#8217;ll learn about in chapter 23. Without going into detail here, I&#8217;ll just say that the Pod can&#8217;t be scheduled to any of the three nodes because one node is a control plane node, another node is unreachable (duh, you just made it so), but the most important part of the warning message is the part about the affinity conflict. The new</span> <code>quiz-1</code> <span>Pod can only be scheduled to the same node as the previous Pod instance, because that&#8217;s where its volume is located. And since this node isn&#8217;t reachable, the Pod can&#8217;t be scheduled.</span></p>
</div>
<div class="readable-text" data-hash="d8e42ffe05513aadec6926fbce6168fe" data-text-hash="55c18de45bd178fda8a05f7e72ecbeb6" id="187" refid="187">
<p><span>If you&#8217;re running this exercise on GKE or other cluster that uses network-attached volumes, the Pod will be scheduled to another node but may not be able to run if the volume can&#8217;t be detached from the failed node and attached to that other node. In this case, the</span> <code>STATUS</code> <span>of the Pod is as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="6dfcc433bc3d66ecb08f5e30013b38f9" data-text-hash="eb6a7f8d063b0cbe6cd559764e7cfe05" id="188" refid="188">
<div class="code-area-container">
<pre class="code-area">$ kubectl get pod quiz-1 -o wide
NAME     READY   STATUS              RESTARTS   AGE   IP        NODE     
quiz-1   0/2     ContainerCreating   0          38s   1.2.3.4   gke-kiada-...   #A</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIFBvZCBoYXMgYmVlbiBzY2hlZHVsZWQsIGJ1dCBpdHMgY29udGFpbmVycyBoYXZlbuKAmXQgYmVlbiBzdGFydGVkLg=="></div>
</div>
</div>
<div class="readable-text" data-hash="7247928cfccb81a972eee21b39d544e4" data-text-hash="12d3fd3356252989cb655f09768cb306" id="189" refid="189">
<p><span>The Pod&#8217;s events indicate that the PersistentVolume can&#8217;t be detached. Use</span> <code>kubectl describe</code> <span>as follows to display them:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="3e44c75b864e78852598e03d56cdcea4" data-text-hash="3264368ffa19e6f3b48dae9ce9f6ee39" id="190" refid="190">
<div class="code-area-container">
<pre class="code-area">$ kubectl describe pod quiz-1
...
Events:
  Type     Reason              Age   From                     Message
  ----     ------              ----  ----                     -------
Warning FailedAttachVolume 77s attachdetach-controller Multi-Attach error for volume "pvc-8d9ec7e7-bc51-497c-8879-2ae7c3eb2fd2" Volume is already exclusively attached to one node and can't be attached to another</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="75fa03aac132a70af3d21cfc93043a67" data-text-hash="a6d7449c9b4a4f46847b517d2c63391a" id="191" refid="191">
<h4>Deleting the PersistentVolumeClaim to get the new Pod to run</h4>
</div>
<div class="readable-text" data-hash="9fa5158321dc2b0ea5e3624ed2939206" data-text-hash="6b20b78bb6740745a4bfe1a861721a13" id="192" refid="192">
<p><span>What do you do if the Pod can&#8217;t be attached to the same volume? If the workload running in the Pod can rebuild its data from scratch, for example by replicating the data from the other replicas, you can delete the PersistentVolumeClaim so that a new one can be created and bound to a new PersistentVolume. However, since the StatefulSet controller only creates the PersistentVolumeClaims when it creates the Pod, you must also delete the Pod object. You can delete both objects as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="c04709cf456d1b0cb19f62e983d91d1f" data-text-hash="0cccf1ff63d7d1b3819f3ca29f589816" id="193" refid="193">
<div class="code-area-container">
<pre class="code-area">$ kubectl delete pvc/db-data-quiz-1 pod/quiz-1
persistentvolumeclaim "db-data-quiz-1" deleted
pod "quiz-1" deleted</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="e29103e02cd642d71f9a19b163cc0224" data-text-hash="60e7279ce67013ad37e1e2f2aa382599" id="194" refid="194">
<p><span>A new PersistentVolumeClaim and a new Pod are created. The PersistentVolume bound to the claim is empty, but MongoDB replicates the data automatically.</span></p>
</div>
<div class="readable-text" data-hash="bd3f04623b40a8600655b572360607d4" data-text-hash="3d08af4387c1cb0b39479ca0a522f83f" id="195" refid="195">
<h4>Fixing the node</h4>
</div>
<div class="readable-text" data-hash="1d9ada750b84843a9857cea3eeb6f03a" data-text-hash="0ba368aad0f7fbe298002624da4a9b9d" id="196" refid="196">
<p><span>Of course, you can save yourself all that trouble if you can fix the node. If you&#8217;re running this example on GKE, the system does it automatically by restarting the node a few minutes after it goes offline. To restore the node when using the <i>kind</i> tool, run the following commands:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="22f4f4b941e2be68f5503f8d66be7e0d" data-text-hash="31632ab4c2720a9f81b74e32c914e7b0" id="197" refid="197">
<div class="code-area-container">
<pre class="code-area">$ docker exec kind-worker2 ip link set eth0 up
$ docker exec kind-worker2 ip route add default via 172.18.0.1    #A</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgWW91ciBjbHVzdGVyIG1heSB1c2UgYSBkaWZmZXJlbnQgSVAgZ2F0ZXdheS4gWW91IGNhbiBmaW5kIGl0IHdpdGggdGhlIGRvY2tlciBpbnNwZWN0IG5ldHdvcmsgY29tbWFuZCwgYXMgZGVzY3JpYmVkIGluIGNoYXB0ZXIgMTMu"></div>
</div>
</div>
<div class="readable-text" data-hash="f18e42ae9ab8ca000e3d1569423fd0ba" data-text-hash="71e5b1bc966630a287e227c469a664b2" id="198" refid="198">
<p><span>When the node is back online, the deletion of the Pod is complete, and the new</span> <code>quiz-1</code> <span>Pod is created. In a <i>kind</i> cluster, the Pod is scheduled to the same node because the volume is local.</span></p>
</div>
<div class="readable-text" data-hash="7af15a811a7441858029ec14cd276c0d" data-text-hash="0b00ecad422d65beae6a1db4b6604f36" id="199" refid="199">
<h3 id="sigil_toc_id_276">15.2.3&#160;Scaling a StatefulSet</h3>
</div>
<div class="readable-text" data-hash="aa7d62c1cf62edbf7e6ad625bbc39eb3" data-text-hash="58e5105dba7dfa12a75885b18378f526" id="200" refid="200">
<p><span>Just like ReplicaSets and Deployments, you can also scale StatefulSets. When you scale up a StatefulSet, the controller creates both a new Pod and a new PersistentVolumeClaim. But what happens when you scale it down? Are the PersistentVolumeClaims deleted along with the Pods?</span></p>
</div>
<div class="readable-text" data-hash="412a8b647d76cd4a4b86bd558f0c5458" data-text-hash="1df5800b97942924057a8c76d485a99c" id="201" refid="201">
<h4>Scaling down</h4>
</div>
<div class="readable-text" data-hash="263348f91efc662f45090e4e82cb3ce9" data-text-hash="0e08d6658f9571f6b045f19755bf037c" id="202" refid="202">
<p><span>To scale a StatefulSet, you can use the</span> <code>kubectl scale</code> <span>command or change the value of the</span> <code>replicas</code> <span>field in the manifest of the StatefulSet object. Using the first approach, scale the</span> <code>quiz</code> <span>StatefulSet down to a single replica as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="5b87dc4b4ba21eddba176c8b9a631ecd" data-text-hash="b44b8be962b467a920d559fb091b9a15" id="203" refid="203">
<div class="code-area-container">
<pre class="code-area">$ kubectl scale sts quiz --replicas 1
statefulset.apps/quiz scaled</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="5f0d4a684bd2feaeaac1f8ca3ff879d7" data-text-hash="35ec9b54ee25910f2056ec58aa4642ba" id="204" refid="204">
<p><span>As expected, two Pods are now in the process of termination:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="fc733cfabc22640444ea7018472f5bb6" data-text-hash="488c21febffeb50d7c84bfca74dd1288" id="205" refid="205">
<div class="code-area-container">
<pre class="code-area">$ kubectl get pods -l app=quiz
NAME     READY   STATUS        RESTARTS   AGE
quiz-0   2/2     Running       0          1h
quiz-1   2/2     Terminating   0          14m    #A
quiz-2   2/2     Terminating   0          1h    #A</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlc2UgUG9kcyBhcmUgYmVpbmcgZGVsZXRlZC4="></div>
</div>
</div>
<div class="readable-text" data-hash="a3ccfddab11192029480b13916e7ec83" data-text-hash="d1b47ed1c16bdde5bfb1a274395aa0db" id="206" refid="206">
<p><span>Unlike ReplicaSets, when you scale down a StatefulSet, the Pod with the highest ordinal number is deleted first. You scaled down the</span> <code>quiz</code> <span>StatefulSet from three replicas to one, so the two Pods with the highest ordinal numbers,</span> <code>quiz-2</code> <span>and</span> <code>quiz-1</code><span>, were deleted. This scaling method ensures that the ordinal numbers of the Pods always start at zero and end at a number less than the number of replicas.</span></p>
</div>
<div class="readable-text" data-hash="8cfb2a9351c37db24f5247e850556de0" data-text-hash="867d890e0be76e844c3a17d31f449bed" id="207" refid="207">
<p><span>But what happens to the PersistentVolumeClaims? List them as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="d097a4e36529313445950c3777378dc7" data-text-hash="edabf99f69d3d33f40b9617d9389c632" id="208" refid="208">
<div class="code-area-container">
<pre class="code-area">$ kubectl get pvc -l app=quiz
NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE
db-data-quiz-0   Bound    pvc...1bf8ccaf   1Gi        RWO            standard       1h
db-data-quiz-1   Bound    pvc...c8f860c2   1Gi        RWO            standard       1h
db-data-quiz-2   Bound    pvc...2cc494d6   1Gi        RWO            standard       1h</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="95ee4055d8f28750a0133621a2d57173" data-text-hash="c495322c153df466a4bd2e739d1133c4" id="209" refid="209">
<p><span>Unlike Pods, their PersistentVolumeClaims are preserved. This is because deleting a claim would cause the bound PersistentVolume to be recycled or deleted, resulting in data loss. Retaining PersistentVolumeClaims is the default behavior, but you can configure the StatefulSet to delete them via the</span> <code>persistentVolumeClaimRetentionPolicy</code> <span>field, as you&#8217;ll learn later. The other option is to delete the claims manually.</span></p>
</div>
<div class="readable-text" data-hash="f2f71b562cfa05a2064dde720220bf0c" data-text-hash="a71c23aa54930257c4d3edfd8c4e4e4a" id="210" refid="210">
<p><span>It&#8217;s worth noting that if you scale the</span> <code>quiz</code> <span>StatefulSet to just one replica, the</span> <code>quiz</code> <span>Service is no longer available, but this has nothing to do with Kubernetes. It&#8217;s because you configured the MongoDB replica set with three replicas, so at least two replicas are needed to have quorum. A single replica has no quorum and therefore must deny both reads and writes. This causes the readiness probe in the</span> <code>quiz-api</code> <span>container to fail, which in turn causes the Pod to be removed from the Service and the Service to be left with no Endpoints. To confirm, list the Endpoints as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="222f236836366e38c6a10f096672b98c" data-text-hash="3b93d8288f42eba177beb9edf0a8e7d0" id="211" refid="211">
<div class="code-area-container">
<pre class="code-area">$ kubectl get endpoints -l app=quiz
NAME        ENDPOINTS          AGE
quiz                           1h    #A
quiz-pods   10.244.1.9:27017   1h    #B</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIHF1aXogU2VydmljZSBoYXMgbm8gZW5kcG9pbnRzLgojQiBUaGUgcXVpei1wb2RzIFNlcnZpY2Ugc3RpbGwgaGFzIHF1aXotMCBhcyBhbiBlbmRwb2ludCBiZWNhdXNlIHRoZSBTZXJ2aWNlIGlzIGNvbmZpZ3VyZWQgdG8gaW5jbHVkZSBhbGwgZW5kcG9pbnRzIHJlZ2FyZGxlc3Mgb2YgdGhlaXIgcmVhZHkgc3RhdGUu"></div>
</div>
</div>
<div class="readable-text" data-hash="aa43b9c4d3245519568e3878bdae4bb9" data-text-hash="36afa15bdc886775a0c9c83c26e41470" id="212" refid="212">
<p><span>After you scale down the StatefulSet, you need to reconfigure the MongoDB replica set to work with the new number of replicas, but that&#8217;s beyond the scope of this book. Instead, let&#8217;s scale the StatefulSet back up to get the quorum again.</span></p>
</div>
<div class="readable-text" data-hash="4a8452fc9fc44b8099b842daa2242cd3" data-text-hash="402f27728c39b90ff1b3cac43077bcf4" id="213" refid="213">
<h4>Scaling up</h4>
</div>
<div class="readable-text" data-hash="9e64b032f77c93237c532082e2eb25d7" data-text-hash="7d2e4acf4708f4a1b0702c74f9d3ea8e" id="214" refid="214">
<p><span>Since PersistentVolumeClaims are preserved when you scale down a StatefulSet, they can be reattached when you scale back up, as shown in the following figure. Each Pod is associated with the same PersistentVolumeClaim as before, based on the Pod&#8217;s ordinal number.</span></p>
</div>
<div class="browsable-container figure-container" data-hash="30d27c0bba8cdb9e5f0ef9e27bc888f6" data-text-hash="41c1bf27f8d59cd96db69fec3f5f3a2d" id="215" refid="215">
<h5><span>Figure 15.6 StatefulSets don&#8217;t delete PersistentVolumeClaims when scaling down; then they reattach them when scaling back up.</span></h5>
<img alt="" data-processed="true" height="418" id="Picture_21" loading="lazy" src="EPUB/images/15_img_0008.png" width="893">
</div>
<div class="readable-text" data-hash="2b6bd988fc1446511e0dc0e4f5663a5e" data-text-hash="4797712d3b7801d55db64e51ae355bf9" id="216" refid="216">
<p><span>Scale the</span> <code>quiz</code> <span>StatefulSet back up to three replicas as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="908a8212f8f5956b8e873f1810bf4784" data-text-hash="8bb4efdaa4baf24d61a5870689ca7b65" id="217" refid="217">
<div class="code-area-container">
<pre class="code-area">$ kubectl scale sts quiz --replicas 3
statefulset.apps/quiz scaled</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="8f5a2f60b442c6f816783ac11cef34db" data-text-hash="704f04ede3f2cb6c666f5d603aaa7afb" id="218" refid="218">
<p><span>Now check each Pod to see if it&#8217;s associated with the correct PersistentVolumeClaim. The quorum is restored, all Pods are ready, and the Service is available again. Use your web browser to confirm.</span></p>
</div>
<div class="readable-text" data-hash="3a9c6c62abdffcb2c72ed00240f72a28" data-text-hash="4d247dff3e3bdd54b7ae1b8d3f018e79" id="219" refid="219">
<p><span>Now scale the StatefulSet to five replicas. The controller creates two additional Pods and PersistentVolumeClaims, but the Pods aren&#8217;t ready. Confirm this as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="e6a53751556e27aa662c9e928e764ad2" data-text-hash="b43748af52894dc8cdc5ef4f583bcd44" id="220" refid="220">
<div class="code-area-container">
<pre class="code-area">$ kubectl get pods quiz-3 quiz-4
NAME     READY   STATUS    RESTARTS   AGE
quiz-3   1/2     Running   0          4m55s    #A
quiz-4   1/2     Running   0          4m55s    #A</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgQSBjb250YWluZXIgaW4gZWFjaCBvZiB0aGVzZSBQb2RzIGlzIG5vdCByZWFkeS4="></div>
</div>
</div>
<div class="readable-text" data-hash="3e81a1c5c08d25c2c40af71d3a305c72" data-text-hash="e1ca1d5a9d76ace162e97fe2e246cf40" id="221" refid="221">
<p><span>As you can see, only one of the two containers is ready in each replica. There&#8217;s nothing wrong with these replicas except that they haven&#8217;t been added to the MongoDB replica set. You could add them by reconfiguring the replica set, but that&#8217;s beyond the scope of this book, as mentioned earlier.</span></p>
</div>
<div class="readable-text" data-hash="ab11d77b5fe49d6f0a0c76ef204c8727" data-text-hash="4275587c4fa9f2eac67b66fdf2803e00" id="222" refid="222">
<p><span>You&#8217;re probably starting to realize that managing stateful applications in Kubernetes involves more than just creating and managing a StatefulSet object. That&#8217;s why you usually use a Kubernetes Operator for this, as explained in the last part of this chapter.</span></p>
</div>
<div class="readable-text" data-hash="d9044e35c9004b92a0bf02aa4ab96cec" data-text-hash="ff1b45e835a0142a44caee00fabbf67a" id="223" refid="223">
<p><span>Before I conclude this section on StatefulSet scaling, I want to point out one more thing. The</span> <code>quiz</code> <span>Pods are exposed by two Services: the regular</span> <code>quiz</code> <span>Service, which addresses only Pods that are ready, and the headless</span> <code>quiz-pods</code> <span>Service, which includes all Pods, regardless of their readiness status. The kiada Pods connect to the</span> <code>quiz</code> <span>Service, and therefore all the requests sent to the Service are successful, as the requests are forwarded only to the three healthy Pods.</span></p>
</div>
<div class="readable-text" data-hash="ccd7dbde19e3f5e71d9a8ecc3beb9230" data-text-hash="a7ad92ad681691ffe62a83e436734254" id="224" refid="224">
<p><span>Instead of adding the</span> <code>quiz-pods</code> <span>Service, you could&#8217;ve made the</span> <code>quiz</code> <span>Service headless, but then you&#8217;d have had to choose whether or not the Service should publish the addresses of unready Pods. From the clients&#8217; point of view, Pods that aren&#8217;t ready shouldn&#8217;t be part of the Service. From MongoDB&#8217;s perspective, all Pods must be included because that&#8217;s how the replicas find each other. Using two Services solves this problem. For this reason, it&#8217;s common for a StatefulSet to be associated with both a regular Service and a headless Service.</span></p>
</div>
<div class="readable-text" data-hash="e0e783eb696f8499e0278faf4ada14f0" data-text-hash="5a7c35f6e5866fd836fc6c1ae6918bc3" id="225" refid="225">
<h3 id="sigil_toc_id_277">15.2.4&#160;<span>Changing the PersistentVolumeClaim retention policy</span></h3>
</div>
<div class="readable-text" data-hash="f608809ec3736c630f1b1ae4247edf75" data-text-hash="6fceac6a38dd3046c00e87ec5ff76550" id="226" refid="226">
<p><span>In the previous section, you learned that StatefulSets preserve the PersistentVolumeClaims by default when you scale them down. However, if the workload managed by the StatefulSet never requires data to be preserved, you can configure the StatefulSet to automatically delete the PersistentVolumeClaim by setting the</span> <code>persistentVolumeClaimRetentionPolicy</code> <span>field. In this field, you specify the retention policy to be used during scaledown and when the StatefulSet is deleted.</span></p>
</div>
<div class="readable-text" data-hash="36666a463c165825a1e6f0b2a2abb978" data-text-hash="aa9c7c097da9b4d4bd219947bab530cf" id="227" refid="227">
<p><span>For example, to configure the</span> <code>quiz</code> <span>StatefulSet to delete the PersistentVolumeClaims when the StatefulSet is scaled but retain them when it&#8217;s deleted, you must set the policy as shown in the following listing, which shows part of the</span> <code>sts.quiz.pvcRetentionPolicy.yaml</code> <span>manifest file.</span></p>
</div>
<div class="browsable-container listing-container" data-hash="133ec5686cff6e9e50e661d28e9681ab" data-text-hash="174d1503fa10f8c0e2b56c205e4de5d6" id="228" refid="228">
<h5>Listing 15.4 Configuring the PersistentVolumeClaim retention policy in a StatefulSet</h5>
<div class="code-area-container">
<pre class="code-area">apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: quiz
spec:
  persistentVolumeClaimRetentionPolicy:
    whenScaled: Delete    #A
    whenDeleted: Retain    #B
  ...</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgV2hlbiB0aGUgU3RhdGVmdWxTZXQgaXMgc2NhbGVkIGRvd24sIHRoZSBQZXJzaXN0ZW50Vm9sdW1lQ2xhaW1zIGFyZSBkZWxldGVkLgojQiBXaGVuIHRoZSBTdGF0ZWZ1bFNldCBpcyBkZWxldGVkLCB0aGUgUGVyc2lzdGVudFZvbHVtZUNsYWltcyBhcmUgcHJlc2VydmVkLg=="></div>
</div>
</div>
<div class="readable-text" data-hash="38b0c8505975ff3fde347b1e75638eac" data-text-hash="68fae9d404edea53ada809026c0aa256" id="229" refid="229">
<p><span>The whenScaled and whenDeleted fields are self-explanatory. Each field can either have the value</span> <code>Retain</code><span>, which is the default, or</span> <code>Delete</code><span>. Apply this manifest file using</span> <code>kubectl apply</code> <span>to change the PersistentVolumeClaim retention policy in the</span> <code>quiz</code> <span>StatefulSet as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="85ad1ffb259137cf56b129f9aa53a071" data-text-hash="6c052d3e12005cca851737d5b9f1ac24" id="230" refid="230">
<div class="code-area-container">
<pre class="code-area">$ kubectl apply -f sts.quiz.pvcRetentionPolicy.yaml</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="231" refid="231">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="bbd10a3033572537ce03805ca57c17f0" data-text-hash="e221aca9be5faf5b284ca0e9c212cf38" id="232" refid="232">
<p> <span>At the time of writing, this is still an alpha-level feature. For the policy to be honored by the StatefulSet controller, you must enable the feature gate</span> <code>StatefulSetAutoDeletePVC</code> <span>when you create the cluster. To do this in the kind tool, use the</span> <code>create-kind-cluster.sh</code> <span>and</span> <code>kind-multi-node.yaml</code> <span>files in the</span> <code>Chapter15/</code> <span>directory in the book&#8217;s code archive.</span></p>
</div>
</div>
<div class="readable-text" data-hash="c3e90db62f3c464715f1ac4abfd69823" data-text-hash="1d123669336e4d7c2dbc2ce072ffc01c" id="233" refid="233">
<h4>Scaling the StatefulSet</h4>
</div>
<div class="readable-text" data-hash="e720a6a0da2a774e496dd1f61f2c6292" data-text-hash="b78bfa7549701dc32adca8e5376fc2ab" id="234" refid="234">
<p><span>The</span> <code>whenScaled</code> <span>policy in the</span> <code>quiz</code> <span>StatefulSet is now set to</span> <code>Delete</code><span>. Scale the StatefulSet to three replicas, to remove the two unhealthy Pods and their PersistentVolumeClaims.</span></p>
</div>
<div class="browsable-container listing-container" data-hash="908a8212f8f5956b8e873f1810bf4784" data-text-hash="8bb4efdaa4baf24d61a5870689ca7b65" id="235" refid="235">
<div class="code-area-container">
<pre class="code-area">$ kubectl scale sts quiz --replicas 3
statefulset.apps/quiz scaled</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="7d6457e2290ec9cb651cadb945ac77d6" data-text-hash="fab89155e960300836d4682051f9b1c5" id="236" refid="236">
<p><span>List the PersistentVolumeClaims to confirm that there are only three left.</span></p>
</div>
<div class="readable-text" data-hash="d32e48a2e7dc672dc28e517862a20115" data-text-hash="e9a1e3f6f14666d4543f39e7935dad44" id="237" refid="237">
<h4>Deleting the StatefulSet</h4>
</div>
<div class="readable-text" data-hash="94b1612eacba1ef53197c5230b062700" data-text-hash="56c65ce34c2f08dd7df9ad42eab2734f" id="238" refid="238">
<p><span>Now let&#8217;s see if the</span> <code>whenDeleted</code> <span>policy is followed. Your aim is to delete the Pods, but not the PersistentVolumeClaims. You&#8217;ve already set the</span> <code>whenDeleted</code> <span>policy to</span> <code>Retain</code><span>, so you can delete the StatefulSet as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="8a137bce4cc73d04c5f2fdf22f05adee" data-text-hash="f4a12cacaa5bac103731e1d25de17192" id="239" refid="239">
<div class="code-area-container">
<pre class="code-area">$ kubectl delete sts quiz
statefulset.apps "quiz" deleted</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="4ca4e420569250cae6abdf0d3ef5a2ed" data-text-hash="a1e49a6261964c463f78c50991860d30" id="240" refid="240">
<p><span>List the PersistentVolumeClaims to confirm that all three are present. The MongoDB data files are therefore preserved.</span></p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="241" refid="241">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="7e02b29abc294a31d047f4bf22e7dcd7" data-text-hash="34a6454c11774a9cebcbb83be016778d" id="242" refid="242">
<p> <span>If you want to delete a StatefulSet but keep the Pods and the PersistentVolumeClaims, you can use the</span> <code>--cascade=orphan</code> <span>option. In this case, the PersistentVolumeClaims will be preserved even if the retention policy is set to</span> <code>Delete</code><span>.</span></p>
</div>
</div>
<div class="readable-text" data-hash="865ddcb517c350bf3dd56834e035cd7a" data-text-hash="f5724fe10007ccfbac94bd1f286959ff" id="243" refid="243">
<h4>Ensuring data is never lost</h4>
</div>
<div class="readable-text" data-hash="ccf1f3a8cf70fa4e57b61153ddcc5d60" data-text-hash="290e79b6100a5980322948cb00b76002" id="244" refid="244">
<p><span>To conclude this section, I want to caution you against setting either retention policy to</span> <code>Delete</code><span>. Consider the example just shown. You set the</span> <code>whenDeleted</code> <span>policy to</span> <code>Retain</code> <span>so that the data is preserved if the StatefulSet is accidentally deleted, but since the</span> <code>whenScaled</code> <span>policy is set to</span> <code>Delete</code><span>, the data would still be lost if the StatefulSet is scaled to zero before it&#8217;s deleted.</span></p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="90ba996504b189776b490cb27e1ac229" data-text-hash="1eab470a6f707b92e15040773251a746" id="245" refid="245">
<h5>TIP</h5>
</div>
<div class="readable-text" data-hash="cd44c8edd2a76b574e6b20120a841fbf" data-text-hash="0ffb80a819b2e8ef5f9118efd9d799df" id="246" refid="246">
<p> <span>Set the</span> <code>persistentVolumeClaimRetentionPolicy</code> <span>to</span> <code>Delete</code> <span>only if the data stored in the PersistentVolumes associated with the StatefulSet is retained elsewhere or doesn&#8217;t need to be retained. You can always delete the PersistentVolumeClaims manually. Another way to ensure data retention is to set the</span> <code>reclaimPolicy</code> <span>in the StorageClass referenced in the PersistentVolumeClaim template to</span> <code>Retain</code><span>.</span></p>
</div>
</div>
<div class="readable-text" data-hash="52731aee92582e431e14e50f4e529b3e" data-text-hash="c9f7c5b231b491cb3e51f79422da2c6d" id="247" refid="247">
<h3 id="sigil_toc_id_278">15.2.5&#160;<span>Using the OrderedReady Pod management policy</span></h3>
</div>
<div class="readable-text" data-hash="a9fe118513e6a332569877e7fa2d6d15" data-text-hash="a92dfdb7322d729f8623e3a98f7401a9" id="248" refid="248">
<p><span>Working with the</span> <code>quiz</code> <span>StatefulSet has been easy. However, you may recall that in the StatefulSet manifest, you set the</span> <code>podManagementPolicy</code> <span>field to</span> <code>Parallel</code><span>, which instructs the controller to create all Pods at the same time rather then one at a time. While MongoDB has no problem starting all replicas simultaneously, some stateful workloads do.</span></p>
</div>
<div class="readable-text" data-hash="1e9df3dc71d12eda8c70ad57ab5c5dc9" data-text-hash="d642b9cc2451bb7f534be0ebabef603e" id="249" refid="249">
<h4>Introducing the two Pod management policies</h4>
</div>
<div class="readable-text" data-hash="608df0ce51ce550d8257db3f9495a8a4" data-text-hash="c4f6fa1365ca67d8f966c98b8b6cba14" id="250" refid="250">
<p><span>When StatefulSets were introduced, the Pod management policy wasn&#8217;t configurable, and the controller always deployed the Pods sequentially. To maintain backward compatibility, this way of working had to be maintained when this field was introduced. Therefore, the default</span> <code>podManagementPolicy</code> <span>is</span> <code>OrderedReady</code><span>, but you can relax the StatefulSet ordering guarantees by changing the policy to</span> <code>Parallel</code><span>. The following figure shows how Pods are created and deleted over time with each policy.</span></p>
</div>
<div class="browsable-container figure-container" data-hash="4fb3808255da0db69f4b56d7e695a57f" data-text-hash="f776563009e18788f19bb566676a99ae" id="251" refid="251">
<h5><span>Figure 15.7 Comparison between the OrderedReady and Parallel Pod management policy</span></h5>
<img alt="" data-processed="true" height="484" id="Picture_16" loading="lazy" src="EPUB/images/15_img_0009.png" width="770">
</div>
<div class="readable-text" data-hash="893d8465d57bd4afeb217c5ca29fd49d" data-text-hash="76f619310a85ada677e5c5577969732b" id="252" refid="252">
<p><span>The following table explains the differences between the two policies in more detail.</span></p>
</div>
<div class="browsable-container" data-hash="50c5e3e19550de607e984a47d574df97" data-text-hash="4502b327503264bc166492717538b39b" id="253" refid="253">
<h5><span>Table 15.1 The supported podManagementPolicy values</span></h5>
<table border="1" cellpadding="0" cellspacing="0" width="100%">
<tbody>
<tr>
<td> <p><span>Value</span></p> </td>
<td> <p><span>Description</span></p> </td>
</tr>
<tr>
<td> <p></p><pre>OrderedReady
</pre> </td>
<td> <p><span>Pods are created one at a time in ascending order. After creating each Pod, the controller waits until the Pod is ready before creating the next Pod. The same process is used when scaling up and replacing Pods when they&#8217;re deleted or their nodes fail. When scaling down, the Pods are deleted in reverse order. The controller waits until each deleted Pod is finished before deleting the next one.</span></p> </td>
</tr>
<tr>
<td> <p></p><pre>Parallel
</pre> </td>
<td> <p><span>All Pods are created and deleted at the same time. The controller doesn&#8217;t wait for individual Pods to be ready.</span></p> </td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" data-hash="546059072b96d1acbe09b62fb785b2fd" data-text-hash="d3400b52a3a3c8a353d8ec1a9be9d353" id="254" refid="254">
<p><span>The</span> <code>OrderedReady</code> <span>policy is convenient when the workload requires that each replica be fully started before the next one is created and/or fully shut down before the next replica is asked to quit. However, this policy has its drawbacks. Let&#8217;s look at what happens when we use it in the</span> <code>quiz</code> <span>StatefulSet.</span></p>
</div>
<div class="readable-text" data-hash="a5eb15cb88197c8add3a415285e13139" data-text-hash="0f784d55919fda27767215709384ea15" id="255" refid="255">
<h4>Understanding the drawbacks of the OrderedReady Pod management policy</h4>
</div>
<div class="readable-text" data-hash="93f2bd59d3fdf48cd04d1c143a08a32c" data-text-hash="32aae1541efa20b41a3ad454242e9847" id="256" refid="256">
<p><span>Recreate the StatefulSet by applying the manifest file</span> <code>sts.quiz.orderedReady.yaml</code> <span>with the</span> <code>podManagementPolicy</code> <span>set to</span> <code>OrderedReady</code><span>, as shown in the following listing:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="e9a62b0e667d1fdb2e69e58379dc916f" data-text-hash="8d6f10c7760ae8c002643a213ac1b76b" id="257" refid="257">
<h5>Listing 15.5 Specifying the podManagementPolicy in the StatefulSet</h5>
<div class="code-area-container">
<pre class="code-area">apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: quiz
spec:
  podManagementPolicy: OrderedReady    #A
  minReadySeconds: 10    #B
  serviceName: quiz-pods
  replicas: 3
  ...</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIFBvZHMgb2YgdGhpcyBTdGF0ZWZ1bFNldCBhcmUgY3JlYXRlZCBpbiBvcmRlci4gRWFjaCBQb2QgbXVzdCBiZWNvbWUgcmVhZHkgYmVmb3JlIHRoZSBuZXh0IFBvZCBpcyBjcmVhdGVkLgojQiBUaGUgUG9kIG11c3QgYmUgcmVhZHkgZm9yIHRoaXMgbWFueSBzZWNvbmRzIGJlZm9yZSB0aGUgbmV4dCBQb2QgaXMgY3JlYXRlZC4="></div>
</div>
</div>
<div class="readable-text" data-hash="c2f120940ece8d54b2a29ca6ebdbdece" data-text-hash="15bd05c4ffa34e3019dab369a28a589d" id="258" refid="258">
<p><span>In addition to setting the</span> <code>podManagementPolicy</code><span>, the</span> <code>minReadySeconds</code> <span>field is also set to</span> <code>10</code> <span>so you can better see the effects of the</span> <code>OrderedReady</code> <span>policy. This field has the same role as in a Deployment, but is used not only for StatefulSet updates, but also when the StatefulSet is scaled.</span></p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="259" refid="259">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="1cc07d6876a25329d10e29da6cda93bd" data-text-hash="53194bca51cd248815c1fc767b495a2b" id="260" refid="260">
<p> <span>At the time of writing, the</span> <code>podManagementPolicy</code> <span>field is immutable. If you want to change the policy of an existing StatefulSet, you must delete and recreate it, like you just did. You can use the</span> <code>--cascade=orphan</code> <span>option to prevent Pods from being deleted during this operation.</span></p>
</div>
</div>
<div class="readable-text" data-hash="f2bdbcde92b69e8862884eb2770097c3" data-text-hash="a34d5ef1a5cdfc51fc6c107a5b4ca286" id="261" refid="261">
<p><span>Observe the</span> <code>quiz</code> <span>Pods with the</span> <code>--watch</code> <span>option to see how they&#8217;re created. Run the</span> <code>kubectl get</code> <span>command as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="7b2afe5be83c7caf3fd9f28f87c4d46e" data-text-hash="f63012f02fe7cdb372a14d7aa09bab09" id="262" refid="262">
<div class="code-area-container">
<pre class="code-area">$ kubectl get pods -l app=quiz --watch
NAME     READY   STATUS    RESTARTS   AGE
quiz-0   1/2     Running   0          22s</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="e20cf00519ddd499e83b6e0522f62af8" data-text-hash="c47fcae1214a35d234a82a68d6498e28" id="263" refid="263">
<p><span>As you may recall from the previous chapters, the</span> <code>--watch</code> <span>option tells</span> <code>kubectl</code> <span>to watch for changes to the specified objects. The command first lists the objects and then waits. When the state of an existing object is updated or a new object appears, the command prints the updated information about the object.</span></p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="264" refid="264">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="dde56a6a0fdc7fd7c870390a8e566a1a" data-text-hash="c2946424f1cfb8e4b39ab33a5ef7e510" id="265" refid="265">
<p> <span>When you run</span> <code>kubectl</code> <span>with the</span> <code>--watch</code> <span>option, it uses the same API mechanism that controllers use to wait for changes to the objects they&#8217;re observing.</span></p>
</div>
</div>
<div class="readable-text" data-hash="6cfd6ef270db3905d708e23b6b919f5d" data-text-hash="bb7cb95ce331e76d4c75a2e358209e6b" id="266" refid="266">
<p><span>You&#8217;ll be surprised to see that only a single replica is created when you recreate the StatefulSet with the</span> <code>OrderedReady</code> <span>policy, even though the StatefulSet is configured with three replicas. The next Pod,</span> <code>quiz-1</code><span>, doesn&#8217;t show up no matter how long you wait. The reason is that the</span> <code>quiz-api</code> <span>container in Pod</span> <code>quiz-0</code> <span>never becomes ready, as was the case when you scaled the StatefulSet to a single replica. Since the first Pod is never ready, the controller never creates the next Pod. It can&#8217;t do that because of the configured policy.</span></p>
</div>
<div class="readable-text" data-hash="a70e09512dab547fc41450cbd535aac6" data-text-hash="2290cc91e151b2410ed893ec931e348c" id="267" refid="267">
<p><span>As before, the</span> <code>quiz-api</code> <span>container isn&#8217;t ready because the MongoDB instance running alongside it doesn&#8217;t have quorum. Since the readiness probe defined in the</span> <code>quiz-api</code> <span>container depends on the availability of MongoDB, which needs at least two Pods for quorum, and since the StatefulSet controller doesn&#8217;t start the next Pod until the first one&#8217;s ready, the StatefulSet is now stuck in a deadlock.</span></p>
</div>
<div class="readable-text" data-hash="e1aaa87bf223a00ffb2ec46967e2a4d9" data-text-hash="feef0af0d849912915a3699ad8e01cb1" id="268" refid="268">
<p><span>One could argue that the readiness probe in the</span> <code>quiz-api</code> <span>container shouldn&#8217;t depend on MongoDB. This is debatable, but perhaps the problem lies in the use of the</span> <code>OrderedReady</code> <span>policy. Let&#8217;s stick with this policy anyway, since you&#8217;ve already seen how the</span> <code>Parallel</code> <span>policy behaves. Instead, let&#8217;s reconfigure the readiness probe to call the root URI rather than the</span> <code>/healthz/ready</code> <span>endpoint. This way, the probe only checks if the HTTP server is running in the</span> <code>quiz-api</code> <span>container, without connecting to MongoDB.</span></p>
</div>
<div class="readable-text" data-hash="a7e43add10bb994b3dc140b30e599886" data-text-hash="2a88cb17e7a62c3d6f77130bd1b89e2b" id="269" refid="269">
<h4>Updating a stuck StatefulSet with the OrderedReady policy</h4>
</div>
<div class="readable-text" data-hash="e34d699e2df7723a57324c5dc7374e10" data-text-hash="3936817c22fdd70d9ab7682193de7438" id="270" refid="270">
<p><span>Use the</span> <code>kubectl edit sts quiz</code> <span>command to change the path in the readiness probe definition, or use the</span> <code>kubectl apply</code> <span>command to apply the updated manifest file</span> <code>sts.quiz.orderedReady.readinessProbe.yaml</code><span>. The following listing shows how the readiness probe should be configured:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="94553b46eac92e94372be309388031ef" data-text-hash="b51a9993e2c51b4284912e179ffea295" id="271" refid="271">
<h5>Listing 15.6 Setting the readiness probe in the quiz-api container</h5>
<div class="code-area-container">
<pre class="code-area">apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: quiz
spec:
  ...
  template:
    ...
    spec:
      containers:
      - name: quiz-api
        ...
        readinessProbe:
          httpGet:
            port: 8080
            path: /    #A
            scheme: HTTP
      ...</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgQ2hhbmdlIHRoZSBwYXRoIGZyb20gL2hlYWx0aHovcmVhZHkgdG8gLw=="></div>
</div>
</div>
<div class="readable-text" data-hash="85060198e6850be01dc4bc664f7cff65" data-text-hash="fdf7d7eb6ded0258e00d07d9dcfb0a56" id="272" refid="272">
<p><span>After you update the Pod template in the StatefulSet, you expect the</span> <code>quiz-0</code> <span>Pod to be deleted and recreated with the new Pod template, right? List the Pods as follows to check if this happens.</span></p>
</div>
<div class="browsable-container listing-container" data-hash="370918a2bb4cb6a6c18a75352b3f7d0a" data-text-hash="45a0e946ebe09b6e3b2de90368b3b05e" id="273" refid="273">
<div class="code-area-container">
<pre class="code-area">$ kubectl get pods -l app=quiz
NAME     READY   STATUS    RESTARTS   AGE
quiz-0   1/2     Running   0          5m    #A</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIGFnZSBvZiB0aGUgUG9kIGluZGljYXRlcyB0aGF0IHRoaXMgaXMgc3RpbGwgdGhlIG9sZCBQb2Qu"></div>
</div>
</div>
<div class="readable-text" data-hash="47b35bf261162fa012088bed7e6f45ef" data-text-hash="f74010f28dce10aaa18d3bbdee3544ab" id="274" refid="274">
<p><span>As you can see from the age of the Pod, it&#8217;s still the same Pod. Why hasn&#8217;t the Pod been updated? When you update the Pod template in a ReplicaSet or Deployment, the Pods are deleted and recreated, so why not here?</span></p>
</div>
<div class="readable-text" data-hash="16d7d9a075e0505c08effa929a9021b7" data-text-hash="f80730f275bef6a2f3e2909470ef346c" id="275" refid="275">
<p><span>The reason for this is probably the biggest drawback of using StatefulSets with the default Pod management policy</span> <code>OrderedReady</code><span>. When you use this policy, the StatefulSet does nothing until the Pod is ready. If your StatefulSet gets into the same state as shown here, you&#8217;ll have to manually delete the unhealthy Pod.</span></p>
</div>
<div class="readable-text" data-hash="9e4e409759540837c586932cad2c9460" data-text-hash="01b0d1879fdee36950962ec0f46cbd83" id="276" refid="276">
<p><span>Now delete the</span> <code>quiz-0</code> <span>Pod and watch the StatefulSet controller create the three pods one by one as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="e62d4cc05ccfc013e8a1d348efaf16ea" data-text-hash="f8113ec946791199318bfd06d832ebd2" id="277" refid="277">
<div class="code-area-container">
<pre class="code-area">$ kubectl get pods -l app=quiz --watch
NAME     READY   STATUS              RESTARTS   AGE
quiz-0   0/2     Terminating         0          20m     #A
quiz-0   0/2     Pending             0          0s    #B
quiz-0   0/2     ContainerCreating   0          0s    #B
quiz-0   1/2     Running             0          3s    #B
quiz-0   2/2     Running             0          3s    #B
quiz-1   0/2     Pending             0          0s    #C
quiz-1   0/2     ContainerCreating   0          0s    #C
quiz-1   2/2     Running             0          3s    #C
quiz-2   0/2     Pending             0          0s    #D
quiz-2   0/2     ContainerCreating   0          1s    #D
quiz-2   2/2     Running             0          4s    #D</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgWW91IGRlbGV0ZWQgdGhlIFBvZCwgc28gaXTigJlzIGJlaW5nIHRlcm1pbmF0ZWQuCiNCIFRoaXMgaXMgYSBuZXcgUG9kIGluc3RhbmNlIHdpdGggdGhlIG5ldyByZWFkaW5lc3MgcHJvYmUgZGVmaW5pdGlvbi4gQm90aCBvZiBpdHMgY29udGFpbmVycyBzb29uIGJlY29tZSByZWFkeS4KI0MgVGhlIHNlY29uZCByZXBsaWNhIGlzIGNyZWF0ZWQgYW5kIHN0YXJ0ZWQgb25seSBhZnRlciB0aGUgZmlyc3Qgb25lIGJlY29tZXMgcmVhZHkuCiNEIFRoZSB0aGlyZCByZXBsaWNhIGlzIGNyZWF0ZWQgYWZ0ZXIgdGhlIHNlY29uZCByZXBsaWNhIGJlY29tZXMgcmVhZHku"></div>
</div>
</div>
<div class="readable-text" data-hash="a212728ed5fd0790c1d418e12b0a5230" data-text-hash="800427062d243e78c7820ccad5957295" id="278" refid="278">
<p><span>As you can see, the Pods are created in ascending order, one at a time. You can see that Pod</span> <code>quiz-1</code> <span>isn&#8217;t created until both containers in Pod</span> <code>quiz-0</code> <span>are ready. What you can&#8217;t see is that because of the</span> <code>minReadySeconds</code> <span>setting, the controller waits an additional 10 seconds before creating Pod</span> <code>quiz-1</code><span>. Similarly, Pod</span> <code>quiz-2</code> <span>is created 10 seconds after the containers in Pod</span> <code>quiz-1</code> <span>are ready. During the entire process, at most one Pod was being started. For some workloads, this is necessary to avoid race conditions.</span></p>
</div>
<div class="readable-text" data-hash="8e13bf4223126d061ed91af96fe2b00b" data-text-hash="a60ab67b52f49e86b869c20443629129" id="279" refid="279">
<h4>Scaling a StatefulSet with the OrderedReady policy</h4>
</div>
<div class="readable-text" data-hash="8cf748a435c9420591fe0dd61cb6dbac" data-text-hash="bb4744895a30a7a94f82db8e328ce8d7" id="280" refid="280">
<p><span>When you scale the StatefulSet configured with the OrderedReady Pod management policy, the Pods are created/deleted one by one. Scale the</span> <code>quiz</code> <span>StatefulSet to a single replica and watch as the Pods are removed. First, the Pod with the highest ordinal,</span> <code>quiz-2</code><span>, is marked for deletion, while Pod</span> <code>quiz-1</code> <span>remains untouched. When the termination of Pod</span> <code>quiz-2</code> <span>is complete, Pod</span> <code>quiz-1</code> <span>is deleted. The</span> <code>minReadySeconds</code> <span>setting isn&#8217;t used during scale-down, so there&#8217;s no additional delay.</span></p>
</div>
<div class="readable-text" data-hash="043de87a648982d163970a209d362aeb" data-text-hash="e96b74d350694c9aa65dc7faad50f001" id="281" refid="281">
<p><span>Just as with concurrent startup, some stateful workloads don&#8217;t like it when you remove multiple replicas at once. With the</span> <code>OrderedReady</code> <span>policy, you let each replica finish its shutdown procedure before the shutdown of the next replica is triggered.</span></p>
</div>
<div class="readable-text" data-hash="c5660b9a5a9002086988b0651c002b0d" data-text-hash="1a87c7b5812467cd0321033616f47b21" id="282" refid="282">
<h4>Blocked scale-downs</h4>
</div>
<div class="readable-text" data-hash="0bcf2a32423cb378d273604d6dfc48c4" data-text-hash="2507aed91f60a5abd183e7d7bdf2f053" id="283" refid="283">
<p><span>Another feature of the</span> <code>OrderedReady</code> <span>Pod management policy is that the controller blocks the scale-down operation if not all replicas are ready. To see this for yourself, create a new StatefulSet by applying the manifest file</span> <code>sts.demo-ordered.yaml</code><span>. This StatefulSet deploys three replicas using the</span> <code>OrderedReady</code> <span>policy. After the Pods are created, fail the readiness probe in the Pod</span> <code>demo-ordered-0</code> <span>by running the following command:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="05aa1593a0397ce70eb2a86cbb038308" data-text-hash="6f8c0acd5bf6850db5e5f76361418884" id="284" refid="284">
<div class="code-area-container">
<pre class="code-area">$ kubectl exec demo-ordered-0 -- rm /tmp/ready</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="4a14c5190fb49b53a961e6df8dab00d3" data-text-hash="c8718a561f2d0f7c04c2c7f72383c2f0" id="285" refid="285">
<p><span>Running this command removes the</span> <code>/tmp/ready</code> <span>file that the readiness probe checks for. The probe is successful if the file exists. After you run this command, the</span> <code>demo-ordered-0</code> <span>Pod is no longer ready. Now scale the StatefulSet to two replicas as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="89591c5dd175771e9e0e91be3c2a3057" data-text-hash="7aa5ab81c3b48d48a35f5145a310b04d" id="286" refid="286">
<div class="code-area-container">
<pre class="code-area">$ kubectl scale sts demo-ordered --replicas 2
statefulset.apps/demo-ordered scaled</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="1ae4423bc31003353ab13c96a1a2d3f0" data-text-hash="5da116791ddcf8c987f33beedd56ab36" id="287" refid="287">
<p><span>If you list the pods with the</span> <code>app=demo-ordered</code> <span>label selector, you&#8217;ll see that the StatefulSet controller does nothing. Unfortunately, the controller doesn&#8217;t generate any Events or update the status of the StatefulSet object to tell you why it didn&#8217;t perform the scale-down.</span></p>
</div>
<div class="readable-text" data-hash="3dc074aa31e85c0293a0c4b334931e19" data-text-hash="90ad79331bd4c7d3a4790a46cec698a4" id="288" refid="288">
<p><span>The controller completes the scale operation when the Pod is ready. You can make the readiness probe of the demo-ordered-0 Pod succeed by recreating the</span> <code>/tmp/ready</code> <span>file as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="ff2e7b3476559ceee63635a3115ee19a" data-text-hash="7eb1406d138c41be9b9a1051cc0d220a" id="289" refid="289">
<div class="code-area-container">
<pre class="code-area">$ kubectl exec demo-ordered-0 -- touch /tmp/ready</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="047ffb417e89b662610b10c1f0fabee4" data-text-hash="89b8e8f2de7b55a14cddf870d61dcc14" id="290" refid="290">
<p><span>I suggest you investigate the behavior of this StatefulSet further and compare it to the StatefulSet in the manifest file</span> <code>sts.demo-parallel.yaml</code><span>, which uses the</span> <code>Parallel</code> <span>Pod management policy. Use the</span> <code>rm</code> <span>and</span> <code>touch</code> <span>commands as shown to affect the outcome of the readiness probe in different replicas and see how it affects the two StatefulSets.</span></p>
</div>
<div class="readable-text" data-hash="dce245cd03c006375f5eecddfd969fa5" data-text-hash="d8040c95e778eb4fc88e809eb8900d5f" id="291" refid="291">
<h4>Ordered removal of Pods when deleting the StatefulSet</h4>
</div>
<div class="readable-text" data-hash="a02ceb280e6586600f3d8116a3ff484d" data-text-hash="7ad86713819e95663d497e396964a797" id="292" refid="292">
<p><span>The</span> <code>OrderedReady</code> <span>Pod management policy affects the initial rollout of StatefulSet Pods, their scaling, and how Pods are replaced when a node fails. However, the policy doesn&#8217;t apply when you delete the StatefulSet. If you want to terminate the Pods in order, you should first scale the StatefulSet to zero, wait until the last Pod finishes, and only then delete the StatefulSet.</span></p>
</div>
<div class="readable-text" data-hash="8b69aa5ae2d72643237d4d47ded2b085" data-text-hash="5b5c2c5c8c3071da394bcc2a0400c754" id="293" refid="293">
<h2 id="sigil_toc_id_279">15.3&#160;Updating a StatefulSet</h2>
</div>
<div class="readable-text" data-hash="bfdb871ee391f8c83e40eea16a6c26ed" data-text-hash="32899b6cedb8f6300b4e56e4626d8c18" id="294" refid="294">
<p><span>In addition to declarative scaling, StatefulSets also provide declarative updates, similar to Deployments. When you update the Pod template in a StatefulSet, the controller recreates the Pods with the updated template.</span></p>
</div>
<div class="readable-text" data-hash="904208d2c736de229396f0fe53173429" data-text-hash="c4fac52d69fb5bc3eaa65ee41ee4f911" id="295" refid="295">
<p><span>You may recall that the Deployment controller can perform the update in two ways, depending on the strategy specified in the Deployment object. You can also specify the update strategy in the</span> <code>updateStrategy</code> <span>field in the</span> <code>spec</code> <span>section of the StatefulSet manifest, but the available strategies are different from those in a Deployment, as you can see in the following table.</span></p>
</div>
<div class="browsable-container" data-hash="9e70060a9706103a8a6322b532c97e9c" data-text-hash="b1656978a716b06a2c5e2dd6d2cabc15" id="296" refid="296">
<h5><span>Table 15.2 The supported StatefulSet update strategies</span></h5>
<table border="1" cellpadding="0" cellspacing="0" width="100%">
<tbody>
<tr>
<td> <p><span>Value</span></p> </td>
<td> <p><span>Description</span></p> </td>
</tr>
<tr>
<td> <p></p><pre>RollingUpdate
</pre> </td>
<td> <p><span>In this update strategy, the Pods are replaced one by one. The Pod with the highest ordinal number is deleted first and replaced with a Pod created with the new template. When this new Pod is ready, the Pod with the next highest ordinal number is replaced. The process continues until all Pods have been replaced. This is the default strategy.</span></p> </td>
</tr>
<tr>
<td> <p></p><pre>OnDelete
</pre> </td>
<td> <p><span>The StatefulSet controller waits for each Pod to be manually deleted. When you delete the Pod, the controller replaces it with a Pod created with the new template. With this strategy, you can replace Pods in any order and at any rate.</span></p> </td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" data-hash="ba65bfb90d6c26be4bc29a7d32e197ce" data-text-hash="34283c6765777551e8afcde5f5092814" id="297" refid="297">
<p><span>The following figure shows how the Pods are updated over time for each update strategy.</span></p>
</div>
<div class="browsable-container figure-container" data-hash="58d399e06db8dd346f0e3aa5b190776c" data-text-hash="5910897e9c8ebb029cf7c19cc0b1f0d8" id="298" refid="298">
<h5><span>Figure 15.8 How the Pods are updated over time with different update strategies</span></h5>
<img alt="" data-processed="true" height="293" id="Picture_6" loading="lazy" src="EPUB/images/15_img_0010.png" width="943">
</div>
<div class="readable-text" data-hash="a8a87c6be4ba79c944ee01de549bf8d1" data-text-hash="899ac158353383593870bf4550ff191b" id="299" refid="299">
<p><span>The</span> <code>RollingUpdate</code> <span>strategy, which you can find in both Deployments and StatefulSets, is similar between the two objects, but differs in the parameters you can set. The</span> <code>OnDelete</code> <span>strategy lets you replace Pods at your own pace and in any order. It&#8217;s different from the</span> <code>Recreate</code> <span>strategy found in Deployments, which automatically deletes and replaces all Pods at once.</span></p>
</div>
<div class="readable-text" data-hash="7765423dbb3b5fafb29651fae3632e89" data-text-hash="218fc1cd280f2564a2b053e6ce73c963" id="300" refid="300">
<h3 id="sigil_toc_id_280">15.3.1&#160;<span>Using the</span> <span>RollingUpdate strategy</span></h3>
</div>
<div class="readable-text" data-hash="f41bd78b4194a3ea48886d3f850252db" data-text-hash="94fb79bfe2e320f8da5681484c05510d" id="301" refid="301">
<p><span>The RollingUpdate strategy in a StatefulSet behaves similarly to the RollingUpdate strategy in Deployments, but only one Pod is replaced at a time. You may recall that you can configure the Deployment to replace multiple Pods at once using the</span> <code>maxSurge</code> <span>and</span> <code>maxUnavailable</code> <span>parameters. The rolling update strategy in StatefulSets has no such parameters.</span></p>
</div>
<div class="readable-text" data-hash="e70a29f51daec2fc5062a314b802636f" data-text-hash="a13f9beeba4e1d1225d84c1420260d6a" id="302" refid="302">
<p><span>You may also recall that you can slow down the rollout in a Deployment by setting the</span> <code>minReadySeconds</code> <span>field, which causes the controller to wait a certain amount of time after the new Pods are ready before replacing the other Pods. You&#8217;ve already learned that StatefulSets also provide this field and that it affects the scaling of StatefulSets in addition to the updates.</span></p>
</div>
<div class="readable-text" data-hash="db8af0c2d8f59aaa33bb05d814781c62" data-text-hash="5653b3488d94727885f1cecd00ccc4a7" id="303" refid="303">
<p><span>Let&#8217;s update the</span> <code>quiz-api</code> <span>container in the</span> <code>quiz</code> <span>StatefulSet to version</span> <code>0.2</code><span>. Since</span> <code>RollingUpdate</code> <span>is the default update strategy type, you can omit the</span> <code>updateStrategy</code> <span>field in the manifest. To trigger the update, use</span> <code>kubectl edit</code> <span>to change the value of the</span> <code>ver</code> <span>label and the image tag in the</span> <code>quiz-api</code> <span>container to</span> <code>0.2</code><span>. You can also apply the manifest file</span> <code>sts.quiz.0.2.yaml</code> <span>with</span> <code>kubectl apply</code> <span>instead.</span></p>
</div>
<div class="readable-text" data-hash="fd055ea3378e9498e1d394b7fd9f221d" data-text-hash="1b12d8aeb0b997e2149458c7e0fdca32" id="304" refid="304">
<p><span>You can track the rollout with the</span> <code>kubectl rollout status</code> <span>command as in the previous chapter. The full command and its output are as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="2965d973c5b40cef43172b6c46d5a073" data-text-hash="2291ce3585c2c999dfc3625b9d4d329f" id="305" refid="305">
<div class="code-area-container">
<pre class="code-area">$ kubectl rollout status sts quiz
Waiting for partitioned roll out to finish: 0 out of 3 new pods have been updated...
Waiting for 1 pods to be ready...
Waiting for partitioned roll out to finish: 1 out of 3 new pods have been updated...
Waiting for 1 pods to be ready...
...</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="73b916a27268ea6ffd8918315181562a" data-text-hash="804a613151b8c06eb059d1b2b2fa09aa" id="306" refid="306">
<p><span>Because the Pods are replaced one at a time and the controller waits until each replica is ready before moving on to the next, the</span> <code>quiz</code> <span>Service remains accessible throughout the process. If you list the Pods as they&#8217;re updated, you&#8217;ll see that the Pod with the highest ordinal number,</span> <code>quiz-2</code><span>, is updated first, followed by</span> <code>quiz-1</code><span>, as shown here:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="8b93228d3b79ab251ec1bc78c95922f2" data-text-hash="91eff036aa1964df1f3fd9ced49bca43" id="307" refid="307">
<div class="code-area-container">
<pre class="code-area">$ kubectl get pods -l app=quiz -L controller-revision-hash,ver
NAME     READY   STATUS        RESTARTS   AGE   CONTROLLER-REVISION-HASH   VER
quiz-0   2/2     Running       0          50m   quiz-6c48bdd8df            0.1    #A
quiz-1   2/2     Terminating   0          10m   quiz-6c48bdd8df            0.1    #B
quiz-2   2/2     Running       0          20s   quiz-6945968d9             0.2    #C</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIHBvZCB3aXRoIHRoZSBsb3dlc3Qgb3JkaW5hbCBoYXMgeWV0IHRvIGJlIHVwZGF0ZWQuCiNCIFRoaXMgaXMgdGhlIHBvZCB0aGF04oCZcyBiZWluZyB1cGRhdGVkIG5vdy4KI0MgVGhlIHBvZCB3aXRoIHRoZSBoaWdoZXN0IG9yZGluYWwgd2FzIHVwZGF0ZWQgZmlyc3Qu"></div>
</div>
</div>
<div class="readable-text" data-hash="5fa9f5bea25c09bd077c16d5e8e6ad97" data-text-hash="abb26784cc77f86295b5db0060ea32af" id="308" refid="308">
<p><span>The update process is complete when the Pod with the lowest ordinal number,</span> <code>quiz-0</code><span>, is updated. At this point, the</span> <code>kubectl rollout status</code> <span>command reports the following status:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="1548fa04a480e63a2ed7c7fb433e5861" data-text-hash="3b72209cb114a37e00d7333070053f79" id="309" refid="309">
<div class="code-area-container">
<pre class="code-area">$ kubectl rollout status sts quiz
partitioned roll out complete: 3 new pods have been updated...</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="b48ae83f3aecc1d0b9f33ed8609ad521" data-text-hash="b574fc2330f36e1e470b76a071a21576" id="310" refid="310">
<h4>Updates with Pods that aren&#8217;t ready</h4>
</div>
<div class="readable-text" data-hash="2159d1e2728c900f8118cfa2f7293566" data-text-hash="0bf18cf6107c7536856968c2ca6cad22" id="311" refid="311">
<p><span>If the StatefulSet is configured with the</span> <code>RollingUpdate</code> <span>strategy and you trigger the update when not all Pods are ready, the rollout is held back. The</span> <code>kubectl rollout status</code> <span>indicates that the controller is waiting for one or more Pods to be ready.</span></p>
</div>
<div class="readable-text" data-hash="715a9e8e7deae1669dec01af5477d6b9" data-text-hash="4b9857ef7a7ab931be82d756c9952940" id="312" refid="312">
<p><span>If a new Pod fails to become ready during the update, the update is also paused, just like a Deployment update. The rollout will resume when the Pod is ready again. So, if you deploy a faulty version whose readiness probe never succeeds, the update will be blocked after the first Pod is replaced. If the number of replicas in the StatefulSet is sufficient, the service provided by the Pods in the StatefulSet is unaffected.</span></p>
</div>
<div class="readable-text" data-hash="46c70790d22a9f3a0819833f409e1997" data-text-hash="3b1cac577460099c6fec7f954d874b0b" id="313" refid="313">
<h4>Displaying the revision history</h4>
</div>
<div class="readable-text" data-hash="59322bb15fdc39bcec266905bae2e793" data-text-hash="077c150961e99fadf9b2d3a1f06a7aa1" id="314" refid="314">
<p><span>You may recall that Deployments keep a history of recent revisions. Each revision is represented by the ReplicaSet that the Deployment controller created when that revision was active. StatefulSets also keep a revision history. You can use the</span> <code>kubectl rollout history</code> <span>command to display it as follows.</span></p>
</div>
<div class="browsable-container listing-container" data-hash="b002b92558896308a721aa12c9ca6e98" data-text-hash="a13bb447cebf742a86f127f1d3c13e92" id="315" refid="315">
<div class="code-area-container">
<pre class="code-area">$ kubectl rollout history sts quiz
statefulset.apps/quiz
REVISION  CHANGE-CAUSE
1         &lt;none&gt;
2         &lt;none&gt;</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="6108dd834d92dfab3ee0605c365fea8b" data-text-hash="b4773e87d926b222c13b4852543e4789" id="316" refid="316">
<p><span>You may wonder where this history is stored, because unlike Deployments, a StatefulSet manages Pods directly. And if you look at the object manifest of the</span> <code>quiz</code> <span>StatefulSet, you&#8217;ll notice that it only contains the current Pod template and no previous revisions. So where is the revision history of the StatefulSet stored?</span></p>
</div>
<div class="readable-text" data-hash="f5a871ea05a9738ed684ae6f43612585" data-text-hash="d27e2e20e33adf67aeaa6366d6e58f97" id="317" refid="317">
<p><span>The revision history of StatefulSets and DaemonSets, which you&#8217;ll learn about in the next chapter, is stored in ControllerRevision objects. A ControllerRevision is a generic object that represents an immutable snapshot of the state of an object at a particular point in time. You can list ControllerRevision objects as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="c985bf20162be3825ed1c7b7204e206f" data-text-hash="758a5a013bae8d2858a6295f6d862d79" id="318" refid="318">
<div class="code-area-container">
<pre class="code-area">$ kubectl get controllerrevisions
NAME              CONTROLLER              REVISION   AGE
quiz-6945968d9    statefulset.apps/quiz   2          1m
quiz-6c48bdd8df   statefulset.apps/quiz   1          50m</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="7c0e48b3b396a644dcb905c1fa5791dd" data-text-hash="f158fe0685ba45d218d8abcd1b82a3ca" id="319" refid="319">
<p><span>Since these objects are used internally, you don&#8217;t need to know anything more about them. However, if you want to learn more, you can use the</span> <code>kubectl explain</code> <span>command.</span></p>
</div>
<div class="readable-text" data-hash="a403be841a599bf2680030ca1438d180" data-text-hash="a6145286469c43b7d39ec4df26ea0d8e" id="320" refid="320">
<h4>Rolling back to a previous revision</h4>
</div>
<div class="readable-text" data-hash="caa811b35d77ee2e5e04c3971eb00a18" data-text-hash="c69616e49756f64e327a7f7d66e35c7b" id="321" refid="321">
<p><span>If you&#8217;re updating the StatefulSet and the rollout hangs, or if the rollout was successful, but you want to revert to the previous revision, you can use the</span> <code>kubectl rollout undo</code> <span>command, as described in the previous chapter. You&#8217;ll update the</span> <code>quiz</code> <span>StatefulSet again in the next section, so please reset it to the previous version as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="e707861636bf50e1e2d404546a0bf929" data-text-hash="6c8cb7a211618a45d7bbcb12500e02a1" id="322" refid="322">
<div class="code-area-container">
<pre class="code-area">$ kubectl rollout undo sts quiz
statefulset.apps/quiz rolled back</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="7451a2564b28186cd7f31d55349906f4" data-text-hash="c7ffce3cfed7fc2760a06e4df9415944" id="323" refid="323">
<p><span>You can also use the</span> <code>--to-revision</code> <span>option to return to a specific revision. As with Deployments, Pods are rolled back using the update strategy configured in the StatefulSet. If the strategy is</span> <code>RollingUpdate</code><span>, the Pods are reverted one at a time.</span></p>
</div>
<div class="readable-text" data-hash="d2eb59602c24cabc661763f085a91f49" data-text-hash="f5c4e599e6b880bc6b3d7d485a6bde67" id="324" refid="324">
<h3 id="sigil_toc_id_281">15.3.2&#160;RollingUpdate with partition</h3>
</div>
<div class="readable-text" data-hash="f99f09b169d28226dacd7e8d5a9f758c" data-text-hash="6d1eca4431d1cec71255f53386fb42eb" id="325" refid="325">
<p><span>StatefulSets don&#8217;t have a</span> <code>pause</code> <span>field that you can use to prevent a Deployment rollout from being triggered, or to pause it halfway. If you try to pause the StatefulSet with the</span> <code>kubectl rollout pause</code> <span>command, you receive the following error message:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="e68aa6a1edb93f87e893723a5e5cbe3e" data-text-hash="c8cf96d7252b779ab7a2b1b0975fc771" id="326" refid="326">
<div class="code-area-container">
<pre class="code-area">$ kubectl rollout pause sts quiz
error: statefulsets.apps "quiz" pausing is not supported</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="b5f2b877e010cc5608c8abcac94dfea8" data-text-hash="e44eb93d6153535a857d7682562faa35" id="327" refid="327">
<p><span>In a StatefulSet you can achieve the same result and more with the</span> <code>partition</code> <span>parameter of the</span> <code>RollingUpdate</code> <span>strategy. The value of this field specifies the ordinal number at which the StatefulSet should be partitioned. As shown in the following figure, pods with an ordinal number lower than the</span> <code>partition</code> <span>value aren&#8217;t updated.</span></p>
</div>
<div class="browsable-container figure-container" data-hash="b9211546d2f83d1d6fb8a50536c62ea2" data-text-hash="15b6d844ddd13af24889259d449a53eb" id="328" refid="328">
<h5><span>Figure 15.9 Partitioning a rolling update</span></h5>
<img alt="" data-processed="true" height="205" id="Picture_18" loading="lazy" src="EPUB/images/15_img_0011.png" width="795">
</div>
<div class="readable-text" data-hash="c5c94f9ed8a87a14b8303388d54e9a7a" data-text-hash="ca558fd5fc46ceb5ff1f5903b0474764" id="329" refid="329">
<p><span>If you set the</span> <code>partition</code> <span>value appropriately, you can implement a Canary deployment, control the rollout manually, or stage an update instead of triggering it immediately.</span></p>
</div>
<div class="readable-text" data-hash="43cc33c5ff553a819270d6d27800fcf7" data-text-hash="8c0131365ceb9f5a650ae7ac58177a03" id="330" refid="330">
<h4>Staging an update</h4>
</div>
<div class="readable-text" data-hash="0a2557bd726e21b21447f7e2cc2770ba" data-text-hash="57d46138cd4a21479712007c1e613647" id="331" refid="331">
<p><span>To stage a StatefulSet update without actually triggering it, set the partition value to the number of replicas or higher, as in the manifest file</span> <code>sts.quiz.0.2.partition.yaml</code> <span>shown in the following listing.</span></p>
</div>
<div class="browsable-container listing-container" data-hash="a0933c5a130949f67abdf714aafdd6f0" data-text-hash="3a33031a149a940d9deb44fa87cf4bb8" id="332" refid="332">
<h5>Listing 15.7 Staging a StatefulSet update with the partition field</h5>
<div class="code-area-container">
<pre class="code-area">apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: quiz
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 3    #A
  replicas: 3    #A
  ...</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhlIHBhcnRpdGlvbiB2YWx1ZSBpcyBlcXVhbCB0byB0aGUgbnVtYmVyIG9mIHJlcGxpY2FzLg=="></div>
</div>
</div>
<div class="readable-text" data-hash="aae804bfa22c259bb38e3052777bda22" data-text-hash="9bbc2bce55a389e77d8c7c03be89b3d5" id="333" refid="333">
<p><span>Apply this manifest file and confirm that the rollout doesn&#8217;t start even though the Pod template has been updated. If you set the</span> <code>partition</code> <span>value this way, you can make several changes to the StatefulSet without triggering the rollout. Now let&#8217;s look at how you can trigger the update of a single Pod.</span></p>
</div>
<div class="readable-text" data-hash="25576b3994ad42190a084ed94199dc34" data-text-hash="0ffbfc49ee2333d363251bf8a64a63cb" id="334" refid="334">
<h4>Deploying a canary</h4>
</div>
<div class="readable-text" data-hash="571ad3895985d78991b6d73fda4ef486" data-text-hash="7159d4318d18f7a6b7d355be7914bc6e" id="335" refid="335">
<p><span>To deploy a canary, set the</span> <code>partition</code> <span>value to the number of replicas minus one. Since the</span> <code>quiz</code> <span>StatefulSet has three replicas, you set the</span> <code>partition</code> <span>to</span> <code>2</code><span>. You can do this with the</span> <code>kubectl patch</code> <span>command as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="a1c9cc80123fa2bb408db483f35d4105" data-text-hash="3aa06260a2a1dbebebb04f424be2c0b0" id="336" refid="336">
<div class="code-area-container">
<pre class="code-area">$ kubectl patch sts quiz -p '{"spec": {"updateStrategy": {"rollingUpdate": {"partition": 2 }}}}'
statefulset.apps/quiz patched</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="f5064408aeb0c1bfca27d17d502e21c0" data-text-hash="bc4751ea540b371c2aa785a0e3cc6634" id="337" refid="337">
<p><span>If you now look at the list of</span> <code>quiz</code> <span>Pods, you&#8217;ll see that only the Pod</span> <code>quiz-2</code> <span>has been updated to version</span> <code>0.2</code> <span>because only its ordinal number is greater than or equal to the</span> <code>partition</code> <span>value.</span></p>
</div>
<div class="browsable-container listing-container" data-hash="fc11532c5d6d258c519afcb3879ff504" data-text-hash="c2b6ba48d450e8bc70bb244d6383482a" id="338" refid="338">
<div class="code-area-container">
<pre class="code-area">$ kubectl get pods -l app=quiz -L controller-revision-hash,ver
NAME     READY   STATUS    RESTARTS   AGE   CONTROLLER-REVISION-HASH   VER
quiz-0   2/2     Running   0          8m    quiz-6c48bdd8df            0.1
quiz-1   2/2     Running   0          8m    quiz-6c48bdd8df            0.1
quiz-2   2/2     Running   0          20s   quiz-6945968d9             0.2    #A</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgT25seSB0aGUgcXVpei0yIFBvZCB3YXMgdXBkYXRlZCBiZWNhdXNlIG9ubHkgaXRzIG9yZGluYWwgaXMgZ3JlYXRlciB0aGFuIG9yIGVxdWFsIHRvIHRoZSBwYXJ0aXRpb24gdmFsdWUu"></div>
</div>
</div>
<div class="readable-text" data-hash="fc975c2d116733a6e0b68f0448a04dce" data-text-hash="7e3f7b4f3b7b290da40bc32116f5e3d5" id="339" refid="339">
<p><span>The Pod</span> <code>quiz-2</code> <span>is the canary that you use to check if the new version behaves as expected before rolling out the changes to the remaining Pods.</span></p>
</div>
<div class="readable-text" data-hash="f5ce26fc352513fdafbd4cf565f23907" data-text-hash="c7a716fbb4caf5e7a2618a65d36abd6a" id="340" refid="340">
<p><span>At this point I&#8217;d like to draw your attention to the</span> <code>status</code> <span>section of the StatefulSet object. It contains information about the total number of replicas, the number of replicas that are ready and available, the number of current and updated replicas, and their revision hashes. To display the status, run the following command:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="7ab99c94c75f01bd96979c5a95dcea52" data-text-hash="1d4d834b7b08f0a974c4f2d93753d2e6" id="341" refid="341">
<div class="code-area-container">
<pre class="code-area">$ kubectl get sts quiz -o yaml
...
status:
  availableReplicas: 3    #A
  collisionCount: 0
  currentReplicas: 2    #B
  currentRevision: quiz-6c48bdd8df    #B
  observedGeneration: 8
  readyReplicas: 3    #A
  replicas: 3    #A
  updateRevision: quiz-6945968d9    #C
  updatedReplicas: 1    #C</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhyZWUgcmVwbGljYXMgZXhpc3QgYW5kIGFsbCB0aHJlZSBhcmUgcmVhZHkgYW5kIGF2YWlsYWJsZS4KI0IgVHdvIHJlcGxpY2FzIGJlbG9uZyB0byB0aGUgY3VycmVudCByZXZpc2lvbi4KI0MgT25lIHJlcGxpY2EgaGFzIGJlZW4gdXBkYXRlZC4="></div>
</div>
</div>
<div class="readable-text" data-hash="46776c15c4dce9f21348747a9e3fd63a" data-text-hash="c5153334fa30cd404b7f79e71ff004cf" id="342" refid="342">
<p><span>As you can see from the</span> <code>status</code><span>, the StatefulSet is now split into two partitions. If a Pod is deleted at this time, the StatefulSet controller will create it with the correct template. For example, if you delete one of the Pods with version 0.1, the replacement Pod will be created with the previous template and will run again with version 0.1. If you delete the Pod that&#8217;s already been updated, it&#8217;ll be recreated with the new template. Feel free to try this out for yourself. You can&#8217;t break anything.</span></p>
</div>
<div class="readable-text" data-hash="376d06bce9d94feaca49b19ebbe45927" data-text-hash="04e9b2162cc4da88f8ac600f38958359" id="343" refid="343">
<h4>Completing a partitioned update</h4>
</div>
<div class="readable-text" data-hash="fc4bb515472cd67f68851d9a7ef6de7f" data-text-hash="0a041d388e3fee3b447e4f623b5af41a" id="344" refid="344">
<p><span>When you&#8217;re confident the canary is fine, you can let the StatefulSet update the remaining pods by setting the</span> <code>partition</code> <span>value to zero as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="c1beded4a0d61ab98370c28484fa344e" data-text-hash="084511a874754b6b852dd610992aed5a" id="345" refid="345">
<div class="code-area-container">
<pre class="code-area">$ kubectl patch sts quiz -p '{"spec": {"updateStrategy": {"rollingUpdate": {"partition": 0 }}}}'
statefulset.apps/quiz patched</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="0809ea837973370e2ac1e7f7cf0f5617" data-text-hash="0018a00054788fe4270cb0cd3347b78a" id="346" refid="346">
<p><span>When the</span> <code>partition</code> <span>field is set to zero, the StatefulSet updates all Pods. First, the pod</span> <code>quiz-1</code> <span>is updated, followed by</span> <code>quiz-0</code><span>. If you had more Pods, you could also use the</span> <code>partition</code> <span>field to update the StatefulSet in phases. In each phase, you decide how many Pods you want to update and set the</span> <code>partition</code> <span>value accordingly.</span></p>
</div>
<div class="readable-text" data-hash="b7bdc063b754e7d9e29adb563dda49f7" data-text-hash="89dbfee08feee8e0cdc52e15fe9f6860" id="347" refid="347">
<p><span>At the time of writing,</span> <code>partition</code> <span>is the only parameter of the RollingUpdate strategy. You&#8217;ve seen how you can use it to control the rollout. If you want even more control, you can use the</span> <code>OnDelete</code> <span>strategy, which I&#8217;ll try next. Before you continue, please reset the StatefulSet to the previous revision as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="e707861636bf50e1e2d404546a0bf929" data-text-hash="6c8cb7a211618a45d7bbcb12500e02a1" id="348" refid="348">
<div class="code-area-container">
<pre class="code-area">$ kubectl rollout undo sts quiz
statefulset.apps/quiz rolled back</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="e646761387d4b8c3261ff5fe37523b0a" data-text-hash="d30115670764eb70bb93eb15b461e4a7" id="349" refid="349">
<h3 id="sigil_toc_id_282">15.3.3&#160;OnDelete strategy</h3>
</div>
<div class="readable-text" data-hash="16656f94b76f5926ffa6af6ddcdb03d8" data-text-hash="a4b5115f5c6a2cbc0d1599f162e58b3b" id="350" refid="350">
<p><span>If you want to have full control over the rollout process, you can use the</span> <code>OnDelete</code> <span>update strategy. To configure the StatefulSet with this strategy, use</span> <code>kubectl apply</code> <span>to apply the manifest file</span> <code>sts.quiz.0.2.onDelete.yaml</code><span>. The following listing shows how the update strategy is set.</span></p>
</div>
<div class="browsable-container listing-container" data-hash="35e0051e8598a8b2cb7ae26f8c74b51f" data-text-hash="53e2908b7f6839b7d077e4f2d7185d1a" id="351" refid="351">
<h5>Listing 15.8 Setting the OnDelete update strategy</h5>
<div class="code-area-container">
<pre class="code-area">apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: quiz
spec:
  updateStrategy:   #A
    type: OnDelete    #A
  ...</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVG8gZW5hYmxlIHRoZSBPbkRlbGV0ZSBzdHJhdGVneSwgc2V0IHRoZSB0eXBlIGZpZWxkIGxpa2Ugc28uIFRoaXMgc3RyYXRlZ3kgaGFzIG5vIHBhcmFtZXRlcnMu"></div>
</div>
</div>
<div class="readable-text" data-hash="c76bfafd80ff21eeaf3d55c6742ab3e9" data-text-hash="e4271b4adcafe8a2a0422fba5d7bca22" id="352" refid="352">
<p><span>This manifest updates the</span> <code>quiz-api</code> <span>container in the Pod template to use the</span> <code>:0.2</code> <span>image tag. However, because it sets the update strategy to</span> <code>OnDelete</code><span>, nothing happens when you apply the manifest.</span></p>
</div>
<div class="readable-text" data-hash="701307971935d4d8d2385924d18c86e9" data-text-hash="de77e7ba593fb9e4dbccc5bcba49faa4" id="353" refid="353">
<p><span>If you use the</span> <code>OnDelete</code> <span>strategy, the rollout is semi-automatic. You manually delete each Pod, and the StatefulSet controller then creates the replacement Pod with the new template. With this strategy, you can decide which Pod to update and when. You don&#8217;t necessarily have to delete the Pod with the highest ordinal number first. Try deleting the Pod</span> <code>quiz-0</code><span>. When its containers exit, a new</span> <code>quiz-0</code> <span>Pod with version</span> <code>0.2</code> <span>appears:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="4463819c6eafcdb1743cdb660c1150bd" data-text-hash="4a6ebe23c6c4ce20e8740c68b1494e34" id="354" refid="354">
<div class="code-area-container">
<pre class="code-area">$ kubectl get pods -l app=quiz -L controller-revision-hash,ver
NAME     READY   STATUS    RESTARTS   AGE   CONTROLLER-REVISION-HASH   VER
quiz-0   2/2     Running   0          53s   quiz-6945968d9             0.2    #A
quiz-1   2/2     Running   0          11m   quiz-6c48bdd8df            0.1
quiz-2   2/2     Running   0          12m   quiz-6c48bdd8df            0.1</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgWW91IGRlbGV0ZWQgdGhpcyBQb2QsIGFuZCB0aGUgY29udHJvbGxlciByZXBsYWNlZCBpdCB3aXRoIGEgUG9kIGZyb20gdGhlIHVwZGF0ZWQgdGVtcGxhdGUu"></div>
</div>
</div>
<div class="readable-text" data-hash="587508ea9624fbd8acb9e10c69355dac" data-text-hash="4fb58a3a0b834e9ebe574d52cd866a9e" id="355" refid="355">
<p><span>To complete the rollout, you need to delete the remaining Pods. You can do this in the order that the workloads require, or in the order that you want.</span></p>
</div>
<div class="readable-text" data-hash="45e978c57b0a2b9020268adc30519e40" data-text-hash="0433a67de577c14300c18526cf91b046" id="356" refid="356">
<h4>Rolling back with the OnDelete strategy</h4>
</div>
<div class="readable-text" data-hash="81cfa6881389febd0ef828661d1c1978" data-text-hash="a9519120fd32c66af8582dfe178baa33" id="357" refid="357">
<p><span>Since the update strategy also applies when you use the</span> <code>kubectl rollout undo</code> <span>command, the rollback process is also semi-automatic. You must delete each Pod yourself if you want to roll it back to the previous revision.</span></p>
</div>
<div class="readable-text" data-hash="b48ae83f3aecc1d0b9f33ed8609ad521" data-text-hash="b574fc2330f36e1e470b76a071a21576" id="358" refid="358">
<h4>Updates with Pods that aren&#8217;t ready</h4>
</div>
<div class="readable-text" data-hash="b0399f0426317b0bcfd248b43a4dc575" data-text-hash="af72dbfff2eb1a8766616bd4e62fcb07" id="359" refid="359">
<p><span>Since you control the rollout and the controller replaces any Pod you delete, the Pod&#8217;s readiness status is irrelevant. If you delete a Pod that&#8217;s not ready, the controller updates it.</span></p>
</div>
<div class="readable-text" data-hash="ca8b42f859fe6b47adb486940541ed3f" data-text-hash="7a05c4996a81353fab3e6b36d487e2b3" id="360" refid="360">
<p><span>If you delete a Pod and the new Pod isn&#8217;t ready, but you still delete the next Pod, the controller will update that second Pod as well. It&#8217;s your responsibility to consider Pod readiness.</span></p>
</div>
<div class="readable-text" data-hash="873608cc951ca0d112ce4e2c9384a108" data-text-hash="98d2b9169831680e95c7b1428b7d3331" id="361" refid="361">
<h2 id="sigil_toc_id_283">15.4&#160;Managing stateful applications with Kubernetes Operators</h2>
</div>
<div class="readable-text" data-hash="eb2ac0216af7e89330a3d4f70ca52a17" data-text-hash="544096b92cab86d08d1aabbeb3a81ec5" id="362" refid="362">
<p><span>In this chapter, you saw that managing a stateful application can involve more than what Kubernetes provides with the StatefulSet object. In the case of MongoDB, you need to reconfigure the MongoDB replica set every time you scale the StatefulSet. If you don&#8217;t, the replica set may lose quorum and stop working. Also, if a cluster node fails, manual intervention is required to move the Pods to the remaining nodes.</span></p>
</div>
<div class="readable-text" data-hash="817166521139a8a0e75a8522a37d7cb3" data-text-hash="03858d4ef08a9b538d89c664ef5fcfb9" id="363" refid="363">
<p><span>Managing stateful applications is difficult. StatefulSets do a good job of automating some basic tasks, but much of the work still has to be done manually. If you want to deploy a fully automated stateful application, you need more than what StatefulSets can provide. This is where Kubernetes <i>operators</i> come into play. I&#8217;m not referring to the people running Kubernetes clusters, but the software that does it for them.</span></p>
</div>
<div class="readable-text" data-hash="34653d6808923096733346581388d025" data-text-hash="f29298d3e9a1ab33c29139805cfa7a6c" id="364" refid="364">
<p><span>A <i>Kubernetes operator</i> is an application-specific controller that automates the deployment and management of an application running on Kubernetes. An operator is typically developed by the same organization that builds the application, as they know best how to manage it. Kubernetes doesn&#8217;t ship with operators. Instead, you must install them separately.</span></p>
</div>
<div class="readable-text" data-hash="05d212cefa240ad222eec1aa18f467cb" data-text-hash="d6b8b3372fd07f7df1182f05aa794fae" id="365" refid="365">
<p><span>Each operator extends the Kubernetes API with its own set of custom object types that you use to deploy and configure the application. You create an instance of this custom object type using the Kubernetes API and leave it to the operator to create the Deployments or StatefulSets that create the Pods in which the application runs, as shown in the following figure.</span></p>
</div>
<div class="browsable-container figure-container" data-hash="56abb2fe1b6ff56cf914780078da3488" data-text-hash="1ff2846ad921c6281606f84759370373" id="366" refid="366">
<h5><span>Figure 15.10 Managing an application through custom resources and operators</span></h5>
<img alt="" data-processed="true" height="347" id="Picture_17" loading="lazy" src="EPUB/images/15_img_0012.png" width="759">
</div>
<div class="readable-text" data-hash="e035bfa4bb8de4916504ba0af2f8b1a8" data-text-hash="a494bd5682f680cf82667159177862c4" id="367" refid="367">
<p><span>In this section, you&#8217;ll learn how to use the MongoDB Community Operator to deploy MongoDB. Since I don&#8217;t know how the operator will change after the book is published, I won&#8217;t go into too much detail, but I&#8217;ll list all the steps that were necessary to install the Operator and deploy MongoDB at the time I wrote the book so you can get a feel for what&#8217;s required even if you don&#8217;t try it yourself.</span></p>
</div>
<div class="readable-text" data-hash="968a2a80291dab94908b70e3670ca4ea" data-text-hash="7706142ac086a358d0c42e6dbe51e388" id="368" refid="368">
<p><span>If you do want to try this yourself, please follow the documentation in the GitHub repository of the MongoDB community operator at <a href="mongodb.html">https://github.com/mongodb/mongodb-kubernetes-operator</a>.</span></p>
</div>
<div class="readable-text" data-hash="542bad37cc56821d1dbb82ee9fff9d2f" data-text-hash="568ebdb182a90a652185a3becf893855" id="369" refid="369">
<h3 id="sigil_toc_id_284">15.4.1&#160;<span>Deploying the MongoDB community operator</span></h3>
</div>
<div class="readable-text" data-hash="e95d299ebd9324e9fceb3a7bdf6cd47c" data-text-hash="5e5310465e6d50e677da8a2351a2f561" id="370" refid="370">
<p><span>An operator is itself an application that you typically deploy in the same Kubernetes cluster as the application that the operator is to manage. At the time of writing, the MongoDB operator documentation instructs you to first clone the GitHub repository as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="b0fa2c4c045d39931cd08da5df886cbf" data-text-hash="a46f0f1b1d76bee4260a244eff998fbf" id="371" refid="371">
<div class="code-area-container">
<pre class="code-area">$ git clone https://github.com/mongodb/mongodb-kubernetes-operator.git</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="9efa5d1e207a3d992308f440fc8131f2" data-text-hash="c73cd72d998ea24efc6aac8244456955" id="372" refid="372">
<p><span>Then you go to the</span> <code>mongodb-kubernetes-operator</code> <span>directory, where you find the source code of the operator and some Kubernetes object manifests. You can ignore the source code. You&#8217;re only interested in the manifest files.</span></p>
</div>
<div class="readable-text" data-hash="ce2ccce4072e73f65bd3c920bb6e7ba3" data-text-hash="cfcd47dcbcf76c258755904639d8940d" id="373" refid="373">
<p><span>You can decide if you want to deploy the operator and MongoDB in the same namespace, or if you want to deploy the operator so that each user in the cluster can deploy their own MongoDB instance(s). For simplicity, I&#8217;ll use a single namespace.</span></p>
</div>
<div class="readable-text" data-hash="5601314b40f4d7a96e7e503b1f89ff65" data-text-hash="8db23c92957d2ebecb7c9326fcc73491" id="374" refid="374">
<h4>Extending the API with the MongoDBCommunity object kind</h4>
</div>
<div class="readable-text" data-hash="bfe9271a5975fad1a648f87740b9958c" data-text-hash="ab4e4cfbd65ec4afb8d89ced67669886" id="375" refid="375">
<p><span>First, you create a CustomResourceDefinition object that extends your cluster&#8217;s Kubernetes API with an additional object type. To do this, you apply the object manifest as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="bc83001dffd57b4726024bb3267e9e84" data-text-hash="458391fa68bdaf88dab20cc99d3ac68c" id="376" refid="376">
<div class="code-area-container">
<pre class="code-area">$ kubectl apply -f config/crd/bases/mongodbcommunity.mongodb.com_mongodbcommunity.yaml
customresourcedefinition/mongodbcommunity.mongodbcommunity.mongodb.com created</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="d955a12fb015a6316cc2f0e74bd7ab8f" data-text-hash="fa5df649b51ba612d515a3283de9fdda" id="377" refid="377">
<p><span>Using your cluster&#8217;s API, you can now create objects of kind MongoDBCommunity. You&#8217;ll create this object later.</span></p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="378" refid="378">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="bdd33dd1b8bd950e1c8e43de4811aca9" data-text-hash="e12aacc366f65420d29d2c3e244975dd" id="379" refid="379">
<p> <span>Unfortunately, the object kind is MongoDBCommunity, which makes it hard to understand that this object represents a MongoDB deployment and not a community. The reason it&#8217;s called MongoDBCommunity is because you&#8217;re using the community version of the operator. If you use the Enterprise version, the naming is more appropriate. There the object kind is MongoDB, which clearly indicates that the object represents a MongoDB deployment.</span></p>
</div>
</div>
<div class="readable-text" data-hash="ed4d31cd6b28e1f1b03953f614b6c736" data-text-hash="700415071cc2282ce505e8738d3c6acc" id="380" refid="380">
<h4>Creating supporting objects</h4>
</div>
<div class="readable-text" data-hash="f308b3993f8ead665b5ae389334ff755" data-text-hash="790ce71202571086cf57c4ca6af71dd5" id="381" refid="381">
<p><span>Next, you create various other security-related objects by applying their manifests. Here you need to specify the namespace in which these objects should be created. Let&#8217;s use the namespace</span> <code>mongodb</code><span>. Apply the manifests as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="40b86b5fdc3c7ba544c9b10b001e9161" data-text-hash="770846e1b46a522a0559824116220b70" id="382" refid="382">
<div class="code-area-container">
<pre class="code-area">$ kubectl apply -k config/rbac/ -n mongodb
serviceaccount/mongodb-database created
serviceaccount/mongodb-kubernetes-operator created
role.rbac.authorization.k8s.io/mongodb-database created
role.rbac.authorization.k8s.io/mongodb-kubernetes-operator created
rolebinding.rbac.authorization.k8s.io/mongodb-database created
rolebinding.rbac.authorization.k8s.io/mongodb-kubernetes-operator created</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="383" refid="383">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="2ba8981affe12b23e31213f0d96f87bf" data-text-hash="56f4d4ac9ffbad31a1e3ee17ba13648c" id="384" refid="384">
<p> <span>You&#8217;ll learn more about these object types and CustomResourceDefinitions in the remaining chapters of this book.</span></p>
</div>
</div>
<div class="readable-text" data-hash="bc13f04a008eddc04f1903c7104152d4" data-text-hash="d613f212f0e3d9af50148a88b74b24e0" id="385" refid="385">
<h4>Installing the operator</h4>
</div>
<div class="readable-text" data-hash="358fda4c2fb57515bb1f0e2cd3830a78" data-text-hash="1ed94a21a3fcbd99f519839f860cf0c4" id="386" refid="386">
<p><span>The last step is to install the operator by creating a Deployment as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="6fbeb1ccdb1ee5dfa91ad6448f3251a9" data-text-hash="8df8cb18282f28b364eb5028b1e71928" id="387" refid="387">
<div class="code-area-container">
<pre class="code-area">$ kubectl create -f config/manager/manager.yaml -n mongodb
deployment.apps/mongodb-kubernetes-operator created</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="6b76cb07dd5efc7aa47628ab3e68b770" data-text-hash="d6f9447c542ebd60d3382ceb64683a9b" id="388" refid="388">
<p><span>Verify that the operator Pod exists and is running by listing the Pods in the</span> <code>mongodb</code> <span>namespace:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="3d17392c4c23a6cffaa8375d53713a6e" data-text-hash="d77164f8e550fd56af85cbc2a87d48f4" id="389" refid="389">
<div class="code-area-container">
<pre class="code-area">$ kubectl get pods -n mongodb
NAME                                           READY   STATUS    RESTARTS   AGE
mongodb-kubernetes-operator-648bf8cc59-wzvhx   1/1     Running   0          9s</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="27ccaef405128bb20adc450a17ce3f53" data-text-hash="c6f1bb384403a403760fcbe1cdeab2ae" id="390" refid="390">
<p><span>That wasn&#8217;t so hard, was it? The operator is running now, but you haven&#8217;t deployed MongoDB yet. The operator is just the tool you use to do that.</span></p>
</div>
<div class="readable-text" data-hash="fb15703b3e2d053dc305e06485e1cef9" data-text-hash="65d8e332c2fad53c855a89ce1d4aa684" id="391" refid="391">
<h3 id="sigil_toc_id_285">15.4.2&#160;<span>Deploying MongoDB via the operator</span></h3>
</div>
<div class="readable-text" data-hash="899f71e000fce22288f901daafa75030" data-text-hash="c2e3921ea552bd2da2fa892a393ab9f3" id="392" refid="392">
<p><span>To deploy a MongoDB replica set, you create an instance of the MongoDBCommunity object type instead of creating StatefulSets and the other objects.</span></p>
</div>
<div class="readable-text" data-hash="218ffbf21b2d281ee9203d5d3c0b1846" data-text-hash="5048f0357498841b4e238023fd1cadf2" id="393" refid="393">
<h4>Creating an instance of the MongoDBCommunity object type</h4>
</div>
<div class="readable-text" data-hash="1a133ed0613b17330c53b5a32c4d4e8e" data-text-hash="0ae315f5d788cde118202850ea2281aa" id="394" refid="394">
<p><span>First edit the file</span> <code>config/samples/mongodb.com_v1_mongodbcommunity_cr.yaml</code> <span>to replace the string</span> <code>&lt;your-password-here&gt;</code> <span>with the password of your choice.</span></p>
</div>
<div class="readable-text" data-hash="3d146dd06098735c0eb172ffe6199f6b" data-text-hash="7fb34fb50d9ab1a28c3957df140b4a89" id="395" refid="395">
<p><span>The file contains manifests for a MongoDBCommunity and a Secret object. The following listing shows the manifest of the former.</span></p>
</div>
<div class="browsable-container listing-container" data-hash="023cfe265a82f179ebc3893b14610efc" data-text-hash="877d058326f3c53a9caa790ee6a9b9c5" id="396" refid="396">
<h5>Listing 15.9 The MongoDBCommunity custom object manifest</h5>
<div class="code-area-container">
<pre class="code-area">apiVersion: mongodbcommunity.mongodb.com/v1    #A
kind: MongoDBCommunity    #A
metadata:
  name: example-mongodb    #B
spec:
  members: 3    #C
  type: ReplicaSet    #C
  version: "4.2.6"    #D
  security:     #E
    authentication:     #E
      modes: ["SCRAM"]     #E
  users:     #E
    - name: my-user     #E
      db: admin     #E
      passwordSecretRef:      #E
        name: my-user-password     #E
      roles:     #E
        - name: clusterAdmin     #E
          db: admin     #E
        - name: userAdminAnyDatabase     #E
          db: admin     #E
      scramCredentialsSecretName: my-scram     #E
  additionalMongodConfig:     #E
    storage.wiredTiger.engineConfig.journalCompressor: zlib     #E</pre>
<div class="code-annotations-overlay-container" data-annotations="I0EgVGhpcyBvYmplY3Qga2luZCBpcyBNb25nb0RCQ29tbXVuaXR5LCB3aGljaCBpcyBhIGN1c3RvbSBvYmplY3Qga2luZCwgd2hpY2ggaXMgYW4gZXh0ZW5zaW9uIG9mIHRoZSBjb3JlIEt1YmVybmV0ZXMgQVBJLgojQiBUaGUgb2JqZWN0IG5hbWUgaXMgc3BlY2lmaWVkIGluIHRoZSBtZXRhZGF0YSBzZWN0aW9uLCBqdXN0IGxpa2UgaW4gYWxsIG90aGVyIEt1YmVybmV0ZXMgb2JqZWN0cyB0eXBlcy4KI0MgVGhpcyB0ZWxscyB0aGUgb3BlcmF0b3IgdG8gY3JlYXRlIGEgTW9uZ29EQiByZXBsaWNhIHNldCB3aXRoIHRocmVlIHJlcGxpY2FzLgojRCBZb3Ugc3BlY2lmeSB3aGljaCBNb25nb0RCIHZlcnNpb24geW91IHdhbnQgdG8gZGVwbG95LgojRSBZb3UgY2FuIGFsc28gc3BlY2lmeSBtYW55IG90aGVyIGNvbmZpZ3VyYXRpb24gb3B0aW9ucyBmb3IgdGhlIE1vbmdvREIgZGVwbG95bWVudC4="></div>
</div>
</div>
<div class="readable-text" data-hash="db11afbafe55d32195529642bbab629f" data-text-hash="bbf3ddfd7ff75d7fd878430b208f5cba" id="397" refid="397">
<p><span>As you can see, this custom object has the same structure as the Kubernetes API core objects. The</span> <code>apiVersion</code> <span>and</span> <code>kind</code> <span>fields specify the object type, the</span> <code>name</code> <span>field in the</span> <code>metadata</code> <span>section specifies the object name, and the</span> <code>spec</code> <span>section specifies the configuration for the MongoDB deployment, including</span> <code>type</code> <span>and</span> <code>version</code><span>, the desired number of replica set</span> <code>members</code><span>, and the security-related configuration.</span></p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="398" refid="398">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="41c3be6c1c5eb1a4969ab2c6ceb07e17" data-text-hash="8215e4611183bb58c791bd495bf47319" id="399" refid="399">
<p> <span>If the custom resource definition is well done, as in this case, you can use the</span> <code>kubectl explain</code> <span>command to learn more about the fields supported in this object type.</span></p>
</div>
</div>
<div class="readable-text" data-hash="379a7cc1e283a5d0c6f18ed466242a99" data-text-hash="93620e0d8d5ac477f4fd987ddb8ca88e" id="400" refid="400">
<p><span>To deploy MongoDB, you apply this manifest file with</span> <code>kubectl apply</code> <span>as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="78971f9be3fbe8becd473aa3e81642c9" data-text-hash="cce3edc2869a59877e74e2842c1cbc09" id="401" refid="401">
<div class="code-area-container">
<pre class="code-area">$ kubectl apply -f config/samples/mongodb.com_v1_mongodbcommunity_cr.yaml
mongodbcommunity.mongodbcommunity.mongodb.com/example-mongodb created
secret/my-user-password created</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="84a8b2ce23ff77c5b9bdf90b1a6696fb" data-text-hash="e37df8c3fce616a3816f3994312deb50" id="402" refid="402">
<h4>Inspecting the MongoDBCommunity object</h4>
</div>
<div class="readable-text" data-hash="74fdcf83ea8f8981fe4689f340e592a5" data-text-hash="1dc8f075a788fbaef8aa8e6e56c35d63" id="403" refid="403">
<p><span>You can then see the object you created with the</span> <code>kubectl get</code> <span>command as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="18bd456b23c103c4fa1a74d459c8214b" data-text-hash="0033a031762511ebf99f8cd18603a9e0" id="404" refid="404">
<div class="code-area-container">
<pre class="code-area">$ kubectl get mongodbcommunity
NAME              PHASE     VERSION
example-mongodb   Running   4.2.6</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="ab646f334e3d941e4f8cebbbe4002503" data-text-hash="a72f26acfe5f26d9fa77590037aa4e72" id="405" refid="405">
<p><span>Just like the other Kubernetes controllers, the object you created is now processed in the reconciliation loop running in the operator. Based on the MongoDBCommunity object, the operator creates several objects: a StatefulSet, two Services, and some Secrets. If you check the</span> <code>ownerReferences</code> <span>field in these objects, you&#8217;ll see that they&#8217;re all owned by the</span> <code>example-mongodb</code> <span>MongoDBCommunity object. If you make direct changes to these objects, such as scaling the StatefulSet, the operator will immediately undo your changes.</span></p>
</div>
<div class="readable-text" data-hash="1e339ca58c2ba11d7bf81522d83ba43c" data-text-hash="aba615be9102000a40fb30b36d7c8f0a" id="406" refid="406">
<p><span>After the operator creates the Kubernetes core objects, the core controllers do their part. For example, the StatefulSet controller creates the Pods. Use</span> <code>kubectl get</code> <span>to list them as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="702c071025c22229f3312730638a80fa" data-text-hash="7464fcdeac0b2432f23d98e1b19a1814" id="407" refid="407">
<div class="code-area-container">
<pre class="code-area">$ kubectl get pods -l app=example-mongodb-svc
NAME                READY   STATUS    RESTARTS   AGE
example-mongodb-0   2/2     Running   0          3m
example-mongodb-1   2/2     Running   0          2m
example-mongodb-2   2/2     Running   0          1m</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="c7be3a18ca27f392da97434ac61ef3f7" data-text-hash="f3aac30119638926bacc24ef709da5a4" id="408" refid="408">
<p><span>The MongoDB operator not only creates the StatefulSet, but also makes sure that the MongoDB replica set is initiated automatically. You can use it right away. No additional manual configuration is required.</span></p>
</div>
<div class="readable-text" data-hash="494093f31d37a3528d6fbcf9c65e105d" data-text-hash="32a5cb5708073bc4ae247d9c4321d6ef" id="409" refid="409">
<h4>Managing the MongoDB deployment</h4>
</div>
<div class="readable-text" data-hash="39debb1a7ec82d3e58ea7dc98cca6e08" data-text-hash="63c7caf7ef754e6a246bba36f682518f" id="410" refid="410">
<p><span>You control the MongoDB deployment through the MongoDBCommunity object. The operator updates the configuration every time you update this object. For example, if you want to resize the MongoDB replica set, you change the value of the</span> <code>members</code> <span>field in the</span> <code>example-mongodb</code> <span>object. The operator then scales the underlying StatefulSet and reconfigures the MongoDB replica set. This makes scaling MongoDB trivial.</span></p>
</div>
<div class="callout-container fm-callout">
<div class="readable-text" data-hash="f35a087d0dc71beb3a7204d91c1c49e4" data-text-hash="3b0649c72650c313a357338dcdfb64ec" id="411" refid="411">
<h5>Note</h5>
</div>
<div class="readable-text" data-hash="c62466f10778a8ea342f263c777b5d8a" data-text-hash="1ea21ac16ddbdb2e98205b8f490fbec4" id="412" refid="412">
<p> <span>At the time of writing, you can&#8217;t use the</span> <code>kubectl scale</code> <span>command to scale the MongoDBCommunity object, but I&#8217;m sure the MongoDB operator developers will fix this soon.</span></p>
</div>
</div>
<div class="readable-text" data-hash="6a9f8cfa35303e19a178e9a00084663b" data-text-hash="fa59222182f7cbeec03821abea4a289c" id="413" refid="413">
<h3 id="sigil_toc_id_286">15.4.3&#160;<span>Cleaning up</span></h3>
</div>
<div class="readable-text" data-hash="565a94b3c9909de46ae81ba4178eb31a" data-text-hash="a06ab3a0adb597eee61fb17598c2cdba" id="414" refid="414">
<p><span>To uninstall MongoDB, delete the MongoDBCommunity object as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="ec3f6c430d181286454de6a839ae1dbe" data-text-hash="0487b077515e7abaaf3f036eedad383c" id="415" refid="415">
<div class="code-area-container">
<pre class="code-area">$ kubectl delete mongodbcommunity example-mongodb
mongodbcommunity.mongodbcommunity.mongodb.com "example-mongodb" deleted</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="2b0476c6dcba6e9e40f4b7819a51fcc0" data-text-hash="4c75459d31cc77694ab677c356466cd0" id="416" refid="416">
<p><span>As you might expect, this orphans the underlying StatefulSet, Services, and other objects. The garbage collector then deletes them. To remove the operator, you can delete the entire</span> <code>mongodb</code> <span>Namespace as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="c589490e7dc4e36780b7a223f82361d5" data-text-hash="12ff71e4bf03a24e8468ef78f4dde571" id="417" refid="417">
<div class="code-area-container">
<pre class="code-area">$ kubectl delete ns mongodb
namespace "mongodb" deleted</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="c0e76dd722218b279de8f7751c70fa6f" data-text-hash="3f99707ba21121bff8a5d94386bea92d" id="418" refid="418">
<p><span>As a last step, you also need to delete the CustomResourceDefinition to remove the custom object type from the API as follows:</span></p>
</div>
<div class="browsable-container listing-container" data-hash="d2bcaa0280f4ae54039b90f8d4233cf0" data-text-hash="d448e92140cb67538d67aa8019bf39e3" id="419" refid="419">
<div class="code-area-container">
<pre class="code-area">$ kubectl delete crd mongodbcommunity.mongodbcommunity.mongodb.com
customresourcedefinition "mongodbcommunity.mongodbcommunity.mongodb.com" deleted</pre>
<div class="code-annotations-overlay-container" data-annotations=""></div>
</div>
</div>
<div class="readable-text" data-hash="9fbfbcf97c21934b056609494c5ec1ce" data-text-hash="c818b85415afffa0cbbc4178be2bcfcd" id="420" refid="420">
<h2 id="sigil_toc_id_287">15.5 Summary</h2>
</div>
<div class="readable-text" data-hash="6eeb39aa2ab72f6e83168b9c1ccd45f4" data-text-hash="0cc6b841061e9fe631b9b3583d6300ac" id="421" refid="421">
<p><span>In this chapter, you learned</span> <span>how to run stateful applications in Kubernetes. You learned that</span><span>:</span></p>
</div>
<ul>
<li class="readable-text" data-hash="02db05b7d6fdeec4613bfa9eedfe0175" data-text-hash="823d97d0409a550933f7a3a470e06926" id="422" refid="422"><span>Stateful workloads are harder to manage than their stateless counterparts because managing state is difficult. However, with StatefulSets, managing stateful workloads becomes much easier because the StatefulSet controller automates most of the work.</span></li>
<li class="readable-text" data-hash="61c2b7f0443d54482792bee06908d3ea" data-text-hash="56136a17cc4ef4dd4f30076794e0126f" id="423" refid="423"><span>With StatefulSets you can manage a group of Pods as pets, whereas Deployments treat the Pods like cattle. The Pods in a StatefulSet use ordinal numbers instead of having random names.</span></li>
<li class="readable-text" data-hash="263b264e477c356c037dab8e3dfffd77" data-text-hash="65ccfad3cc3adc5e74ab0ace36b5a485" id="424" refid="424"><span>A StatefulSet ensures that each replica gets its own stable identity and its own PersistentVolumeClaim(s). These claims are always associated with the same Pods.</span></li>
<li class="readable-text" data-hash="f36b9e6bc24ddab657e7b48be565de0a" data-text-hash="dffa9a2c4577fab04e805bde95d0e42d" id="425" refid="425"><span>In combination with a StatefulSet, a headless Service ensures that each Pod receives a DNS record that always resolves to the Pod&#8217;s IP address, even if the Pod is moved to another node and receives a new IP address.</span></li>
<li class="readable-text" data-hash="1a7eb1b61ad0703d2a409ba29c27e936" data-text-hash="db01fb8931d309e80018795afbb33ab5" id="426" refid="426"><span>StatefulSet Pods are created in the order of ascending ordinal numbers, and deleted in reverse order.</span></li>
<li class="readable-text" data-hash="d12377bfbac29aa315438cee11fe15c2" data-text-hash="d60e0f057cff96e7985bb9c18dd24ccd" id="427" refid="427"><span>The Pod management policy configured in the StatefulSet determines whether Pods are created and deleted sequentially or simultaneously.</span></li>
<li class="readable-text" data-hash="5fb8bec72ca5817ed7f12cee2974e424" data-text-hash="77b9212ebc670525145bf9a1da2dbc1e" id="428" refid="428"><span>The PersistentVolumeClaim retention policy determines whether claims are deleted or retained when you scale down or delete a StatefulSet.</span></li>
<li class="readable-text" data-hash="dbb23e855b23f41bd6e4604412803be1" data-text-hash="c93edb6cc8c81fcfae4c3d8b6595fe10" id="429" refid="429"><span>When you update the Pod template in a StatefulSet, the controller updates the underlying Pods. This happens on a rolling basis, from highest to lowest ordinal number. Alternatively, you can use a semi-automatic update strategy, where you delete a Pod and the controller then replaces it.</span></li>
<li class="readable-text" data-hash="b871c9d082644b54c10fa50a18875c53" data-text-hash="2e89c43878576e6042d9dae0f1b603cc" id="430" refid="430"><span>Since StatefulSets don&#8217;t provide everything needed to fully manage a stateful workload, these types of workloads are typically managed via custom API object types and Kubernetes Operators. You create an instance of the custom object, and the Operator then creates the StatefulSet and supporting objects.</span></li>
</ul>
<div class="readable-text" data-hash="d3d757a0b55fbe3a2638fc4887b97e55" data-text-hash="9bf5abfc5f350aabb81db48f6965138c" id="431" refid="431">
<p><span>In this chapter, you also created the</span> <code>quiz-data-importer</code> <span>Pod,</span> <span>w</span><span>hich</span><span>,</span> <span>unlike all the other Pods you&#8217;ve created so far,</span> <span>perform</span><span>s</span> <span>a single</span> <span>task</span> <span>and then</span> <span>exits</span><span>.</span> <span>In the next chapter, you&#8217;ll learn how to run these types of workloads using the Job and CronJob object types.</span> <span>You'll also learn how to</span> <span>use a DaemonSet to</span> <span>run a</span> <span>system</span> <span>Pod on each node.</span></p>
</div></div>

        </body>
        
        